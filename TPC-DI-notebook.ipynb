{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPCDI Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import findspark\n",
    "from src.visibilities_queries import tpcdi_visibility_q1, tpcdi_visibility_q2\n",
    "from src.validation_query import tpcdi_validation_query_1, tpcdi_validation_query_2\n",
    "findspark.init()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "import shutil\n",
    "import os\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import queue\n",
    "\n",
    "warehouse_path = os.getcwd()+'/warehouse/'\n",
    "shutil.rmtree(warehouse_path)\n",
    "os.makedirs(warehouse_path)\n",
    "\n",
    "conf = pyspark.SparkConf().set(\"spark.sql.legacy.createHiveTableByDefault\", \"false\").set(\"spark.sql.autoBroadcastJoinThreshold\", -1).set(\"spark.sql.warehouse.dir\", warehouse_path).setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark_ui_url = sc.uiWebUrl\n",
    "print(f\"Spark UI is available at {spark_ui_url}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "#                                                                       #\n",
    "#                         VISIBILITY SESSION                              #        \n",
    "#                                                                       #    \n",
    "#########################################################################\n",
    "\n",
    "# Initialize the Visibility Session (visibility queries)\n",
    "visibility_spark = SparkSession.builder \\\n",
    "    .appName(\"VisibilityQuerySession\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"1\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpcdi_validation_query_1 = \"\"\"\n",
    "insert into DImessages\n",
    "\n",
    "select\n",
    "\n",
    "     CURRENT_TIMESTAMP as MessageDateAndTime\n",
    "     ,case when BatchID is null then 0 else BatchID end as BatchID\n",
    "     ,MessageSource\n",
    "     ,MessageText \n",
    "     ,'Validation' as MessageType\n",
    "     ,MessageData\n",
    "\n",
    "from (\n",
    "     select max(BatchID) as BatchID from DImessages \n",
    ") x join (\n",
    "\n",
    "    /* Basic row counts */\n",
    "     select 'DimAccount' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from DimAccount\n",
    "     union select 'DimBroker' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from DimBroker\n",
    "     union select 'DimCompany' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from DimCompany\n",
    "     union select 'DimCustomer' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from DimCustomer\n",
    "     union select 'DimDate' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from DimDate\n",
    "     union select 'DimSecurity' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from DimSecurity\n",
    "     union select 'DimTime' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from DimTime\n",
    "     union select 'DimTrade' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from DimTrade\n",
    "     union select 'FactCashBalances' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from FactCashBalances\n",
    "     union select 'FactHoldings' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from FactHoldings\n",
    "     union select 'FactMarketHistory' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from FactMarketHistory\n",
    "     union select 'FactWatches' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from FactWatches\n",
    "     union select 'Financial' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from Financial\n",
    "     union select 'Industry' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from Industry\n",
    "     union select 'Prospect' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from Prospect\n",
    "     union select 'StatusType' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from StatusType\n",
    "     union select 'TaxRate' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from TaxRate\n",
    "     union select 'TradeType' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from TradeType\n",
    "     /* Joined row counts for Fact tables */\n",
    "     union select 'FactCashBalances' as MessageSource, 'Row count joined' as MessageText, \n",
    "\t\t\tcount(*) as MessageData \n",
    "\t\t\tfrom FactCashBalances f\n",
    "\t\t\tinner join DimAccount a on f.SK_AccountID = a.SK_AccountID\n",
    "\t\t\tinner join DimCustomer c on f.SK_CustomerID = c.SK_CustomerID\n",
    "\t\t\tinner join DimBroker b on a.SK_BrokerID = b.SK_BrokerID\n",
    "\t\t\tinner join DimDate d on f.SK_DateID = d.SK_DateID\n",
    "     union select 'FactHoldings' as MessageSource, 'Row count joined' as MessageText, \n",
    "\t\t\tcount(*) as MessageData \n",
    "\t\t\tfrom FactHoldings f\n",
    "\t\t\tinner join DimAccount a on f.SK_AccountID = a.SK_AccountID\n",
    "\t\t\tinner join DimCustomer c on f.SK_CustomerID = c.SK_CustomerID\n",
    "\t\t\tinner join DimBroker b on a.SK_BrokerID = b.SK_BrokerID\n",
    "\t\t\tinner join DimDate d on f.SK_DateID = d.SK_DateID\n",
    "\t\t\tinner join DimTime t on f.SK_TimeID = t.SK_TimeID\n",
    "\t\t\tinner join DimCompany m on f.SK_CompanyID = m.SK_CompanyID\n",
    "\t\t\tinner join DimSecurity s on f.SK_SecurityID = s.SK_SecurityID\n",
    "    union select 'FactMarketHistory' as MessageSource, 'Row count joined' as MessageText, \n",
    "\t\t\tcount(*) as MessageData \n",
    "\t\t\tfrom FactMarketHistory f\n",
    "\t\t\tinner join DimDate d on f.SK_DateID = d.SK_DateID\n",
    "\t\t\tinner join DimCompany m on f.SK_CompanyID = m.SK_CompanyID\n",
    "\t\t\tinner join DimSecurity s on f.SK_SecurityID = s.SK_SecurityID\n",
    "    union select 'FactWatches' as MessageSource, 'Row count joined' as MessageText, \n",
    "\t\t\tcount(*) as MessageData \n",
    "\t\t\tfrom FactWatches f\n",
    "\t\t\tinner join DimCustomer c on f.SK_CustomerID = c.SK_CustomerID\n",
    "\t\t\tinner join DimDate dp on f.SK_DateID_DatePlaced = dp.SK_DateID\n",
    "\t\t\t-- (cannot join on SK_DateID_DateRemoved because that field can be null)\n",
    "\t\t\tinner join DimSecurity s on f.SK_SecurityID = s.SK_SecurityID\n",
    "    /* Additional information used at Audit time */\n",
    "    union select 'DimCustomer' as MessageSource, 'Inactive customers' as MessageText, count(*) from DimCustomer where IsCurrent = 1 and Status = 'Inactive'\n",
    "    union select 'FactWatches' as MessageSource, 'Inactive watches' as MessageText, count(*) from FactWatches where SK_DATEID_DATEREMOVED is not null\n",
    ") y on 1=1; \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "tpcdi_validation_query_2 = \"\"\"\n",
    "/* Phase complete record */\n",
    "insert into DImessages\n",
    "select\n",
    "     MessageDateAndTime\n",
    "     ,case when BatchID is null then 0 else BatchID end as BatchID\n",
    "     ,MessageSource\n",
    "     ,MessageText \n",
    "     ,MessageType\n",
    "     ,MessageData\n",
    "from (\n",
    "     select CURRENT_TIMESTAMP as MessageDateAndTime\n",
    "            ,max(BatchID) as BatchID\n",
    "            ,'Phase Complete Record' as MessageSource\n",
    "            ,'Batch Complete' as MessageText\n",
    "            ,'PCR' as MessageType\n",
    "            ,NULL as MessageData\n",
    "  from DImessages\n",
    ") \n",
    ";\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scale_factor = \"Scale3\" # Options \"Scale3\"]):#, \"Scale4\", \"Scale5\", \"Scale6\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Clean Stack\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "def clean_warehouse(dbname=\"test\"):\n",
    "    spark.sql(f\"DROP DATABASE IF EXISTS {dbname} CASCADE\")\n",
    "    warehouse_path = os.getcwd()+'/warehouse/'\n",
    "    shutil.rmtree(warehouse_path)\n",
    "    os.makedirs(warehouse_path)\n",
    "    print(f\"Warehouse {dbname} deleted.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %pip install google-cloud-storage\n",
    "# MAGIC %pip install fsspec\n",
    "# MAGIC %pip install gcsfs\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC \n",
    "# MAGIC #Create Data Warehouse\n",
    "# MAGIC ### Create Dims\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def cast_to_target_schema(source_table: str, target_table: str):\n",
    "    # Load source and target DataFrames\n",
    "    source_df = spark.table(source_table)\n",
    "    target_df = spark.table(target_table)\n",
    "\n",
    "    # Get the schema of the target table\n",
    "    target_schema = target_df.schema\n",
    "\n",
    "    # Create a list to hold columns, casting only the matching columns\n",
    "    casted_columns = []\n",
    "\n",
    "    for column in source_df.columns:\n",
    "        if column in target_df.columns:\n",
    "            # Get the target data type for matching columns\n",
    "            target_dtype = target_schema[column].dataType\n",
    "            # Cast to target data type and add to the list\n",
    "            casted_columns.append(col(column).cast(target_dtype).alias(column))\n",
    "        else:\n",
    "            # Keep the original column if it does not exist in target schema\n",
    "            casted_columns.append(col(column))\n",
    "\n",
    "    # Select all columns from the source with necessary casts applied\n",
    "    casted_df = source_df.select(*casted_columns)\n",
    "\n",
    "    return casted_df\n",
    "\n",
    "\n",
    "def create_dim_account(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS DimAccount(\n",
    "            SK_AccountID INTEGER,\n",
    "            AccountID BIGINT,\n",
    "            SK_BrokerID BIGINT,\n",
    "            SK_CustomerID BIGINT,\n",
    "            Status CHAR(10),\n",
    "            AccountDesc CHAR(50),\n",
    "            TaxStatus INTEGER,\n",
    "            IsCurrent BOOLEAN,\n",
    "            BatchID CHAR(14),\n",
    "            EffectiveDate DATE,\n",
    "            EndDate DATE\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(\"Created dim account.\")\n",
    "\n",
    "\n",
    "def create_dim_broker(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS DimBroker(\n",
    "            SK_BrokerID INTEGER,\n",
    "            BrokerID BIGINT,\n",
    "            ManagerID BIGINT,\n",
    "            FirstName CHAR(50),\n",
    "            LastName CHAR(50),\n",
    "            MiddleInitial CHAR(1),\n",
    "            Branch CHAR(50),\n",
    "            Office CHAR(50),\n",
    "            Phone CHAR(14),\n",
    "            IsCurrent BOOLEAN,\n",
    "            BatchID INTEGER,\n",
    "            EffectiveDate DATE,\n",
    "            EndDate DATE\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(\"Created dim broker.\")\n",
    "\n",
    "\n",
    "def create_dim_company(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS DimCompany(\n",
    "            SK_CompanyID BIGINT,\n",
    "            CompanyID BIGINT,\n",
    "            Status CHAR(10),\n",
    "            Name CHAR(60),\n",
    "            Industry CHAR(50),\n",
    "            SPrating CHAR(4),\n",
    "            isLowGrade BOOLEAN,\n",
    "            CEO CHAR(100),\n",
    "            AddressLine1 CHAR(80),\n",
    "            AddressLine2 CHAR(80),\n",
    "            PostalCode CHAR(12),\n",
    "            City CHAR(25),\n",
    "            StateProv CHAR(20),\n",
    "            Country CHAR(24),\n",
    "            Description CHAR(150),\n",
    "            FoundingDate DATE,\n",
    "            IsCurrent BOOLEAN,\n",
    "            BatchID INTEGER,\n",
    "            EffectiveDate DATE,\n",
    "            EndDate DATE\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(\"Created dim company.\")\n",
    "\n",
    "\n",
    "def create_dim_customer(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS DimCustomer(\n",
    "            SK_CustomerID BIGINT,\n",
    "            CustomerID BIGINT,\n",
    "            TaxID CHAR(20),\n",
    "            Status CHAR(10),\n",
    "            LastName CHAR(30),\n",
    "            FirstName CHAR(30),\n",
    "            MiddleInitial CHAR(1),\n",
    "            Gender CHAR(1),\n",
    "            Tier INTEGER,\n",
    "            DOB DATE,\n",
    "            AddressLine1 CHAR(80),\n",
    "            AddressLine2 CHAR(84),\n",
    "            PostalCode CHAR(12),\n",
    "            City CHAR(25),\n",
    "            StateProv CHAR(20),\n",
    "            Country CHAR(24),\n",
    "            Phone1 CHAR(30),\n",
    "            Phone2 CHAR(30),\n",
    "            Phone3 CHAR(30),\n",
    "            Email1 CHAR(50),\n",
    "            Email2 CHAR(50),\n",
    "            NationalTaxRateDesc CHAR(50),\n",
    "            NationalTaxRate INTEGER,\n",
    "            LocalTaxRateDesc CHAR(50),\n",
    "            LocalTaxRate INTEGER,\n",
    "            AgencyID CHAR(30),\n",
    "            CreditRating INTEGER,\n",
    "            NetWorth FLOAT,\n",
    "            MarketingNamePlate CHAR(100),\n",
    "            IsCurrent BOOLEAN,\n",
    "            BatchID INTEGER,\n",
    "            EffectiveDate DATE,\n",
    "            EndDate DATE\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(\"Created dim customer.\")\n",
    "\n",
    "\n",
    "def create_dim_date(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS DimDate(\n",
    "            SK_DateID INTEGER,\n",
    "            DateValue DATE,\n",
    "            DateDesc CHAR(20),\n",
    "            CalendarYearID INTEGER,\n",
    "            CalendarYearDesc CHAR(20),\n",
    "            CalendarQtrID INTEGER,\n",
    "            CalendarQtrDesc CHAR(20),\n",
    "            CalendarMonthID INTEGER,\n",
    "            CalendarMonthDesc CHAR(20),\n",
    "            CalendarWeekID INTEGER,\n",
    "            CalendarWeekDesc CHAR(20),\n",
    "            DayOfWeekNum TINYINT,\n",
    "            DayOfWeekDesc CHAR(10),\n",
    "            FiscalYearID INTEGER,\n",
    "            FiscalYearDesc CHAR(20),\n",
    "            FiscalQtrID INTEGER,\n",
    "            FiscalQtrDesc CHAR(20),\n",
    "            HolidayFlag BOOLEAN\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(\"Created dim date.\")\n",
    "\n",
    "\n",
    "def create_dim_security(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS DimSecurity(\n",
    "            SK_SecurityID BIGINT,\n",
    "            Symbol CHAR(15),\n",
    "            Issue CHAR(6),\n",
    "            Status CHAR(10),\n",
    "            Name CHAR(70),\n",
    "            ExchangeID CHAR(6),\n",
    "            SK_CompanyID BIGINT,\n",
    "            SharesOutstanding BIGINT,\n",
    "            FirstTrade DATE,\n",
    "            FirstTradeOnExchange DATE,\n",
    "            Dividend FLOAT,\n",
    "            IsCurrent BOOLEAN,\n",
    "            BatchID INTEGER,\n",
    "            EffectiveDate DATE,\n",
    "            EndDate DATE\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(\"Created dim security.\")\n",
    "\n",
    "\n",
    "def create_dim_time(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS DimTime(\n",
    "            SK_TimeID INTEGER,\n",
    "            TimeValue TIMESTAMP,\n",
    "            HourID INTEGER,\n",
    "            HourDesc CHAR(20),\n",
    "            MinuteID INTEGER,\n",
    "            MinuteDesc CHAR(20),\n",
    "            SecondID INTEGER,\n",
    "            SecondDesc CHAR(20),\n",
    "            MarketHoursFlag BOOLEAN,\n",
    "            OfficeHoursFlag BOOLEAN\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(\"Created dim time.\")\n",
    "\n",
    "\n",
    "def create_dim_trade(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS DimTrade(\n",
    "            TradeID INTEGER,\n",
    "            SK_BrokerID INTEGER,\n",
    "            SK_CreateDateID INTEGER,\n",
    "            SK_CreateTimeID INTEGER,\n",
    "            SK_CloseDateID INTEGER,\n",
    "            SK_CloseTimeID INTEGER,\n",
    "            Status CHAR(10),\n",
    "            Type CHAR(12),\n",
    "            CashFlag BOOLEAN,\n",
    "            SK_SecurityID INTEGER,\n",
    "            SK_CompanyID INTEGER,\n",
    "            Quantity FLOAT,\n",
    "            BidPrice FLOAT,\n",
    "            SK_CustomerID INTEGER,\n",
    "            SK_AccountID INTEGER,\n",
    "            ExecutedBy CHAR(64),\n",
    "            TradePrice FLOAT,\n",
    "            Fee FLOAT,\n",
    "            Comission FLOAT,\n",
    "            Tax FLOAT,\n",
    "            BatchID INTEGER\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(\"Created dim trades.\")\n",
    "\n",
    "\n",
    "def create_dimessages_table(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS DIMessages(\n",
    "            MessageDateAndTime TIMESTAMP,\n",
    "            BatchID INTEGER,\n",
    "            MessageSource CHAR(30),\n",
    "            MessageText CHAR(50),\n",
    "            MessageType CHAR(12),\n",
    "            MessageData CHAR(100)\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(\"Created dim messages.\")\n",
    "\n",
    "\n",
    "def create_dims(dbname):\n",
    "    create_dim_account(dbname)\n",
    "    create_dim_broker(dbname)\n",
    "    create_dim_company(dbname)\n",
    "    create_dim_customer(dbname)\n",
    "    create_dim_date(dbname)\n",
    "    create_dim_security(dbname)\n",
    "    create_dim_time(dbname)\n",
    "    create_dim_trade(dbname)\n",
    "    create_dimessages_table(dbname)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Create Facts\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def create_fact_cash_balances(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS FactCashBalances(\n",
    "            SK_CustomerID INTEGER,\n",
    "            SK_AccountID INTEGER,\n",
    "            SK_DateID INTEGER,\n",
    "            Cash FLOAT,\n",
    "            BatchID INTEGER\n",
    "        )\n",
    "    \"\"\")\n",
    "    print(\"Created fact cash balances\")\n",
    "\n",
    "\n",
    "def create_fact_holdings(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS FactHoldings(\n",
    "            TradeID INTEGER,\n",
    "            CurrentTradeID INTEGER,\n",
    "            SK_CustomerID INTEGER,\n",
    "            SK_AccountID INTEGER,\n",
    "            SK_SecurityID INTEGER,\n",
    "            SK_CompanyID INTEGER,\n",
    "            SK_DateID INTEGER,\n",
    "            SK_TimeID INTEGER,\n",
    "            CurrentPrice FLOAT,\n",
    "            CurrentHolding INTEGER,\n",
    "            BatchID INTEGER\n",
    "        )\n",
    "    \"\"\")\n",
    "    print(\"Created fact holdings\")\n",
    "\n",
    "\n",
    "def create_fact_market_history(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS FactMarketHistory(\n",
    "            SK_SecurityID INTEGER,\n",
    "            SK_CompanyID INTEGER,\n",
    "            SK_DateID INTEGER,\n",
    "            PERatio FLOAT,\n",
    "            Yield FLOAT,\n",
    "            FiftyTwoWeekHigh FLOAT,\n",
    "            SK_FiftyTwoWeekHighDate INTEGER,\n",
    "            FiftyTwoWeekLow FLOAT,\n",
    "            SK_FiftyTwoWeekLowDate INTEGER,\n",
    "            ClosePrice FLOAT,\n",
    "            DayHigh FLOAT,\n",
    "            DayLow FLOAT,\n",
    "            Volume INTEGER,\n",
    "            BatchID INTEGER\n",
    "        )\n",
    "    \"\"\")\n",
    "    print(\"Created fact market history\")\n",
    "\n",
    "\n",
    "def create_fact_watches(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS FactWatches(\n",
    "            SK_CustomerID BIGINT,\n",
    "            SK_SecurityID BIGINT,\n",
    "            SK_DateID_DatePlaced BIGINT,\n",
    "            SK_DateID_DateRemoved BIGINT,\n",
    "            BatchID INTEGER\n",
    "        )\n",
    "    \"\"\")\n",
    "    print(\"Created fact watches\")\n",
    "\n",
    "\n",
    "def create_facts(dbname):\n",
    "    create_fact_cash_balances(dbname)\n",
    "    create_fact_holdings(dbname)\n",
    "    create_fact_market_history(dbname)\n",
    "    create_fact_watches(dbname)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Create Other Tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def create_industry_table(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Industry( \n",
    "            IN_ID CHAR(2),\n",
    "            IN_NAME CHAR(50),\n",
    "            IN_SC_ID CHAR(4)\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(\"Created table industry.\")\n",
    "\n",
    "def create_financial_table(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Financial( \n",
    "           SK_CompanyID BIGINT,\n",
    "           FI_YEAR Integer,\n",
    "           FI_QTR Integer,\n",
    "           FI_QTR_START_DATE DATE,\n",
    "           FI_REVENUE Float,\n",
    "           FI_NET_EARN Float,\n",
    "           FI_BASIC_EPS Float,\n",
    "           FI_DILUT_EPS  Float,\n",
    "           FI_MARGIN Float,\n",
    "           FI_INVENTORY Float,\n",
    "           FI_ASSETS Float,\n",
    "           FI_LIABILITY Float,\n",
    "           FI_OUT_BASIC Float,\n",
    "           FI_OUT_DILUT Float             \n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(\"Created table Finacial.\")\n",
    "\n",
    "def create_prospect_table(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "#     spark.sql(\"\"\"\n",
    "#         DROP TABLE Prospect\n",
    "#     \"\"\")\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Prospect( \n",
    "           AgencyID char(30),\n",
    "           SK_RecordDateID Integer,\n",
    "           SK_UpdateDateID Integer,\n",
    "           BatchID Integer,\n",
    "           IsCustomer Boolean,\n",
    "           LastName Char(30),\n",
    "           FirstName Char(30),\n",
    "           MiddleInitial Char(1),\n",
    "           Gender Char(1),\n",
    "           AddressLine1 Char(80),\n",
    "           AddressLine2 Char(80),\n",
    "           PostalCode Char(12),\n",
    "           City Char(25),\n",
    "           State Char(20),\n",
    "           Country Char(24),\n",
    "           Phone Char(30), \n",
    "           Income Char(9),\n",
    "           NumberCars Integer,\n",
    "           NumberChildren Integer,\n",
    "           MaritalStatus Char(1),\n",
    "           Age Integer,\n",
    "           CreditRating Integer,\n",
    "           OwnOrRentFlag Char(1),  \n",
    "           Employer Char(30),\n",
    "           NumberCreditCards Integer,\n",
    "           NetWorth Integer, \n",
    "           MarketingNameplate Char(100)\n",
    "                       \n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(\"Created table Prospect.\")\n",
    "# create_prospect_table(\"test\")\n",
    "\n",
    "\n",
    "def create_status_type_table(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS StatusType( \n",
    "           ST_ID CHAR(4),\n",
    "           ST_NAME CHAR(10)\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(\"Created table StatusType.\")\n",
    "    \n",
    "\n",
    "\n",
    "def create_taxrate_table(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS TaxRate( \n",
    "           TX_ID CHAR(4),\n",
    "           TX_NAME CHAR(50),\n",
    "           TX_RATE Float\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(\"Created table TaxRate.\")\n",
    "    \n",
    "\n",
    "\n",
    "def create_tradetype_table(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS TradeType( \n",
    "           TT_ID CHAR(3),\n",
    "           TT_NAME CHAR(12),\n",
    "           TT_IS_SELL Integer,\n",
    "           TT_IS_MRKT Integer\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(\"Created table TradeType.\")\n",
    "    \n",
    "\n",
    "\n",
    "def create_audit_table(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Audit( \n",
    "           DataSet CHAR(20),\n",
    "           BatchID Integer,\n",
    "           Date Date,\n",
    "           Attribute CHAR(50),\n",
    "           Value float,\n",
    "           DValue float \n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(\"Created table Audit.\")\n",
    "\n",
    "\n",
    "\n",
    "def create_other_tables(dbname):\n",
    "    create_industry_table(dbname)\n",
    "    create_financial_table(dbname)\n",
    "    create_prospect_table(dbname)\n",
    "    create_status_type_table(dbname)\n",
    "    create_taxrate_table(dbname)\n",
    "    create_tradetype_table(dbname)\n",
    "    create_audit_table(dbname)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def create_warehouse(dbname=\"test\"):\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {dbname}\")\n",
    "    create_dims(dbname)\n",
    "    create_facts(dbname)\n",
    "    create_other_tables(dbname)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC \n",
    "# MAGIC ## Execute Step 1\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# def clean_warehouse(dbname=\"test\"):\n",
    "#     spark.sql(f\"DROP DATABASE IF EXISTS {dbname} CASCADE\")\n",
    "#     warehouse_path = os.getcwd()+'/warehouse/'\n",
    "#     shutil.rmtree(warehouse_path)\n",
    "#     os.makedirs(warehouse_path)\n",
    "#     print(f\"Warehouse {dbname} deleted.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# clean_warehouse(\"test\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# create_warehouse()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 2: Historical Load\n",
    "# MAGIC \n",
    "# MAGIC ### This step:\n",
    "# MAGIC \n",
    "# MAGIC - populates the data warehouse\n",
    "# MAGIC - applies transformations described in clause 4.5 of the tpc-di manual (batch1 folder on the generated data)\n",
    "# MAGIC - performs validations based on clause 7.4\n",
    "# MAGIC - upon completion of the validation stage, a phase completion record is written into DIMessages table\n",
    "# MAGIC - this step must be timed\n",
    "# MAGIC \n",
    "# MAGIC \n",
    "# MAGIC \n",
    "# MAGIC ### Load Dim Date and Dim Time\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# General variables setup\n",
    "# staging_area_folder = f\"gs://tpcdi-with-spark-bdma/TPCDI_Data/TPCDI_Data/{scale_factor}/Batch1/\"\n",
    "\n",
    "#from google.cloud import storage\n",
    "#from google.oauth2 import service_account\n",
    "\n",
    "def create_gcs_client():\n",
    "    credentials_dict = {\n",
    "      \"type\": \"service_account\",\n",
    "      \"project_id\": \"bdma-371020\",\n",
    "      \"private_key_id\": \"\",\n",
    "      \"private_key\": \"\",\n",
    "      \"client_email\": \"tpcdi-databricks-bdma@bdma-371020.iam.gserviceaccount.com\",\n",
    "      \"client_id\": \"103179657336858348005\",\n",
    "      \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "      \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "      \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "      \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/tpcdi-databricks-bdma%40bdma-371020.iam.gserviceaccount.com\"\n",
    "    }\n",
    "    credentials = service_account.Credentials.from_service_account_info(credentials_dict)\n",
    "    client = storage.Client(credentials=credentials)\n",
    "    return client\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# General variables setup\n",
    "def load_dim_date(dbname, staging_area_folder):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "            `SK_DateID` INTEGER,\n",
    "            `DateValue` DATE,\n",
    "            `DateDesc` STRING,\n",
    "            `CalendarYearID` INTEGER,\n",
    "            `CalendarYearDesc` STRING,\n",
    "            `CalendarQtrID` INTEGER,\n",
    "            `CalendarQtrDesc` STRING,\n",
    "            `CalendarMonthID` INTEGER,\n",
    "            `CalendarMonthDesc` STRING,\n",
    "            `CalendarWeekID` INTEGER,\n",
    "            `CalendarWeekDesc` STRING,\n",
    "            `DayOfWeekNum` INTEGER,\n",
    "            `DayOfWeekDesc` STRING,\n",
    "            `FiscalYearID` INTEGER,\n",
    "            `FiscalYearDesc` STRING,\n",
    "            `FiscalQtrID` INTEGER,\n",
    "            `FiscalQtrDesc` STRING,\n",
    "            `HolidayFlag` BOOLEAN\n",
    "    \"\"\"\n",
    "    dates = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"|\")\n",
    "        .schema(schema)\n",
    "        .load(f\"{staging_area_folder}/Date.txt\")\n",
    "    )\n",
    "    dates.write.option(\"overwriteSchema\", \"true\").saveAsTable(\n",
    "        \"DimDate\", mode=\"overwrite\"\n",
    "    )\n",
    "    return dates\n",
    "# dates = load_dim_date(\"test\")\n",
    "# dates.limit(3).toPandas()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def load_dim_time(dbname, staging_area_folder):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "            `SK_TimeID` INTEGER,\n",
    "            `TimeValue` TIMESTAMP,\n",
    "            `HourID` INTEGER,\n",
    "            `HourDesc` STRING,\n",
    "            `MinuteID` INTEGER,\n",
    "            `MinuteDesc` STRING,\n",
    "            `SecondID` INTEGER,\n",
    "            `SecondDesc` STRING,\n",
    "            `MarketHoursFlag` BOOLEAN,\n",
    "            `OfficeHoursFlag` BOOLEAN\n",
    "    \"\"\"\n",
    "    times = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"|\")\n",
    "        .schema(schema)\n",
    "        .load(f\"{staging_area_folder}/Time.txt\")\n",
    "    )\n",
    "    times.write.option(\"overwriteSchema\", \"true\").saveAsTable(\n",
    "        \"DimTime\", mode=\"overwrite\"\n",
    "    )\n",
    "    return times\n",
    "# times = load_dim_time(\"test\")\n",
    "# times.limit(3).toPandas()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#Load tax rate\n",
    "\n",
    "def load_tax_rate(dbname, staging_area_folder):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "        `TX_ID` String,\n",
    "        `TX_NAME` String,\n",
    "        `TX_RATE` Float\n",
    "\n",
    "    \"\"\"\n",
    "    Tax_Rate = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"|\")\n",
    "        .schema(schema)\n",
    "        .load(f\"{staging_area_folder}/TaxRate.txt\")\n",
    "    )\n",
    "    Tax_Rate.createOrReplaceTempView(\"TaxRateView\")\n",
    "    \n",
    "    spark.sql(\n",
    "    \"\"\"\n",
    "        INSERT INTO TaxRate(TX_ID, TX_NAME,TX_RATE)\n",
    "        SELECT TX_ID, \n",
    "               TX_NAME as NationalTaxRateDesc, \n",
    "               TX_RATE as NationalTaxRate \n",
    "        FROM TaxRateView\n",
    "    \"\"\"\n",
    "    )\n",
    "    return Tax_Rate\n",
    "\n",
    "\n",
    "# Tax_Rate=load_tax_rate(\"test\")\n",
    "# Tax_Rate.limit(3).toPandas()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md \n",
    "# MAGIC ### HR File (DimBroker)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\"\"\"\n",
    "Records where EmployeeJobCode is not 314 are not broker records, and are ignored. The remaining steps are for records where the job code is 314.\n",
    "- BrokerID, ManagerID, FirstName, LastName, MiddleInitial, Branch, Office and Phone are obtained from these fields of the HR.csv file: EmployeeID, ManagerID, EmployeeFirstName, EmployeeLastName, EmployeeMI, EmployeeBranch, EmployeeOffice and EmployeePhone.\n",
    "- SK_BrokerID is set appropriately for new records as described in section 4.4.1.3.\n",
    "- IsCurrent is set to true\n",
    "- EffectiveDate is set to the earliest date in the DimDate table and EndDate is set to 9999-12-31.\n",
    "- BatchID is set as described in section 4.4.2\n",
    "            SK_BrokerID INTEGER GENERATED ALWAYS AS IDENTITY,\n",
    "            BrokerID BIGINT,\n",
    "            ManagerID BIGINT,\n",
    "            FirstName CHAR(50),\n",
    "            LastName CHAR(50),\n",
    "            MiddleInitial CHAR(1),\n",
    "            Branch CHAR(50),\n",
    "            Office CHAR(50),\n",
    "            Phone CHAR(14),\n",
    "            IsCurrent BOOLEAN,\n",
    "            BatchID INTEGER,\n",
    "            EffectiveDate DATE,\n",
    "            EndDate DATE\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_staging_hr_file(dbname, staging_area_folder):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "            `EmployeeID` INTEGER,\n",
    "            `ManagerID`  INTEGER,\n",
    "            `EmployeeFirstName` STRING,\n",
    "            `EmployeeLastName` STRING,\n",
    "            `EmployeeMI` STRING,\n",
    "            `EmployeeJobCode` INTEGER,\n",
    "            `EmployeeBranch` STRING,\n",
    "            `EmployeeOffice` STRING,\n",
    "            `EmployeePhone` STRING\n",
    "            \n",
    "    \"\"\"\n",
    "    brokers = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \",\")\n",
    "        .schema(schema)\n",
    "        .load(f\"{staging_area_folder}/HR.csv\")\n",
    "        .where(\"EmployeeJobCode = 314\")\n",
    "    )\n",
    "    # Save staging data into temp view\n",
    "    brokers.createOrReplaceTempView(\"hr\")\n",
    "    # Copy data into warehouse table\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "            SELECT \n",
    "                monotonically_increasing_id() AS SK_BrokerID,\n",
    "                EmployeeID AS BrokerID,\n",
    "                ManagerID,\n",
    "                EmployeeFirstName AS FirstName,\n",
    "                EmployeeLastName AS LastName,\n",
    "                EmployeeMI AS MiddleInitial,\n",
    "                EmployeeBranch AS Branch,\n",
    "                EmployeeOffice AS Office,\n",
    "                EmployeePhone AS Phone,\n",
    "                (SELECT TRUE) AS IsCurrent,\n",
    "                (SELECT 1) AS BatchID,\n",
    "                (SELECT MIN(DateValue) FROM DimDate) AS EffectiveDate,\n",
    "                TO_DATE('9999-12-31') AS EndDate\n",
    "            FROM hr\n",
    "    \"\"\").createOrReplaceTempView(\"DimBrokerNoUpdate\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            *, \n",
    "            MAX(SK_BrokerID) OVER (PARTITION BY BrokerID) AS LAST_SK, \n",
    "            LAG(EffectiveDate, -1) IGNORE NULLS OVER (PARTITION BY BrokerID ORDER BY EffectiveDate) \n",
    "                AS NewEndDate\n",
    "        FROM DimBrokerNoUpdate\n",
    "    \"\"\") \\\n",
    "    .withColumn(\"IsCurrent\", expr(\"CASE WHEN LAST_SK != SK_BrokerID THEN False ELSE TRUE END\")) \\\n",
    "    .withColumn(\n",
    "        \"EndDate\", \n",
    "        expr(\"CASE WHEN LAST_SK != SK_BrokerID THEN NewEndDate ELSE EndDate END\")) \\\n",
    "    .drop(\"LAST_SK\", \"NewEndDate\") \\\n",
    "    .createOrReplaceTempView(\"DimBrokerUpdated\")\n",
    "    \n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        INSERT INTO DimBroker(\n",
    "            SK_BrokerID, BrokerID, ManagerID, FirstName, LastName, MiddleInitial, Branch, Office, Phone, IsCurrent, BatchID, EffectiveDate, EndDate)\n",
    "        SELECT * FROM DimBrokerUpdated\n",
    "    \"\"\"\n",
    "    )\n",
    "    return spark.sql(\"SELECT * FROM DimBroker\")\n",
    "\n",
    "# spark.sql(\"DELETE FROM DimBroker\")\n",
    "# brokers = load_staging_hr_file(\"test\")\n",
    "# brokers.limit(10).toPandas()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Finwire Files \n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC #### Columnarize UDF Function\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col, udf, explode, map_keys, array, pandas_udf\n",
    "from pyspark.sql.types import StringType, Row\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def extract_finwire_type(finwire_str):\n",
    "    finwire_type = finwire_str.str[15:18]\n",
    "    return finwire_type\n",
    "\n",
    "@pandas_udf(\"\"\"\n",
    "            `PTS` string, `RecType` string, `CompanyName` string, `CIK` string, \n",
    "            `Status` string, `IndustryID` string , `SPrating` string, `FoundingDate` string,\n",
    "            `AddrLine1` string, `AddrLine2` string, `PostalCode` string, `City` string,\n",
    "            `StateProvince` string, `Country` string, `CEOname` string, `Description` string\n",
    "        \"\"\")\n",
    "def columnarize_finwire_data_cmp(finwire_str):\n",
    "    row = pd.DataFrame(columns=['PTS', 'RecType', 'CompanyName', 'CIK', 'Status',\n",
    "            'IndustryID', 'SPrating', 'FoundingDate',\n",
    "            'AddrLine1', 'AddrLine2', 'PostalCode', 'City',\n",
    "            'StateProvince', 'Country', 'CEOname', 'Description'])\n",
    "    row['PTS'] = finwire_str.str[0:15]\n",
    "    row['RecType'] = finwire_str.str[15:18]\n",
    "    row['CompanyName'] = finwire_str.str[18:78]\n",
    "    row['CIK'] = finwire_str.str[78:88]\n",
    "    row['Status'] = finwire_str.str[88:92]\n",
    "    row['IndustryID'] = finwire_str.str[92:94]\n",
    "    row['SDPrating'] = finwire_str.str[94:98]\n",
    "    row['FoundingDate'] = finwire_str.str[98:106]\n",
    "    row['AddrLine1'] = finwire_str.str[106:186]\n",
    "    row['AddrLine2'] = finwire_str.str[186:266]\n",
    "    row['PostalCode'] = finwire_str.str[266:278]\n",
    "    row['City'] = finwire_str.str[278:303]\n",
    "    row['StateProvince'] = finwire_str.str[303:323]\n",
    "    row['Country'] = finwire_str.str[323:347]\n",
    "    row['CEOname'] = finwire_str.str[347:393]\n",
    "    row['Description'] = finwire_str.str[393:]\n",
    "    return row\n",
    "\n",
    "\n",
    "@pandas_udf(\"\"\"\n",
    "            `PTS` string, `RecType` string, `Symbol` string, `IssueType` string, `Status` string, \n",
    "            `Name` string, `ExID` string, `ShOut` string, `FirstTradeDate` string, \n",
    "            `FirstTradeExchg` string, `Dividend` string, `CoNameOrCIK` string\n",
    "\"\"\")\n",
    "def columnarize_finwire_data_sec(finwire_str):\n",
    "    row = pd.DataFrame(columns=['PTS', 'RecType', 'Symbol', 'IssueType', 'Status', 'Name', 'ExID',\n",
    "                                'ShOut', 'FirstTradeDate', 'FirstTradeExchg', 'Dividend',\n",
    "                                'CoNameOrCIK'])\n",
    "    row['PTS'] = finwire_str.str[0:15]\n",
    "    row['RecType'] = finwire_str.str[15:18]\n",
    "    row['Symbol'] = finwire_str.str[18:33]\n",
    "    row['IssueType'] = finwire_str.str[33:39]\n",
    "    row['Status'] = finwire_str.str[39:43]\n",
    "    row['Name'] = finwire_str.str[43:113]\n",
    "    row['ExID'] = finwire_str.str[113:119]\n",
    "    row['ShOut'] = finwire_str.str[119:132]\n",
    "    row['FirstTradeDate'] = finwire_str.str[132:140]\n",
    "    row['FirstTradeExchg'] = finwire_str.str[140:148]\n",
    "    row['Dividend'] = finwire_str.str[148:160]\n",
    "    row['CoNameOrCIK'] = finwire_str.str[160:]\n",
    "    return row\n",
    "\n",
    "@pandas_udf(\"\"\"\n",
    "            `PTS` string, `RecType` string , `Year` string , `Quarter` string, `QtrStartDate` string,\n",
    "            `PostingDate` string, \n",
    "            `Revenue` string, `Earnings` string, `EPS` string , `DilutedEPS` string, `Margin` string,\n",
    "            `Inventory` string, `Assets` string,\n",
    "            `Liabilities` string, `ShOut` string, `DilutedShOut` string, `CoNameOrCIK` string\n",
    "\"\"\")\n",
    "def columnarize_finwire_data_fin(finwire_str):\n",
    "    row = pd.DataFrame(columns=['PTS', 'RecType', 'Year', 'Quarter', 'QtrStartDate', 'PostingDate', \n",
    "                                'Revenue', 'Earnings', 'EPS', 'DilutedEPS', 'Margin', 'Inventory', \n",
    "                                'Assets', 'Liabilities', 'ShOut', 'DilutedShOut', 'CoNameOrCIK'])\n",
    "    row['PTS'] = finwire_str.str[0:15]\n",
    "    row['RecType'] = finwire_str.str[15:18]\n",
    "    row['Year'] = finwire_str.str[18:22]\n",
    "    row['Quarter'] = finwire_str.str[22:23]\n",
    "    row['QtrStartDate'] = finwire_str.str[23:31]\n",
    "    row['PostingDate'] = finwire_str.str[31:39]\n",
    "    row['Revenue'] = finwire_str.str[39:56]\n",
    "    row['Earnings'] = finwire_str.str[56:73]\n",
    "    row['EPS'] = finwire_str.str[73:85]\n",
    "    row['DilutedEPS'] = finwire_str.str[85:97]\n",
    "    row['Margin'] = finwire_str.str[97:109]\n",
    "    row['Inventory'] = finwire_str.str[109:126]\n",
    "    row['Assets'] = finwire_str.str[126:143]\n",
    "    row['Liabilities'] = finwire_str.str[143:160]\n",
    "    row['ShOut'] = finwire_str.str[160:173]\n",
    "    row['DilutedShOut'] = finwire_str.str[173:186]\n",
    "    row['CoNameOrCIK'] = finwire_str.str[186:]\n",
    "    return row\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC #### Load CMP Files (DimCompany)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "def load_finwire_file(finwire_file_path, dbname, extract_type='CMP'):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    finwire = spark.read.format(\"text\").load(finwire_file_path)\\\n",
    "        .withColumn(\"RecType\", extract_finwire_type(col(\"value\")))\n",
    "\n",
    "    finwire_cmp = finwire.where(f\"RecType == 'CMP'\") \\\n",
    "        .withColumn(\"columnarized\", columnarize_finwire_data_cmp(\"value\")) \\\n",
    "        .select(\"columnarized.*\")\n",
    "    finwire_sec = finwire.where(f\"RecType == 'SEC'\") \\\n",
    "        .withColumn(\"columnarized\", columnarize_finwire_data_sec(\"value\")) \\\n",
    "        .select(\"columnarized.*\")\n",
    "    finwire_fin = finwire.where(f\"RecType == 'FIN'\") \\\n",
    "        .withColumn(\"columnarized\", columnarize_finwire_data_fin(\"value\")) \\\n",
    "        .select(\"columnarized.*\")\n",
    "    return finwire_cmp, finwire_sec, finwire_fin\n",
    "\n",
    "\n",
    "#from google.cloud import storage\n",
    "\n",
    "def load_finwire_files(dbname, scale_factor):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(f\"DROP TABLE DimCompany\")\n",
    "    create_dim_company(dbname)\n",
    "    \n",
    "    files_path = f\"{os.getcwd()}/data/{scale_factor}/Batch1/\"\n",
    "    files = os.listdir(files_path)\n",
    "    finwire_files = [file for file in files if \"FINWIRE\" in file and \"_audit\" not in file]\n",
    "    # First load the Finwire Data into dataframe\n",
    "    cmp = None\n",
    "    sec = None\n",
    "    fin = None\n",
    "    for i, finwire_file in enumerate(sorted(finwire_files)):\n",
    "        if i == 0:\n",
    "            finwire_file_path = files_path + finwire_file\n",
    "            cmp, sec, fin = load_finwire_file(finwire_file_path, \"test\")\n",
    "        else:\n",
    "            finwire_file_path = files_path + finwire_file\n",
    "            newcmp, newsec, newfin = load_finwire_file(finwire_file_path, \"test\")\n",
    "            cmp = cmp.union(newcmp)\n",
    "            sec = sec.union(newsec)\n",
    "            fin = fin.union(newfin)\n",
    "    cmp.orderBy(['CIK', 'PTS']).createOrReplaceTempView(\"finwire_cmp\")\n",
    "    sec.orderBy(['Symbol', 'PTS']).createOrReplaceTempView(\"finwire_sec\")\n",
    "    fin.createOrReplaceTempView(\"finwire_fin\")\n",
    "\n",
    "# load_finwire_files(\"test\", scale_Factor)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def load_finwires_into_dim_company(dbname, scale_factor):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "\n",
    "    # Now load industry.txt file\n",
    "#     industry_schema = \"`IN_ID` string, `IN_NAME` string, `IN_SC_ID` string\"\n",
    "#     industry = spark.read.format(\"csv\") \\\n",
    "#         .option(\"delimiter\", \"|\") \\\n",
    "#         .schema(industry_schema) \\\n",
    "#         .load(f\"gs://{bucket_name}/TPCDI_Data/TPCDI_Data/{scale_factor}/Batch1/Industry.txt\")\n",
    "#     industry.createOrReplaceTempView(\"industry\")\n",
    "    \n",
    "    # Now load status file\n",
    "    status_schema = \"`ST_ID` string, `ST_NAME` string\"\n",
    "    status = spark.read.format(\"csv\") \\\n",
    "        .option(\"delimiter\", \"|\") \\\n",
    "        .schema(status_schema) \\\n",
    "        .load(f\"{os.getcwd()}/data/{scale_factor}/Batch1/StatusType.txt\")\n",
    "    status.createOrReplaceTempView(\"status\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "    SELECT\n",
    "           monotonically_increasing_id() AS SK_CompanyID,\n",
    "           CIK AS CompanyID,\n",
    "           ST_NAME AS Status,\n",
    "           CompanyName AS Name,\n",
    "           IN_NAME AS Industry,\n",
    "           SPrating AS SPrating,\n",
    "           (SELECT CASE \n",
    "                WHEN SUBSTRING(SPrating, 1, 1) == 'A' OR SUBSTRING(SPrating, 1, 3) == 'BBB' THEN FALSE\n",
    "                ELSE TRUE\n",
    "            END) AS isLowGrade,\n",
    "           CEOname AS CEO,\n",
    "           AddrLine1 AS AddressLine1,\n",
    "           AddrLine2 AS AddressLine2,\n",
    "           PostalCode,\n",
    "           City,\n",
    "           StateProvince AS StateProv,\n",
    "           Country,\n",
    "           Description,\n",
    "           TO_DATE(FoundingDate, 'yyyyMMdd') AS FoundingDate,\n",
    "           (SELECT TRUE) AS IsCurrent,\n",
    "           (SELECT 1) AS BatchID,\n",
    "           (SELECT \n",
    "                 CASE WHEN TO_DATE(PTS, 'yyyyMMdd-kkmmss') IS NOT NULL \n",
    "                     THEN TO_DATE(PTS, 'yyyyMMdd-kkmmss')\n",
    "                 ELSE current_date()\n",
    "            END) AS EffectiveDate,\n",
    "           TO_DATE('9999-12-31') AS EndDate\n",
    "       FROM finwire_cmp, industry, status\n",
    "       WHERE\n",
    "           finwire_cmp.Status = status.ST_ID AND\n",
    "           finwire_cmp.IndustryID = industry.IN_ID\n",
    "    \"\"\").createOrReplaceTempView(\"DimCompanyNoUpdate\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            *, \n",
    "            MAX(SK_CompanyID) OVER (PARTITION BY CompanyID) AS LAST_SK, \n",
    "            LAG(EffectiveDate, -1) IGNORE NULLS OVER (PARTITION BY CompanyID ORDER BY EffectiveDate) \n",
    "                AS NewEndDate\n",
    "        FROM DimCompanyNoUpdate\n",
    "    \"\"\") \\\n",
    "    .withColumn(\"IsCurrent\", expr(\"CASE WHEN LAST_SK != SK_CompanyID THEN False ELSE TRUE END\")) \\\n",
    "    .withColumn(\n",
    "        \"EndDate\", \n",
    "        expr(\"CASE WHEN LAST_SK != SK_CompanyID THEN NewEndDate ELSE EndDate END\")) \\\n",
    "    .drop(\"LAST_SK\", \"NewEndDate\") \\\n",
    "    .createOrReplaceTempView(\"DimCompanyUpdated\")\n",
    "    \n",
    "    cast_to_target_schema(\"DimCompanyUpdated\", \"DimCompany\").createOrReplaceTempView(\"DimCompanyUpdated\")\n",
    "\n",
    "\n",
    "    # Insert data into dimension table\n",
    "    spark.sql(\"\"\"\n",
    "       INSERT INTO DimCompany(SK_CompanyID, CompanyID, Status, Name, Industry, SPrating, isLowGrade,\n",
    "                               CEO,\n",
    "                               AddressLine1, AddressLine2, PostalCode, City, StateProv, Country, \n",
    "                               Description, FoundingDate, IsCurrent, BatchID, EffectiveDate, EndDate)\n",
    "       SELECT * FROM DimCompanyUpdated\n",
    "    \"\"\")\n",
    "    return spark.sql(\"SELECT * FROM DimCompany\")\n",
    "\n",
    "# spark.sql(\"DELETE FROM DimCompany\")\n",
    "# finwires = load_finwires_into_dim_company(\"test\")\n",
    "# finwires.limit(5).toPandas()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC #### Load SEC Files (DimSecurity)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\"\"\"         \n",
    "            SK_SecurityID INTEGER,\n",
    "            Symbol CHAR(15),\n",
    "            Issue CHAR(6),\n",
    "            Status CHAR(10),\n",
    "            Name CHAR(70),\n",
    "            ExchangeID CHAR(6),\n",
    "            SK_CompanyID BIGINT,\n",
    "            SharesOutstanding BIGINT,\n",
    "            FirstTrade DATE,\n",
    "            FirstTradeOnExchange DATE,\n",
    "            Dividend FLOAT,\n",
    "            IsCurrent BOOLEAN,\n",
    "            BatchID INTEGER,\n",
    "            EffectiveDate DATE,\n",
    "            EndDate DATE\n",
    "            \n",
    "            `PTS` string, `RecType` string, `Symbol` string, `IssueType` string, `Status` string, \n",
    "            `Name` string, `ExID` string, `ShOut` string, `FirstTradeDate` string, \n",
    "            `FirstTradeExchg` string, `Dividend` string, `CoNameOrCIK` string\n",
    "\"\"\"\n",
    "def load_finwires_into_dim_security(dbname):\n",
    "\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(f\"DROP TABLE DimSecurity\")\n",
    "    create_dim_security(dbname)\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "            SELECT\n",
    "            monotonically_increasing_id() AS SK_SecurityID,\n",
    "            Symbol,\n",
    "            IssueType AS Issue,\n",
    "            ST_Name AS Status,\n",
    "            f.Name,\n",
    "            ExId as ExchangeID,\n",
    "            SK_CompanyID,\n",
    "            ShOut AS SharesOutstanding,\n",
    "            TO_DATE(FirstTradeDate, 'yyyyMMdd') AS FirstTrade,\n",
    "            TO_DATE(FirstTradeExchg, 'yyyyMMdd')AS FirstTradeOnExchange,\n",
    "            Dividend,\n",
    "            (SELECT TRUE) AS IsCurrent,\n",
    "            1 AS BatchID,\n",
    "            (SELECT \n",
    "                 CASE WHEN TO_DATE(PTS, 'yyyyMMdd-kkmmss') IS NOT NULL \n",
    "                     THEN TO_DATE(PTS, 'yyyyMMdd-kkmmss')\n",
    "                 ELSE current_date()\n",
    "            END) AS EffectiveDate,\n",
    "            TO_DATE('9999-12-31') AS EndDate\n",
    "        FROM finwire_sec f JOIN status s ON\n",
    "            (f.Status = s.ST_ID) JOIN DimCompany c ON\n",
    "            (c.CompanyID = f.CoNameOrCIK OR c.Name=f.CoNameOrCIK)\n",
    "    \"\"\").createOrReplaceTempView(\"DimSecurityNoUpdate\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            *, \n",
    "            MAX(SK_SecurityID) OVER (PARTITION BY Symbol) AS LAST_SK, \n",
    "            LAG(EffectiveDate, -1) IGNORE NULLS OVER (PARTITION BY Symbol ORDER BY EffectiveDate) \n",
    "                AS NewEndDate\n",
    "        FROM DimSecurityNoUpdate\n",
    "    \"\"\") \\\n",
    "    .withColumn(\"IsCurrent\", expr(\"CASE WHEN LAST_SK != SK_SecurityID THEN False ELSE TRUE END\")) \\\n",
    "    .withColumn(\n",
    "        \"EndDate\", \n",
    "        expr(\"CASE WHEN LAST_SK != SK_SecurityID THEN NewEndDate ELSE EndDate END\")) \\\n",
    "    .drop(\"LAST_SK\", \"NewEndDate\") \\\n",
    "    .createOrReplaceTempView(\"DimSecurityUpdated\")\n",
    "    \n",
    "    cast_to_target_schema(\"DimSecurityUpdated\", \"DimSecurity\").createOrReplaceTempView(\"DimSecurityUpdated\")\n",
    "\n",
    "    # Now insert values into dimSecurity\n",
    "    spark.sql(\"\"\"\n",
    "        INSERT INTO DimSecurity(SK_SecurityID, Symbol, Issue, Status, Name, ExchangeID, SK_CompanyID,\n",
    "                                SharesOutstanding, FirstTrade, FirstTradeOnExchange, Dividend,\n",
    "                                IsCurrent, BatchID, EffectiveDate, EndDate)\n",
    "        SELECT * FROM DimSecurityUpdated\n",
    "           \"\"\")\n",
    "    return spark.sql(\"SELECT * FROM DimSecurity\")\n",
    "\n",
    "# spark.sql(\"DELETE FROM DimSecurity\")\n",
    "# finwire_sec = load_finwires_into_dim_security(\"test\")\n",
    "# finwire_sec.limit(5).toPandas()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC #### Load FIN files (Financial table)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\"\"\"\n",
    "            `PTS` string, `RecType` string , `Year` string , `Quarter` string, `QtrStartDate` string,\n",
    "            `PostingDate` string, \n",
    "            `Revenue` string, `Earnings` string, `EPS` string , `DilutedEPS` string, `Margin` string,\n",
    "            `Inventory` string, `Assets` string,\n",
    "            `Liabilities` string, `ShOut` string, `DilutedShOut` string, `CoNameOrCIK` string\n",
    "        \"\"\"\n",
    "def load_finwires_into_financial_table(dbname):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    \n",
    "    cast_to_target_schema(\"finwire_fin\", \"Financial\").createOrReplaceTempView(\"finwire_fin\")\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "        INSERT INTO Financial(SK_CompanyID, FI_YEAR, FI_QTR, FI_QTR_START_DATE, FI_REVENUE,\n",
    "                              FI_NET_EARN, FI_BASIC_EPS, FI_DILUT_EPS, FI_MARGIN, FI_INVENTORY,\n",
    "                              FI_ASSETS, FI_LIABILITY, FI_OUT_BASIC, FI_OUT_DILUT)\n",
    "          SELECT DISTINCT\n",
    "              SK_CompanyID,\n",
    "              CAST(Year AS INT) AS FI_YEAR,\n",
    "              CAST(Quarter AS INT) AS FI_QTR,\n",
    "              TO_DATE(QtrStartDate, 'yyyyMMdd') AS FI_QTR_START_DATE,\n",
    "              CAST(Revenue AS FLOAT) AS FI_REVENUE,\n",
    "              CAST(Earnings AS FLOAT) AS FI_NET_EARN,\n",
    "              CAST(EPS AS FLOAT) AS FI_BASIC_EPS,\n",
    "              CAST(DilutedEPS AS FLOAT) AS FI_DILUT_EPS,\n",
    "              CAST(Margin AS FLOAT) AS FI_MARGIN,\n",
    "              CAST(Inventory AS FLOAT) AS FI_INVENTORY,\n",
    "              CAST(Assets AS FLOAT) AS FI_ASSETS,\n",
    "              cast(Liabilities AS FLOAT) AS FI_LIABILITY,\n",
    "              cast(ShOut AS FLOAT) AS FI_OUT_BASIC,\n",
    "              cast(DilutedSHOut AS FLOAT) AS FI_OUT_DILUT\n",
    "          FROM finwire_fin f \n",
    "          JOIN DimCompany c ON (c.CompanyID = f.CoNameOrCIK OR c.Name = f.CoNameOrCIK)\n",
    "    \"\"\")\n",
    "    return spark.sql(\"SELECT * FROM Financial\")\n",
    "\n",
    "# spark.sql(\"DELETE FROM Financial\")\n",
    "# financial = load_finwires_into_financial_table(\"test\")\n",
    "# financial.limit(5).toPandas()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Dim Trade\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\"\"\"\n",
    "    Loading status type and trade type into staging database\n",
    "\"\"\"\n",
    "\n",
    "def load_status_type(dbname, staging_area_folder):\n",
    "    #spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "        `ST_ID` String,\n",
    "        `ST_NAME` String\n",
    "    \"\"\"\n",
    "    status_type = spark.read.format(\"csv\").option(\"delimiter\", \"|\").schema(schema).load(f\"{staging_area_folder}/StatusType.txt\")\n",
    "    \n",
    "    status_type.createOrReplaceTempView(\"status_type\")\n",
    "    \n",
    "    spark.sql(\n",
    "    \"\"\"\n",
    "        INSERT INTO StatusType(ST_ID, ST_NAME)\n",
    "        SELECT ST_ID, ST_NAME FROM status_type\n",
    "    \"\"\")\n",
    "    return status_type\n",
    "\n",
    "\n",
    "def load_trade_type(dbname, staging_area_folder):\n",
    "    #spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "        `TT_ID` String,\n",
    "        `TT_NAME` String,\n",
    "        `TT_IS_SELL` INTEGER,\n",
    "        `TT_IS_MRKT` INTEGER\n",
    "    \"\"\"\n",
    "    trade_type = spark.read.format(\"csv\").option(\"delimiter\", \"|\").schema(schema).load(f\"{staging_area_folder}/TradeType.txt\")\n",
    "    \n",
    "    trade_type.createOrReplaceTempView(\"trade_type\")\n",
    "    \n",
    "    spark.sql(\n",
    "    \"\"\"\n",
    "        INSERT INTO TradeType(TT_ID, TT_NAME, TT_IS_SELL, TT_IS_MRKT)\n",
    "        SELECT TT_ID, TT_NAME, TT_IS_SELL, TT_IS_MRKT FROM trade_type\n",
    "    \"\"\")\n",
    "    return trade_type\n",
    "\n",
    "# trade_type = load_trade_type(\"test\")\n",
    "# trade_type.limit(3).toPandas()\n",
    "\n",
    "# status_type = load_status_type(\"test\")\n",
    "# status_type.limit(3).toPandas()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#TradeHistory.txt\n",
    "#The TradeHistory.txt file is a plain-text file with variable length fields separated by a vertical\n",
    "#bar (|). Records have a terminator character appropriate for the System Under Test. This\n",
    "#file is used only in the Historical Load.\n",
    "\n",
    "\n",
    "def load_trade_view(dbname, staging_area_folder):\n",
    "    #spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "            `T_ID` INTEGER,\n",
    "            `T_DTS` TIMESTAMP,\n",
    "            `T_ST_ID` String,\n",
    "            `T_TT_ID` String,\n",
    "            `T_IS_CASH`  Boolean,\n",
    "            `T_S_SYMB` String,\n",
    "            `T_QTY` Float,\n",
    "            `T_BID_PRICE` Float,\n",
    "            `T_CA_ID` String,\n",
    "            `T_EXEC_NAME` String,\n",
    "            `T_TRADE_PRICE` Float,\n",
    "            `T_CHRG` Float,\n",
    "            `T_COMM` Float,\n",
    "            `T_TAX` Float   \n",
    "    \"\"\"\n",
    "    \n",
    "    trade = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"|\")\n",
    "        .schema(schema)\n",
    "        .load(f\"{staging_area_folder}/Trade.txt\")\n",
    "        \n",
    "    )\n",
    "    # Save staging data into temp view\n",
    "    trade.createOrReplaceTempView(\"trade\")\n",
    "    \n",
    "    return trade\n",
    "    \n",
    "def load_tradehistory_view(dbname, staging_area_folder):\n",
    "    #spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "            `TH_T_ID` INTEGER,\n",
    "            `TH_DTS` TIMESTAMP,\n",
    "            `TH_ST_ID` String\n",
    "    \"\"\"\n",
    "    \n",
    "    trade_history = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"|\")\n",
    "        .schema(schema)\n",
    "        .load(f\"{staging_area_folder}/TradeHistory.txt\")\n",
    "        \n",
    "    )\n",
    "    # Save staging data into temp view\n",
    "    trade_history.createOrReplaceTempView(\"tradeHistory\")\n",
    "    \n",
    "    return trade_history\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## FactMarketHistory\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def load_staging_FactMarketStory(dbname, staging_area_folder):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "        `DM_DATE` DATE,\n",
    "        `DM_S_SYMB` STRING,\n",
    "        `DM_CLOSE` FLOAT,\n",
    "        `DM_HIGH` FLOAT,\n",
    "        `DM_LOW` FLOAT,\n",
    "        `DM_VOL` INTEGER\n",
    "    \"\"\"\n",
    "    DailyMarket_ = spark.read.format(\"csv\").option(\"delimiter\", \"|\").schema(schema).load(f\"{staging_area_folder}/DailyMarket.txt\")\n",
    "    DailyMarket_.createOrReplaceTempView(\"dailymarket\")\n",
    "\n",
    "    # TODO: DI Message\n",
    "    DailyMarket_ = spark.sql(\n",
    "        \"\"\"\n",
    "        WITH DailyMarket AS (\n",
    "            SELECT DM.*, MIN(dm2.DM_DATE) as FiftyTwoWeekHighDate, MIN(dm3.DM_DATE) as FiftyTwoWeekLowDate\n",
    "            FROM (\n",
    "             SELECT dm.DM_CLOSE,\n",
    "                dm.DM_S_SYMB,\n",
    "                dm.DM_HIGH,\n",
    "                dm.DM_LOW,\n",
    "                dm.DM_VOL,\n",
    "                dm.DM_DATE,\n",
    "                max(dm.DM_HIGH) OVER (\n",
    "                    PARTITION BY dm.DM_S_SYMB\n",
    "                    ORDER BY CAST(dm.DM_DATE AS timestamp)\n",
    "                    RANGE BETWEEN INTERVAL 364 DAYS PRECEDING AND CURRENT ROW\n",
    "                 ) AS FiftyTwoWeekHigh,\n",
    "                 min(dm.DM_LOW) OVER (\n",
    "                    PARTITION BY dm.DM_S_SYMB\n",
    "                    ORDER BY CAST(dm.DM_DATE AS timestamp)\n",
    "                    RANGE BETWEEN INTERVAL 364 DAYS PRECEDING AND CURRENT ROW\n",
    "                 ) AS FiftyTwoWeekLow\n",
    "                 FROM dailymarket dm\n",
    "            ) DM INNER JOIN dailymarket dm2 ON DM.FiftyTwoWeekHigh = dm2.DM_HIGH AND dm2.DM_DATE BETWEEN date_sub(DM.DM_DATE, 364) AND DM.DM_DATE\n",
    "         INNER JOIN dailymarket dm3 ON DM.FiftyTwoWeekLow = dm3.DM_LOW AND dm3.DM_DATE BETWEEN date_sub(DM.DM_DATE, 364) AND dm.DM_DATE\n",
    "            GROUP BY DM.DM_DATE, DM.DM_CLOSE, DM.DM_HIGH, DM.DM_LOW, DM.DM_VOL, DM.FiftyTwoWeekHigh, DM.FiftyTwoWeekLow, dm.DM_S_SYMB\n",
    "        ), FinData AS (\n",
    "            SELECT\n",
    "            SK_CompanyID,\n",
    "            SUM(FI_BASIC_EPS) OVER (\n",
    "                PARTITION BY FI_QTR\n",
    "                ORDER BY FI_YEAR, FI_QTR\n",
    "                ROWS BETWEEN 4 PRECEDING AND CURRENT ROW\n",
    "            ) as Eps\n",
    "            From Financial\n",
    "        ), CompEarning AS (\n",
    "            SELECT dc.CompanyID, fd.Eps\n",
    "            FROM DimCompany dc\n",
    "            INNER JOIN FinData fd ON dc.CompanyID = fd.SK_CompanyID\n",
    "        )\n",
    "        SELECT  cast(dm.DM_CLOSE as float) as ClosePrice,\n",
    "                cast(dm.DM_HIGH as float) as DayHigh,\n",
    "                cast(dm.DM_LOW as float) as DayLow,\n",
    "                cast(dm.DM_VOL as int) as Volume,\n",
    "                cast(ds.SK_SecurityID as int) as SK_SecurityID,\n",
    "                cast(ds.SK_CompanyID as int) as SK_CompanyID,\n",
    "                cast(dd1.SK_DateID as int) as SK_DateID,\n",
    "                cast(dd2.SK_DateID as int) as SK_FiftyTwoWeekHighDate,\n",
    "                cast(dd3.SK_DateID as int) as SK_FiftyTwoWeekLowDate,\n",
    "                cast(dm.FiftyTwoWeekHigh as float) as FiftyTwoWeekHigh,\n",
    "                cast(dm.FiftyTwoWeekLow as float) as FiftyTwoWeekLow,\n",
    "                cast(((ds.dividend / dm.DM_CLOSE) * 100.0) as float) as Yield,\n",
    "                CASE \n",
    "                    WHEN ISNULL(ce.Eps) or ce.Eps = 0 THEN NULL \n",
    "                    ELSE cast((dm.DM_CLOSE / ce.Eps) as float)\n",
    "                END as PERatio,\n",
    "                cast('1' as int) as BatchID\n",
    "        FROM DailyMarket dm\n",
    "        INNER JOIN DimSecurity ds ON ds.Symbol = dm.DM_S_SYMB AND dm.DM_DATE BETWEEN ds.EffectiveDate AND ds.EndDate\n",
    "        INNER JOIN DimDate dd1 ON dd1.DateValue = dm.DM_DATE\n",
    "        INNER JOIN DimDate dd2 ON dd2.DateValue = dm.FiftyTwoWeekHighDate\n",
    "        INNER JOIN DimDate dd3 ON dd3.DateValue = dm.FiftyTwoWeekLowDate\n",
    "        LEFT JOIN CompEarning ce ON ds.SK_CompanyID = ce.CompanyID\n",
    "         \"\"\")\n",
    "    \n",
    "    DailyMarket_.createOrReplaceTempView(\"dailymarket_insert\")\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "               INSERT INTO FactMarketHistory(ClosePrice, DayHigh, DayLow, Volume, SK_SecurityID, SK_CompanyID, \n",
    "                                            SK_DateID, SK_FiftyTwoWeekHighDate, SK_FiftyTwoWeekLowDate,  FiftyTwoWeekHigh, \n",
    "                                            FiftyTwoWeekLow, Yield, PERatio, BatchID)\n",
    "       SELECT * FROM dailymarket_insert\n",
    "    \"\"\")\n",
    "    \n",
    "    return spark.sql(\"\"\"\n",
    "       Select * from FactMarketHistory\n",
    "    \"\"\")\n",
    "\n",
    "# spark.sql(\"\"\"DELETE FROM FactMarketHistory\"\"\")\n",
    "# load_staging_FactMarketStory(\"test\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Prospect\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#Create prospect\n",
    "from pyspark.sql.functions import udf, struct\n",
    "from datetime import datetime\n",
    "\n",
    "def get_marketingnameplate(row):\n",
    "    result = []\n",
    "    \n",
    "    if (row.NetWorth and row.NetWorth > 1000000) or (row.Income and row.Income > 200000):\n",
    "        result.append(\"HighValue\")\n",
    "    if (row.NumberChildren and row.NumberChildren > 3) or ( row.NumberCreditCards and row.NumberCreditCards > 5):\n",
    "        result.append(\"Expenses\")\n",
    "    if (row.Age and row.Age > 45):\n",
    "        result.append(\"Boomer\")\n",
    "    if (row.Income and row.Income < 50000) or (row.CreditRating and row.CreditRating < 600) or (row.NetWorth and row.NetWorth < 100000):\n",
    "        result.append(\"MoneyAlert\")\n",
    "    if (row.NumberCars and row.NumberCars > 3) or (row.NumberCreditCards and row.NumberCreditCards > 7):\n",
    "        result.append(\"Spender\")\n",
    "    if (row.Age and row.Age < 25) and (row.NetWorth and row.NetWorth > 1000000):\n",
    "        result.append(\"Inherited\")\n",
    "    \n",
    "    return \"+\".join(result) if result else None\n",
    "\n",
    "\n",
    "def load_staging_Prospect(dbname, staging_area_folder):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    spark.sql(f\"DROP TABLE Prospect\")\n",
    "    create_prospect_table(dbname)\n",
    "\n",
    "    schema = \"\"\"\n",
    "        `AgencyID` String,\n",
    "        `LastName` String,\n",
    "        `FirstName` String,\n",
    "        `MiddleInitial` String,\n",
    "        `Gender` String,\n",
    "        `AddressLine1` String,\n",
    "        `AddressLine2` String,\n",
    "        `PostalCode` String,\n",
    "        `City` String,\n",
    "        `State` String,\n",
    "        `Country` String,\n",
    "        `Phone` String,\n",
    "        `Income` Integer,\n",
    "        `NumberCars` Integer,\n",
    "        `NumberChildren` Integer,\n",
    "        `MaritalStatus` String,\n",
    "        `Age` Integer,\n",
    "        `CreditRating` Integer,\n",
    "        `OwnOrRentFlag` String,\n",
    "        `Employer` String,\n",
    "        `NumberCreditCards` Integer,\n",
    "        `NetWorth` Integer\n",
    "    \"\"\"\n",
    "    Prospect_ = spark.read.format(\"csv\").option(\"delimiter\", \",\").schema(schema).load(f\"{staging_area_folder}/Prospect.csv\")\n",
    "    \n",
    "    udf_marketing = udf(lambda row: get_marketingnameplate(row), StringType())\n",
    "    Prospect_ = Prospect_.withColumn('MarketingNameplate', udf_marketing(struct([Prospect_[x] for x in Prospect_.columns])))\n",
    "    \n",
    "    now = datetime.utcnow()\n",
    "    \n",
    "    DimDate = spark.sql(\"\"\"\n",
    "        SELECT SK_DateID FROM DimDate WHERE SK_DateID = 20201231\n",
    "    \"\"\")\n",
    "    Prospect_ = Prospect_.crossJoin(DimDate)\n",
    "    Prospect_.createOrReplaceTempView(\"Prospect_\")\n",
    "    \n",
    "    spark.sql(\n",
    "    \"\"\"\n",
    "        INSERT INTO Prospect (\n",
    "               AgencyID, \n",
    "               BatchID,\n",
    "               IsCustomer,\n",
    "               SK_RecordDateID,\n",
    "               SK_UpdateDateID,\n",
    "               LastName,\n",
    "               FirstName,\n",
    "               MiddleInitial,\n",
    "               Gender,\n",
    "               AddressLine1,\n",
    "               AddressLine2,\n",
    "               PostalCode,\n",
    "               City,\n",
    "               State,\n",
    "               Country,\n",
    "               Phone,\n",
    "               Income,\n",
    "               NumberCars,\n",
    "               NumberChildren,\n",
    "               MaritalStatus,\n",
    "               Age,\n",
    "               CreditRating,\n",
    "               OwnOrRentFlag,\n",
    "               Employer,\n",
    "               NumberCreditCards,\n",
    "               NetWorth,\n",
    "               MarketingNameplate)\n",
    "        SELECT p.AgencyID, \n",
    "               1, \n",
    "               CASE\n",
    "                   WHEN dc.Status = 'ACTIVE' THEN True ELSE False\n",
    "               END,\n",
    "               p.SK_DateID,\n",
    "               p.SK_DateID,\n",
    "               p.LastName,\n",
    "               p.FirstName,\n",
    "               p.MiddleInitial,\n",
    "               p.Gender,\n",
    "               p.AddressLine1,\n",
    "               p.AddressLine2,\n",
    "               p.PostalCode,\n",
    "               p.City,\n",
    "               p.State,\n",
    "               p.Country,\n",
    "               p.Phone,\n",
    "               p.Income,\n",
    "               p.NumberCars,\n",
    "               p.NumberChildren,\n",
    "               p.MaritalStatus,\n",
    "               p.Age,\n",
    "               p.CreditRating,\n",
    "               p.OwnOrRentFlag,\n",
    "               p.Employer,\n",
    "               p.NumberCreditCards,\n",
    "               p.NetWorth,\n",
    "               p.MarketingNameplate\n",
    "        FROM Prospect_ p\n",
    "        LEFT JOIN DimCustomer dc ON \n",
    "        upper(p.FirstName) = upper(dc.FirstName) AND upper(p.LastName) = upper(dc.LastName)\n",
    "        AND upper(p.AddressLine1) = upper(dc.AddressLine1) AND upper(p.AddressLine2) = upper(dc.AddressLine2)\n",
    "        AND upper(p.PostalCode) = upper(dc.PostalCode)\n",
    "    \"\"\")\n",
    "    return Prospect_\n",
    "\n",
    "#Prospect_ = load_staging_Prospect(\"test\")\n",
    "#Prospect_.limit(3).toPandas()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Industry\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#Load industry \n",
    "\n",
    "def load_staging_Industry(dbname, staging_area_folder):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "        `IN_ID` String,\n",
    "        `IN_NAME` String,\n",
    "        `IN_SC_ID` String\n",
    "    \"\"\"\n",
    "    Industry_ = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"|\")\n",
    "        .schema(schema)\n",
    ".load(f\"{staging_area_folder}/Industry.txt\")       \n",
    "    )\n",
    "    Industry_.createOrReplaceTempView(\"industry\")\n",
    "    \n",
    "#     spark.sql(\n",
    "#     \"\"\"\n",
    "#         INSERT INTO Industry(IN_ID, IN_NAME, IN_SC_ID)\n",
    "#         SELECT i.IN_ID, i.IN_NAME, i.IN_SC_ID FROM industry i\n",
    "#     \"\"\")\n",
    "    return Industry_\n",
    "\n",
    "# spark.sql(\"USE test\")\n",
    "# spark.sql(\"DROP TABLE Industry\")\n",
    "# create_industry_table(\"test\")\n",
    "# Industry_ = load_staging_Industry(\"test\", f\"gs://tpcdi-with-spark-bdma/TPCDI_Data/TPCDI_Data/Scale3/Batch1\")\n",
    "# Industry_.limit(3).toPandas()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### XML Customer \n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType, FloatType, DateType, TimestampType\n",
    "import xml.etree.ElementTree as ET\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def customer_parser(rdd):\n",
    "    root = ET.fromstring(rdd[0])\n",
    "    records= []\n",
    "    for Action in root:\n",
    "        for Customer in Action:\n",
    "            ActionType = Action.attrib['ActionType']\n",
    "            if ActionType == 'NEW':\n",
    "                record = []\n",
    "                list_of_attributes = ['C_ID', 'C_TAX_ID', 'C_GNDR', 'C_TIER', 'C_DOB']\n",
    "                for attribute in list_of_attributes:\n",
    "                    try:\n",
    "                        record.append(Customer.attrib[attribute])\n",
    "                    except:\n",
    "                        record.append(None)\n",
    "                for Element in Customer:\n",
    "                    if Element.tag == 'ContactInfo':\n",
    "                        for Subelement in Element:\n",
    "                            if Subelement.tag[:-1] == 'C_PHONE_':\n",
    "                                phone_number = ''\n",
    "                                for Subsubelement in Subelement:\n",
    "                                    if isinstance(Subsubelement.text, str):                                \n",
    "                                        phone_number += Subsubelement.text + \" \"\n",
    "                                if len(phone_number)>1:\n",
    "                                    phone_number = phone_number[:-1]\n",
    "                                else:\n",
    "                                    phone_number = None\n",
    "                                record.append(phone_number)\n",
    "                            else:\n",
    "                                record.append(Subelement.text)\n",
    "                    elif Element.tag == 'Account':\n",
    "                        for attribute in Element.attrib.values():\n",
    "                            record.append(attribute)\n",
    "                        for Subelement in Element:\n",
    "                            record.append(Subelement.text)\n",
    "                    else:\n",
    "                        for Subelement in Element:\n",
    "                            record.append(Subelement.text)\n",
    "                records.append(record)\n",
    "    return records\n",
    "\n",
    "def add_account_parser(rdd):\n",
    "    root = ET.fromstring(rdd[0])\n",
    "    records= []\n",
    "    for Action in root:\n",
    "        for Customer in Action:\n",
    "            ActionType = Action.attrib['ActionType']\n",
    "            if ActionType == 'ADDACCT':\n",
    "                record = []\n",
    "                record.append(Customer.attrib['C_ID'])\n",
    "                for Element in Customer:\n",
    "                    if Element.tag == 'Account':\n",
    "                        for attribute in Element.attrib.values():\n",
    "                            record.append(attribute)\n",
    "                        for Subelement in Element:\n",
    "                            record.append(Subelement.text)\n",
    "                records.append(record)\n",
    "    return records\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#Update\n",
    "\n",
    "def update_customer_parser(rdd):\n",
    "    root = ET.fromstring(rdd[0])\n",
    "    records= []\n",
    "    for Action in root:\n",
    "        for Customer in Action:\n",
    "            ActionType = Action.attrib['ActionType']\n",
    "            if ActionType == 'UPDCUST':\n",
    "                record = []\n",
    "                list_of_attributes = ['C_ID', 'C_TAX_ID', 'C_GNDR', 'C_TIER', 'C_DOB']\n",
    "                for attribute in list_of_attributes:\n",
    "                    try:\n",
    "                        record.append(Customer.attrib[attribute])\n",
    "                    except:\n",
    "                        record.append(None)\n",
    "                for Element in Customer:\n",
    "                    dict={\n",
    "                    \"C_L_NAME\":None,\n",
    "                    \"C_F_NAME\":None,\n",
    "                    \"C_M_NAME\":None,\n",
    "                    'C_ADLINE1':None,\n",
    "                    'C_ADLINE2':None,\n",
    "                    'C_ZIPCODE':None,\n",
    "                    'C_CITY':None,\n",
    "                    'C_STATE_PROV':None,\n",
    "                    'C_CTRY':None,\n",
    "                    'C_PRIM_EMAIL':None,\n",
    "                    'C_ALT_EMAIL':None,\n",
    "                    'C_PHONE_1':None,\n",
    "                    'C_PHONE_2':None,\n",
    "                    'C_PHONE_3':None,\n",
    "                    \"C_LCL_TX_ID\":None,\n",
    "                    \"C_NAT_TX_ID\":None\n",
    "                    }\n",
    "                    if Element.tag == 'ContactInfo':\n",
    "                        for Subelement in Element:\n",
    "                            if Subelement.tag[:-1] == 'C_PHONE_':\n",
    "                                phone_number = ''\n",
    "                                for Subsubelement in Subelement:\n",
    "                                    if isinstance(Subsubelement.text, str):                                \n",
    "                                        phone_number += Subsubelement.text + \" \"\n",
    "                                if len(phone_number)>1:\n",
    "                                    phone_number = phone_number[:-1]\n",
    "                                else:\n",
    "                                    phone_number = None\n",
    "                                dict[Subelement.tag] = phone_number\n",
    "                            else:\n",
    "                                dict[Subelement.tag] = Subelement.text\n",
    "                    elif Element.tag == 'Account':\n",
    "                        continue\n",
    "                    else:\n",
    "                        for Subelement in Element:\n",
    "                            dict[Subelement.tag] = Subelement.text\n",
    "                records.append(record+list(dict.values()))\n",
    "    return records\n",
    "\n",
    "def update_account_parser(rdd):\n",
    "    root = ET.fromstring(rdd[0])\n",
    "    records= []\n",
    "    for Action in root:\n",
    "        for Customer in Action:\n",
    "            ActionType = Action.attrib['ActionType']\n",
    "            if ActionType == 'UPDACCT':\n",
    "                record = []\n",
    "                record.append(Customer.attrib['C_ID'])\n",
    "                dict = {\n",
    "                        \"CA_B_ID\":None,\n",
    "                        \"CA_NAME\":None}\n",
    "                \n",
    "                for Account in Customer:\n",
    "                    record.append(Account.attrib['CA_ID'])\n",
    "                    try:\n",
    "                        record.append(Account.attrib['CA_TAX_ST'])\n",
    "                    except:\n",
    "                        record.append(None)\n",
    "                        dict = {\n",
    "                        \"CA_B_ID\":None,\n",
    "                        \"CA_NAME\":None}\n",
    "                    for element in Account:\n",
    "                        dict[element.tag] = element.text\n",
    "                records.append(record+list(dict.values()))\n",
    "    return records\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#inactive\n",
    "\n",
    "def inactive_parser(rdd):\n",
    "    root = ET.fromstring(rdd[0])\n",
    "    records= []\n",
    "    for Action in root:\n",
    "        for Customer in Action:\n",
    "            ActionType = Action.attrib['ActionType']\n",
    "            if ActionType == 'INACT' or ActionType == 'CLOSEACCT':\n",
    "                records.append(Customer.attrib['C_ID'])\n",
    "    return records\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def load_customers(dbname, staging_area_folder):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    file_rdd = spark.read.text(f\"{staging_area_folder}/CustomerMgmt.xml\", wholetext=True).rdd\n",
    "    new_customer_records_rdd = file_rdd.flatMap(customer_parser)\n",
    "    new_customer_schema = StructType([\n",
    "        StructField(\"C_ID\", StringType(), False),\n",
    "        StructField(\"C_TAX_ID\", StringType(), False),\n",
    "        StructField(\"C_GNDR\", StringType(), True),\n",
    "        StructField(\"C_TIER\", StringType(), True),\n",
    "        StructField(\"C_DOB\", StringType(), False),\n",
    "        StructField(\"C_L_NAME\", StringType(), False),\n",
    "        StructField(\"C_F_NAME\", StringType(), False),\n",
    "        StructField(\"C_M_NAME\", StringType(), True),\n",
    "        StructField(\"C_ADLINE1\", StringType(), False),\n",
    "        StructField(\"C_ADLINE2\", StringType(), True),\n",
    "        StructField(\"C_ZIPCODE\", StringType(), False),\n",
    "        StructField(\"C_CITY\", StringType(), False),\n",
    "        StructField(\"C_STATE_PROV\", StringType(), False),\n",
    "        StructField(\"C_CTRY\", StringType(), False),\n",
    "        StructField(\"C_PRIM_EMAIL\", StringType(), False),\n",
    "        StructField(\"C_ALT_EMAIL\", StringType(), True),\n",
    "        StructField(\"C_PHONE_1\", StringType(), True),\n",
    "        StructField(\"C_PHONE_2\", StringType(), True),\n",
    "        StructField(\"C_PHONE_3\", StringType(), True),\n",
    "        StructField(\"C_LCL_TX_ID\", StringType(), False),\n",
    "        StructField(\"C_NAT_TX_ID\", StringType(), False),\n",
    "        StructField(\"CA_ID\", StringType(), False),\n",
    "        StructField(\"CA_TAX_ST\", StringType(), False),\n",
    "        StructField(\"CA_B_ID\", StringType(), False),\n",
    "        StructField(\"CA_NAME\", StringType(), True)])\n",
    "    customer_schema = StructType([\n",
    "        StructField(\"C_ID\", StringType(), True),\n",
    "        StructField(\"C_TAX_ID\", StringType(), True),\n",
    "        StructField(\"C_GNDR\", StringType(), True),\n",
    "        StructField(\"C_TIER\", StringType(), True),\n",
    "        StructField(\"C_DOB\", StringType(), True),\n",
    "        StructField(\"C_L_NAME\", StringType(), True),\n",
    "        StructField(\"C_F_NAME\", StringType(), True),\n",
    "        StructField(\"C_M_NAME\", StringType(), True),\n",
    "        StructField(\"C_ADLINE1\", StringType(), True),\n",
    "        StructField(\"C_ADLINE2\", StringType(), True),\n",
    "        StructField(\"C_ZIPCODE\", StringType(), True),\n",
    "        StructField(\"C_CITY\", StringType(), True),\n",
    "        StructField(\"C_STATE_PROV\", StringType(), True),\n",
    "        StructField(\"C_CTRY\", StringType(), True),\n",
    "        StructField(\"C_PRIM_EMAIL\", StringType(), True),\n",
    "        StructField(\"C_ALT_EMAIL\", StringType(), True),\n",
    "        StructField(\"C_PHONE_1\", StringType(), True),\n",
    "        StructField(\"C_PHONE_2\", StringType(), True),\n",
    "        StructField(\"C_PHONE_3\", StringType(), True),\n",
    "        StructField(\"C_LCL_TX_ID\", StringType(), True),\n",
    "        StructField(\"C_NAT_TX_ID\", StringType(), True)])\n",
    "    new_customer_df = new_customer_records_rdd.toDF(new_customer_schema).select(\"C_ID\", \"C_TAX_ID\", \"C_GNDR\", \"C_TIER\", \"C_DOB\", \"C_L_NAME\", \"C_F_NAME\", \"C_M_NAME\", \"C_ADLINE1\", \"C_ADLINE2\", \"C_ZIPCODE\", \"C_CITY\", \"C_STATE_PROV\", \"C_CTRY\", \"C_PRIM_EMAIL\", \"C_ALT_EMAIL\", \"C_PHONE_1\", \"C_PHONE_2\", \"C_PHONE_3\", \"C_LCL_TX_ID\", \"C_NAT_TX_ID\")\n",
    "    update_customer_rdd = file_rdd.flatMap(update_customer_parser)\n",
    "    update_customer_df = update_customer_rdd.toDF(customer_schema)\n",
    "    customers_not_updated = new_customer_df.join(update_customer_df, on=['C_ID'], how='left_anti')\n",
    "    customers_updated = new_customer_df.join(update_customer_df, on=['C_ID'], how='inner')\n",
    "    columns = []\n",
    "    for index, column in enumerate(customers_updated.columns):\n",
    "        if index <= 20:\n",
    "            columns.append(column)\n",
    "        else:\n",
    "            columns.append(column+'_update')\n",
    "\n",
    "    customers_updated = customers_updated.toDF(*columns).rdd\n",
    "    def customer_updater(row):\n",
    "        new_row= [row.C_ID]\n",
    "        for column in columns:\n",
    "            if column != 'C_ID' and (not '_update' in column):\n",
    "                if not getattr(row,column+'_update') is None:\n",
    "                    new_row.append(getattr(row,column+'_update'))\n",
    "                else:\n",
    "                    new_row.append(getattr(row,column))\n",
    "        return new_row\n",
    "    customers_updated = customers_updated.map(customer_updater).toDF(customer_schema)\n",
    "    Customers = customers_not_updated.union(customers_updated)\n",
    "    Customers.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "    dimCustomer = spark.sql(\"\"\"\n",
    "                       Select \n",
    "                       monotonically_increasing_id() AS SK_CustomerID,\n",
    "                       c.C_ID as CustomerID,\n",
    "                       C_TAX_ID as TaxID,\n",
    "                       C_L_NAME as LastName,\n",
    "                       C_F_NAME as FirstName,\n",
    "                       C_M_NAME as MiddleInitial,\n",
    "                       C_GNDR as Gender,\n",
    "                       C_TIER as Tier,\n",
    "                       C_DOB as DOB,\n",
    "                       C_ADLINE1 as AddressLine1,\n",
    "                       C_ADLINE2 as AddressLine2,\n",
    "                       C_ZIPCODE as PostalCode,\n",
    "                       C_CITY as City,\n",
    "                       C_STATE_PROV as StateProv,\n",
    "                       C_CTRY as Country,\n",
    "                       C_PHONE_1 as Phone1,\n",
    "                       C_PHONE_2 as Phone2,\n",
    "                       C_PHONE_3 as Phone3,\n",
    "                       C_PRIM_EMAIL as Email1,\n",
    "                       C_ALT_EMAIL as Email2,\n",
    "                       NAT.TX_NAME as NationalTaxRateDesc,\n",
    "                       NAT.TX_RATE as NationalTaxRate,\n",
    "                       LCL.TX_NAME as LocalTaxRateDesc,\n",
    "                       LCL.TX_RATE as LocalTaxRate,\n",
    "                       AgencyID as AgencyID,\n",
    "                       CreditRating as CreditRating,\n",
    "                       NetWorth as NetWorth,\n",
    "                        COALESCE(CASE \n",
    "                            WHEN NetWorth > 1000000 THEN 'HighValue+' \n",
    "                            ELSE NULL \n",
    "                        END,\n",
    "                       CASE \n",
    "                            WHEN NumberChildren > 3 THEN 'Expenses+' \n",
    "                            WHEN NumberCreditCards > 5 THEN 'Expenses+'\n",
    "                            ELSE NULL \n",
    "                        END,\n",
    "                       CASE \n",
    "                            WHEN Age > 45 THEN 'Boomer+' \n",
    "                            ELSE NULL \n",
    "                        END,\n",
    "                       CASE \n",
    "                            WHEN Income < 50000 THEN 'MoneyAlert+' \n",
    "                            WHEN CreditRating < 600 THEN 'MoneyAlert+' \n",
    "                            WHEN NetWorth < 100000 THEN 'MoneyAlert+' \n",
    "                            ELSE Null \n",
    "                        END,\n",
    "                       CASE \n",
    "                            WHEN NumberCars > 3 THEN 'Spender+' \n",
    "                            WHEN NumberCreditCards > 7 THEN 'Spender+' \n",
    "                            ELSE Null \n",
    "                        END,\n",
    "                       CASE \n",
    "                            WHEN Age < 25 THEN 'Inherited' \n",
    "                            WHEN NetWorth > 100000 THEN 'Inherited'  \n",
    "                            ELSE Null  \n",
    "                        END) as MarketingNameplate, \n",
    "                       CAST('True' as BOOLEAN) as IsCurrent, \n",
    "                       CAST('1' as INT) as BatchID, \n",
    "                       to_date('2015-01-01', 'yyyy-MM-dd') as EffectiveDate, \n",
    "                       to_date('9999-12-31', 'yyyy-MM-dd') as EndDate                       \n",
    "                       From customers as c \n",
    "                       left join TaxRate as NAT on c.C_NAT_TX_ID = NAT.TX_ID \n",
    "                       left join TaxRate as LCL on c.C_LCL_TX_ID = LCL.TX_ID \n",
    "                       left join Prospect as p on (c.C_L_NAME = p.LastName and c.C_F_NAME = p.FirstName \n",
    "                            and c.C_ADLINE1 = p.AddressLine1 and c.C_ADLINE2 =  p.AddressLine2 and c.C_ZIPCODE = p.PostalCode)\"\"\")\n",
    "\n",
    "    inactive_accounts = file_rdd.flatMap(inactive_parser)\n",
    "    inact_list = inactive_accounts.collect()\n",
    "    inact_func = udf(lambda x: 'Inactive' if str(x) in inact_list else 'Active')\n",
    "\n",
    "    dimCustomer = dimCustomer.withColumn('Status', inact_func(dimCustomer.CustomerID))\n",
    "    dimCustomer.createOrReplaceTempView(\"tbldimCustomer\")\n",
    "    \n",
    "    spark.sql(f\"DROP TABLE if exists DimCustomer\")\n",
    "    \n",
    "    dimCustomer.write.mode('overwrite').saveAsTable(\"DimCustomer\", mode=\"overwrite\")\n",
    "    return dimCustomer\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def load_account(dbname, staging_area_folder):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    file_rdd = spark.read.text(f\"{staging_area_folder}/CustomerMgmt.xml\", wholetext=True).rdd\n",
    "    new_customer_records_rdd = file_rdd.flatMap(customer_parser)\n",
    "    new_customer_schema = StructType([\n",
    "        StructField(\"C_ID\", StringType(), False),\n",
    "        StructField(\"C_TAX_ID\", StringType(), False),\n",
    "        StructField(\"C_GNDR\", StringType(), True),\n",
    "        StructField(\"C_TIER\", StringType(), True),\n",
    "        StructField(\"C_DOB\", StringType(), False),\n",
    "        StructField(\"C_L_NAME\", StringType(), False),\n",
    "        StructField(\"C_F_NAME\", StringType(), False),\n",
    "        StructField(\"C_M_NAME\", StringType(), True),\n",
    "        StructField(\"C_ADLINE1\", StringType(), False),\n",
    "        StructField(\"C_ADLINE2\", StringType(), True),\n",
    "        StructField(\"C_ZIPCODE\", StringType(), False),\n",
    "        StructField(\"C_CITY\", StringType(), False),\n",
    "        StructField(\"C_STATE_PROV\", StringType(), False),\n",
    "        StructField(\"C_CTRY\", StringType(), False),\n",
    "        StructField(\"C_PRIM_EMAIL\", StringType(), False),\n",
    "        StructField(\"C_ALT_EMAIL\", StringType(), True),\n",
    "        StructField(\"C_PHONE_1\", StringType(), True),\n",
    "        StructField(\"C_PHONE_2\", StringType(), True),\n",
    "        StructField(\"C_PHONE_3\", StringType(), True),\n",
    "        StructField(\"C_LCL_TX_ID\", StringType(), False),\n",
    "        StructField(\"C_NAT_TX_ID\", StringType(), False),\n",
    "        StructField(\"CA_ID\", StringType(), False),\n",
    "        StructField(\"CA_TAX_ST\", StringType(), False),\n",
    "        StructField(\"CA_B_ID\", StringType(), False),\n",
    "        StructField(\"CA_NAME\", StringType(), True)])\n",
    "    account_schema = StructType([\n",
    "        StructField(\"C_ID\", StringType(), True),\n",
    "        StructField(\"CA_ID\", StringType(), True),\n",
    "        StructField(\"CA_TAX_ST\", StringType(), True),\n",
    "        StructField(\"CA_B_ID\", StringType(), True),\n",
    "        StructField(\"CA_NAME\", StringType(), True)])\n",
    "\n",
    "    new_account_df = new_customer_records_rdd.toDF(new_customer_schema).select(\"C_ID\", \"CA_ID\", \"CA_TAX_ST\", \"CA_B_ID\", \"CA_NAME\")\n",
    "    add_account_records_rdd = file_rdd.flatMap(add_account_parser)\n",
    "    add_account_df = add_account_records_rdd.toDF(account_schema)\n",
    "    updated_account_rdd = file_rdd.flatMap(update_account_parser)\n",
    "    updated_account_df = updated_account_rdd.toDF(account_schema)\n",
    "    \n",
    "    Accounts = new_account_df.union(add_account_df).join(updated_account_df, on=['C_ID','CA_ID'], how='left_anti').union(updated_account_df)\n",
    "    inactive_accounts = file_rdd.flatMap(inactive_parser)\n",
    "    inact_list = inactive_accounts.collect()\n",
    "    inact_func = udf(lambda x: 'INAC' if str(x) in inact_list else 'ACTV')\n",
    "\n",
    "    Accounts = Accounts.withColumn('CA_ST_ID', inact_func(Accounts.C_ID))\n",
    "    \n",
    "    Accounts.createOrReplaceTempView(\"accounts\")\n",
    "\n",
    "    dimAccount = spark.sql(\"\"\" Select monotonically_increasing_id() AS SK_AccountID,\n",
    "                           CA_ID as AccountID,\n",
    "                           C_ID as SK_CustomerID,\n",
    "                           CA_B_ID as SK_BrokerID,\n",
    "                           ST_NAME as Status,\n",
    "                           CA_NAME as AccountDesc,\n",
    "                           CA_TAX_ST as TaxStatus,\n",
    "                           CAST('True' as BOOLEAN) as IsCurrent,\n",
    "                           CAST('1' as INT) as BatchID,\n",
    "                           to_date('2015-01-01', 'yyyy-MM-dd') as EffectiveDate, \n",
    "                           to_date('9999-12-31', 'yyyy-MM-dd') as EndDate \n",
    "                           From accounts join StatusType on accounts.CA_ST_ID = StatusType.ST_ID \"\"\")\n",
    "\n",
    "    dimAccount.createOrReplaceTempView(\"dimAccount_tbl\")\n",
    "    spark.sql(f\"DROP TABLE if exists DimAccount\")\n",
    "    dimAccount.write.mode('overwrite').saveAsTable( \"DimAccount\", mode=\"overwrite\")\n",
    "    return dimAccount\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Continue DimTrade\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# spark.sql(f\"DROP TABLE if exists DimTrade\")\n",
    "# create_dim_trade(\"test\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def load_staging_dim_trade(dbname, staging_area_folder):\n",
    "    trade_view = load_trade_view(dbname, staging_area_folder)\n",
    "    tradehistory_view = load_tradehistory_view(dbname, staging_area_folder)\n",
    "    \n",
    "    trade = spark.sql(\"\"\"\n",
    "            SELECT T.T_ID,\n",
    "                CASE WHEN (TH.TH_ST_ID = 'SBMT' AND T.T_TT_ID in ('TMS', 'TMB')) OR TH.TH_ST_ID = 'PNDG' THEN TH.TH_DTS ELSE NULL END as create_date,\n",
    "                CASE WHEN (TH.TH_ST_ID = 'SBMT' AND T.T_TT_ID in ('TMS', 'TMB')) OR TH.TH_ST_ID = 'PNDG' THEN TH.TH_DTS ELSE NULL END as create_time,\n",
    "                CASE WHEN TH.TH_ST_ID in ('CMPT', 'CNCL') THEN TH.TH_DTS ELSE NULL END as close_date,\n",
    "                CASE WHEN TH.TH_ST_ID in ('CMPT', 'CNCL') THEN TH.TH_DTS ELSE NULL END as close_time,\n",
    "             ST.ST_NAME,\n",
    "             TT.TT_NAME,\n",
    "             T.T_IS_CASH,\n",
    "             T.T_QTY,\n",
    "             T.T_BID_PRICE,\n",
    "             T.T_EXEC_NAME,\n",
    "             T.T_TRADE_PRICE,\n",
    "             T.T_CA_ID,\n",
    "             T.T_S_SYMB,\n",
    "             TH.TH_DTS,\n",
    "             T.T_CHRG,\n",
    "             T.T_COMM,\n",
    "             T.T_TAX\n",
    "             FROM trade T\n",
    "             INNER JOIN tradeHistory TH ON T.T_ID = TH.TH_T_ID\n",
    "             INNER JOIN StatusType ST ON T.T_ST_ID = ST.ST_ID\n",
    "             INNER JOIN TradeType TT ON T.T_TT_ID = TT.TT_ID\n",
    "    \"\"\")\n",
    "    create_date_dim = spark.sql(\"\"\"\n",
    "        SELECT SK_DateID as SK_CreateDateID, DateValue FROM DimDate\n",
    "    \"\"\")\n",
    "    create_time_dim = spark.sql(\"\"\"\n",
    "        SELECT SK_TimeID as SK_CreateTimeID, TimeValue FROM DimTime\n",
    "    \"\"\")\n",
    "    close_date_dim = spark.sql(\"\"\"\n",
    "        SELECT SK_DateID as SK_CloseDateID, DateValue FROM DimDate\n",
    "    \"\"\")\n",
    "    close_time_dim = spark.sql(\"\"\"\n",
    "        SELECT SK_TimeID as SK_CloseTimeID, TimeValue FROM DimTime\n",
    "    \"\"\")\n",
    "\n",
    "#     trade.show()\n",
    "    \n",
    "    trade = (\n",
    "    trade\n",
    "    .groupBy('T_ID', 'ST_NAME', 'TT_NAME', 'T_IS_CASH','T_CA_ID','T_S_SYMB','TH_DTS', 'T_QTY', 'T_BID_PRICE', 'T_EXEC_NAME', 'T_TRADE_PRICE', 'T_CHRG', 'T_COMM', 'T_TAX')\n",
    "    .agg(\n",
    "        collect_set(col('create_date')).alias('create_date'), \n",
    "        collect_set(col('create_time')).alias('create_time'),\n",
    "        collect_set(col('close_date')).alias('close_date'),\n",
    "        collect_set(col('close_time')).alias('close_time')\n",
    "    )\n",
    "    .select(\n",
    "        expr('filter(create_date, element -> element is not null)')[0].alias('create_date'),\n",
    "        expr('filter(create_time, element -> element is not null)')[0].alias('create_time'),\n",
    "        expr('filter(close_date, element -> element is not null)')[0].alias('close_date'),\n",
    "        expr('filter(close_time, element -> element is not null)')[0].alias('close_time'),\n",
    "'T_ID', 'ST_NAME', 'TT_NAME', 'T_IS_CASH','T_CA_ID','T_S_SYMB','TH_DTS', 'T_QTY', 'T_BID_PRICE', 'T_EXEC_NAME', 'T_TRADE_PRICE', 'T_CHRG', 'T_COMM', 'T_TAX'\n",
    "    )\n",
    ")\n",
    "\n",
    "    # Join with date\n",
    "    trade = trade.join(create_date_dim, to_date(create_date_dim.DateValue) == to_date(trade.create_date), \"left\").join(create_time_dim, date_format(create_time_dim.TimeValue, \"HH:mm:ss\") == date_format(trade.create_time, \"HH:mm:ss\"), \"left\").join(close_date_dim, to_date(close_date_dim.DateValue) == to_date(trade.create_date), \"left\").join(close_time_dim, date_format(close_time_dim.TimeValue, \"HH:mm:ss\") == date_format(trade.create_time, \"HH:mm:ss\"), \"left\")\n",
    "\n",
    "    # Create new view\n",
    "    trade.createOrReplaceTempView(\"trade_insert\")\n",
    "    #trade.printSchema()\n",
    "    \n",
    "    trade_final=spark.sql(\"\"\"\n",
    "        SELECT  T_ID as TradeID,\n",
    "         da.SK_BrokerID  as SK_BrokerID,\n",
    "         SK_CreateDateID as SK_CreateDateID,\n",
    "         SK_CreateTimeID as SK_CreateTimeID,\n",
    "         SK_CloseDateID as SK_CloseDateID,\n",
    "         SK_CloseTimeID as SK_CloseTimeID,\n",
    "         ST_NAME as Status,\n",
    "         TT_NAME as Type,\n",
    "         T_IS_CASH as CashFlag,\n",
    "         ds.SK_SecurityID as SK_SecurityID,\n",
    "         ds.SK_CompanyID as SK_CompanyID,\n",
    "         T_QTY as Quantity,\n",
    "         T_BID_PRICE as BidPrice,\n",
    "         da.SK_CustomerID as SK_CustomerID,\n",
    "         da.SK_AccountID as SK_AccountID,\n",
    "         T_EXEC_NAME as ExecutedBy,\n",
    "         T_TRADE_PRICE as TradePrice,\n",
    "         T_CHRG as Fee,\n",
    "         T_COMM as Comission,\n",
    "         T_TAX as Tax,\n",
    "         1 as BatchID\n",
    "        FROM trade_insert inner join DimAccount as da on trade_insert.T_CA_ID = da.SK_AccountID \n",
    "        AND trade_insert.TH_DTS BETWEEN da.EffectiveDate AND da.EndDate inner join DimSecurity\n",
    "        as ds on (trade_insert.T_S_SYMB = ds.Symbol AND trade_insert.TH_DTS BETWEEN ds.EffectiveDate AND ds.EndDate)\n",
    "    \"\"\")\n",
    "  \n",
    "    trade_final.createOrReplaceTempView(\"trade_insert\")\n",
    "    \n",
    "    trade_final.write.option(\"overwriteSchema\", \"true\").saveAsTable(\"DimTrade\", mode=\"overwrite\")\n",
    "    return spark.sql(\"\"\"SELECT * FROM DimTrade\"\"\")\n",
    "    \n",
    "# load_staging_dim_trade(\"test\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# dim_trade = spark.sql(\"\"\"\n",
    "#         SELECT * FROM DimTrade\n",
    "# \"\"\")\n",
    "#dim_trade.show()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Facts\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Fact Cash Balances\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def load_fact_cash_balances(dbname, staging_area_folder):\n",
    "    #spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "            `CT_CA_ID` INTEGER,\n",
    "            `CT_DTS` TIMESTAMP,\n",
    "            `CT_AMT` FLOAT,\n",
    "            `CT_NAME` STRING\n",
    "    \"\"\"\n",
    "    cash = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"|\")\n",
    "        .schema(schema)\n",
    "        .load(f\"{staging_area_folder}/CashTransaction.txt\")\n",
    "    )\n",
    "    \n",
    "    cash.createOrReplaceTempView(\"cashTrans\")\n",
    "    factCashBalances = spark.sql(\"\"\" \n",
    "                       Select SK_CustomerID, \n",
    "                           AccountID AS SK_AccountID, \n",
    "                           SK_DateID, \n",
    "                           sum(CT_AMT) as Cash, \n",
    "                           CAST('1' as INT) as BatchID \n",
    "                       From cashTrans join DimAccount as ac on (CT_CA_ID =ac.AccountID) \n",
    "                       join DimDate as dt on dt.DateValue = Date(CT_DTS) \n",
    "                       Group by AccountID, SK_CustomerID, SK_DateID\"\"\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    factCashBalances.write.option(\"overwriteSchema\", \"true\").saveAsTable(\"FactCashBalances\", mode=\"overwrite\")\n",
    "    factCashBalances.show(3)\n",
    "    return factCashBalances\n",
    "\n",
    "# load_fact_cash_balances(\"FactCashBalances\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Fact Holdings\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def load_fact_holdings(dbname, staging_area_folder):\n",
    "    #spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "            `HH_H_T_ID` INTEGER,\n",
    "            `HH_T_ID` INTEGER,\n",
    "            `HH_BEFORE_QTY` FLOAT,\n",
    "            `HH_AFTER_QTY` FLOAT\n",
    "    \"\"\"\n",
    "    holding = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"|\")\n",
    "        .schema(schema)\n",
    "        .load(f\"{staging_area_folder}/HoldingHistory.txt\")\n",
    "    )\n",
    "    holding.createOrReplaceTempView(\"holdings\")\n",
    "    factHoldings = spark.sql(\"\"\" \n",
    "                       Select \n",
    "                       SK_CustomerID, \n",
    "                       SK_AccountID, \n",
    "                       SK_SecurityID, \n",
    "                       SK_CompanyID,\n",
    "                       TradePrice as CurrentPrice,\n",
    "                       SK_CloseDateID as SK_DateID ,\n",
    "                       SK_CloseTimeID as SK_TimeID,\n",
    "                       HH_H_T_ID as TradeId,\n",
    "                       HH_T_ID as CurrentTradeID,\n",
    "                       HH_AFTER_QTY as CurrentHolding,\n",
    "                       CAST('1' as INT) as BatchID \n",
    "                       From holdings join DimTrade as ac on (HH_T_ID =ac.TradeID)\"\"\")\n",
    "    \n",
    "    \n",
    "    factHoldings.write.option(\"overwriteSchema\", \"true\").saveAsTable(\"FactHoldings\", mode=\"overwrite\")\n",
    "    factHoldings.show(2)\n",
    "    \n",
    "    return factHoldings\n",
    "\n",
    "# load_fact_holdings(\"FactHoldings\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md \n",
    "# MAGIC ### Fact Watches\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\"\"\"\n",
    "            SK_CustomerID BIGINT,\n",
    "            SK_SecurityID BIGINT,\n",
    "            SK_DateID_DatePlaced BIGINT,\n",
    "            SK_DateID_DateRemoved BIGINT,\n",
    "            BatchID INTEGER\n",
    "            \n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "def load_fact_watches(dbname, staging_area_folder):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    # Customer ID, Ticker symbol, Datetime, activate or cancel watch\n",
    "    schema = \"\"\"\n",
    "            `W_C_ID` BIGINT, \n",
    "            `W_S_SYMB` STRING,\n",
    "            `W_DTS` DATE,\n",
    "            `W_ACTION` STRING\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.read.format(\"csv\") \\\n",
    "        .option(\"delimiter\", \"|\") \\\n",
    "        .schema(schema) \\\n",
    "        .load(f\"{staging_area_folder}/WatchHistory.txt\") \\\n",
    "    .createOrReplaceTempView(\"watches\")\n",
    "    \n",
    "    actv_watches = spark.sql(\"SELECT * FROM watches\").where(col(\"W_ACTION\") == 'ACTV')\n",
    "    actv_watches.createOrReplaceTempView(\"actv_watches\")\n",
    "    cncl_watches = spark.sql(\"SELECT * FROM watches\").where(col(\"W_ACTION\") == 'CNCL')\n",
    "    cncl_watches.createOrReplaceTempView(\"cncl_watches\")\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT w1.W_C_ID, w1.W_S_SYMB, w1.W_DTS AS DatePlaced, w2.W_DTS AS DateRemoved FROM actv_watches w1 LEFT JOIN cncl_watches w2 ON w1.W_C_ID = w2.W_C_ID AND w1.W_S_SYMB = w2.W_S_SYMB\n",
    "    \"\"\").createOrReplaceTempView(\"watches\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "        INSERT INTO FactWatches(SK_CustomerID, SK_SecurityID, SK_DateID_DatePlaced, \n",
    "                                SK_DateID_DateRemoved, BatchID)\n",
    "            SELECT \n",
    "                c.SK_CustomerID,\n",
    "                s.SK_SecurityID,\n",
    "                d1.SK_DateID AS SK_DateID_DatePlaced, \n",
    "                d2.SK_DateID AS SK_DateID_DateRemoved,\n",
    "                1 AS BatchID\n",
    "            FROM watches w LEFT JOIN \n",
    "                DimDate d1 ON DatePlaced = d1.DateValue LEFT JOIN \n",
    "                DimDate d2 ON DateRemoved = d2.DateValue LEFT JOIN\n",
    "                DimSecurity s ON (\n",
    "                    W_S_SYMB = s.Symbol AND \n",
    "                    DatePlaced >= s.EffectiveDate AND\n",
    "                    DatePlaced < s.EndDate\n",
    "                ) LEFT JOIN \n",
    "                DimCustomer c ON (\n",
    "                    W_C_ID = c.CustomerID\n",
    "                )\n",
    "    \"\"\")\n",
    "    \n",
    "    return spark.sql(\"SELECT * FROM FactWatches\")\n",
    "\n",
    "# spark.sql(\"USE test\")\n",
    "# spark.sql(\"DELETE FROM FactWatches\")\n",
    "# watches= load_fact_watches(\"test\")\n",
    "# watches.limit(10).toPandas()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Run Historical Load\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# import time\n",
    "# import pandas as pd\n",
    "\n",
    "# def run_historical_load(scale_factors=[\"Scale3\"]):#, \"Scale4\", \"Scale5\", \"Scale6\"]):\n",
    "#     dbname = \"test\"\n",
    "#      # Options \"Scale3\"]):#, \"Scale4\", \"Scale5\", \"Scale6\"\n",
    "#     for scale_factor in scale_factors:\n",
    "#         metrics = {}\n",
    "#         # Init DB\n",
    "#         start = time.time()\n",
    "#         clean_warehouse(dbname)\n",
    "#         create_warehouse(dbname)\n",
    "#         end = time.time() - start\n",
    "        \n",
    "#         metrics[\"create_db_time\"] = end\n",
    "        \n",
    "#         staging_area_folder = f\"{os.getcwd()}/data/{scale_factor}/Batch1\"\n",
    "        \n",
    "#         # Run historical load\n",
    "#         start = time.time()\n",
    "#         dimdate = load_dim_date(dbname, staging_area_folder)\n",
    "#         dimtime = load_dim_time(dbname, staging_area_folder)\n",
    "#         taxrate = load_tax_rate(dbname, staging_area_folder)\n",
    "#         staginghr = load_staging_hr_file(dbname, staging_area_folder)\n",
    "#         industry = load_staging_Industry(dbname, staging_area_folder)\n",
    "\n",
    "        \n",
    "#         load_finwire_files(dbname, scale_factor)\n",
    "#         dimcompany = load_finwires_into_dim_company(dbname, scale_factor)\n",
    "#         dimsecurity = load_finwires_into_dim_security(dbname)\n",
    "#         fintable = load_finwires_into_financial_table(dbname)\n",
    "        \n",
    "#         statustype = load_status_type(dbname, staging_area_folder)\n",
    "#         tradetype = load_trade_type(dbname, staging_area_folder)\n",
    "        \n",
    "#         factmarkethistory =load_staging_FactMarketStory(dbname, staging_area_folder)\n",
    "#         prospect = load_staging_Prospect(dbname, staging_area_folder)\n",
    "        \n",
    "#         customer = load_customers(dbname, staging_area_folder)\n",
    "#         account = load_account(dbname, staging_area_folder)\n",
    "        \n",
    "#         dimtrade = load_staging_dim_trade(dbname, staging_area_folder)\n",
    "#         factcashbalance = load_fact_cash_balances(dbname, staging_area_folder)\n",
    "#         holding = load_fact_holdings(dbname, staging_area_folder)\n",
    "#         watch = load_fact_watches(dbname, staging_area_folder)\n",
    "#         end = time.time() - start\n",
    "        \n",
    "#         metrics[\"et\"] = end\n",
    "        \n",
    "#         dimdate_count = dimdate.count()\n",
    "#         dimtime_count = dimtime.count()\n",
    "#         taxrate_count = taxrate.count()\n",
    "#         staginghr_count = staginghr.count()\n",
    "#         dimcompany_count = dimcompany.count()\n",
    "#         dimsecurity_count = dimsecurity.count()\n",
    "#         fintable_count = fintable.count()\n",
    "#         statustype_count = statustype.count()\n",
    "#         tradetype_count = tradetype.count()\n",
    "#         factmarkethistory_count = factmarkethistory.count()\n",
    "#         prospect_count = prospect.count()\n",
    "#         industry_count = industry.count()\n",
    "#         dimtrade_count = dimtrade.count()\n",
    "#         factcashbalance_count = factcashbalance.count()\n",
    "#         holding_count = holding.count()\n",
    "#         watch_count = watch.count()\n",
    "#         customer_count = customer.count()\n",
    "#         account_count = account.count()\n",
    "\n",
    "#         # Sum the individual counts\n",
    "#         rows = dimdate_count + dimtime_count + taxrate_count + staginghr_count + dimcompany_count + dimsecurity_count + fintable_count + statustype_count + tradetype_count + factmarkethistory_count + prospect_count + industry_count + dimtrade_count + factcashbalance_count + holding_count + watch_count + customer_count + account_count\n",
    "\n",
    "#         metrics[\"rows\"] = rows\n",
    "#         metrics[\"throughput\"] = (rows / end)\n",
    "        \n",
    "#         metrics_df = pd.DataFrame(metrics, index=[0])\n",
    "\n",
    "#         metrics_df.to_csv(f\"{os.getcwd()}/results/data/historical_load_{scale_factor}.csv\", index=False)\n",
    "\n",
    "# run_historical_load()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# import pandas as pd\n",
    "# metrics = {'create_db_time': -66.36713218688965, 'et': -1484.7563452720642, 'rows': 22806424, 'throughput': -15360.381568748904}\n",
    "# metrics_df = pd.DataFrame(metrics, index=[0])\n",
    "# scale_factor = \"Scale3\"\n",
    "# client = create_gcs_client()\n",
    "# dbutils.fs.put(\"/FileStore/test.csv\", metrics_df.to_csv())\n",
    "# # metrics_df.to_csv(f\"/results/data/historical_load.csv\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# spark.sql(\"SELECT * FROM Industry\").toPandas()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 3: Incremental Update 1\n",
    "# MAGIC \n",
    "# MAGIC ### This step:\n",
    "# MAGIC - populates the data warehouse\n",
    "# MAGIC - applies transformations described in clause 4.6 of the tpc-di manual (batch2 folder on the generated data)\n",
    "# MAGIC - performs validations based on clause 7.4\n",
    "# MAGIC - upon completion of the validation stage, a phase completion record is written into DIMessages table\n",
    "# MAGIC - this step must be timed\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "staging_area_folder_up1 = f\"{os.getcwd()}/data/{scale_factor}/Batch2/\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC \n",
    "# MAGIC ### Transformation Details for Incremental Updates\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC \n",
    "# MAGIC ### DimenCustomer\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def load_dimen_customer(dbname, staging_area_folder_upl):\n",
    "    #spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "            `CDC_FLAG` STRING,\n",
    "            `CDC_DSN` LONG,\n",
    "            `C_ID` STRING,\n",
    "            `C_TAX_ID` STRING,\n",
    "            `C_ST_ID` STRING,\n",
    "            `C_L_NAME` STRING,\n",
    "            `C_F_NAME` STRING,\n",
    "            `C_M_NAME` STRING,\n",
    "            `C_GNDR` STRING,\n",
    "            `C_TIER` STRING,\n",
    "            `C_DOB` STRING,\n",
    "            `C_ADLINE1` STRING,\n",
    "            `C_ADLINE2` STRING,\n",
    "            `C_ZIPCODE` STRING,\n",
    "            `C_CITY` STRING,\n",
    "            `C_STATE_PROV` STRING,\n",
    "            `C_CTRY` STRING,\n",
    "            `C_CTRY_1` STRING,\n",
    "            `C_AREA_1` STRING,\n",
    "            `C_LOCAL_1` STRING,\n",
    "            `C_EXT_1` STRING,\n",
    "            `C_CTRY_2` STRING,\n",
    "            `C_AREA_2` STRING,\n",
    "            `C_LOCAL_2` STRING,\n",
    "            `C_EXT_2` STRING,\n",
    "            `C_CTRY_3` STRING,\n",
    "            `C_AREA_3` STRING,\n",
    "            `C_LOCAL_3` STRING,\n",
    "            `C_EXT_3` STRING,\n",
    "            `C_EMAIL_1` STRING,\n",
    "            `C_EMAIL_2` STRING,\n",
    "            `C_LCL_TX_ID` STRING,\n",
    "            `C_NAT_TX_ID` STRING\n",
    "    \"\"\"\n",
    "    customer_base = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"|\")\n",
    "        .schema(schema)\n",
    "        .load(f\"{staging_area_folder_upl}/Customer.txt\")\n",
    "    )\n",
    "    customer_base.createOrReplaceTempView(\"customer_base_batch1\")    \n",
    "    new_customer_df= spark.sql(\"\"\" Select *\n",
    "                           From customer_base_batch1 as a where a.CDC_FLAG = 'I'  \"\"\")\n",
    "\n",
    "    update_customer_df=spark.sql(\"\"\" Select *\n",
    "                           From customer_base_batch1 as a where a.CDC_FLAG = 'U'  \"\"\")\n",
    "    \n",
    "    Customers = new_customer_df.join(update_customer_df, on=['C_ID'], how='left_anti')\n",
    "    \n",
    "    Customers.createOrReplaceTempView(\"customers\")\n",
    "    \n",
    "    #### Added on line 2540 \"ST_NAME as Status, \"\n",
    "    dimCustomer = spark.sql(\"\"\"\n",
    "                       Select \n",
    "                       CDC_DSN AS SK_CustomerID,\n",
    "                       c.C_ID as CustomerID,\n",
    "                       C_TAX_ID as TaxID,\n",
    "                        ST_NAME as Status, \n",
    "                       C_L_NAME as LastName,\n",
    "                       C_F_NAME as FirstName,\n",
    "                       C_M_NAME as MiddleInitial,\n",
    "                       (CASE WHEN (C_GNDR = 'F' OR C_GNDR='M') THEN C_GNDR ELSE 'U' END) as Gender,\n",
    "                       C_TIER as Tier,\n",
    "                       C_DOB as DOB,\n",
    "                       C_ADLINE1 as AddressLine1,\n",
    "                       C_ADLINE2 as AddressLine2,\n",
    "                       C_ZIPCODE as PostalCode,\n",
    "                       C_CITY as City,\n",
    "                       C_STATE_PROV as StateProv,\n",
    "                       C_CTRY as Country,\n",
    "                       (\n",
    "                           CASE \n",
    "                           WHEN (C_CTRY_1 IS NOT NULL AND C_AREA_1 IS NOT NULL AND C_LOCAL_1 IS NOT NULL AND C_EXT_1 IS NULL) \n",
    "                           THEN CONCAT('+' , C_CTRY_1 , ' (' , C_AREA_1 , ') ' , C_LOCAL_1)\n",
    "                           WHEN (C_CTRY_1 IS NULL AND C_AREA_1 IS NOT NULL AND C_LOCAL_1 IS NOT NULL AND C_EXT_1 IS NULL) \n",
    "                           THEN CONCAT(' (' , C_AREA_1 , ') ' , C_LOCAL_1)\n",
    "                           WHEN (C_CTRY_1 IS NULL AND C_AREA_1 IS NULL AND C_LOCAL_1 IS NOT NULL AND C_EXT_1 IS NULL) \n",
    "                           THEN C_LOCAL_1\n",
    "                           \n",
    "                           WHEN (C_CTRY_1 IS NOT NULL AND C_AREA_1 IS NOT NULL AND C_LOCAL_1 IS NOT NULL AND C_EXT_1 IS NOT NULL) \n",
    "                           THEN CONCAT('+' , C_CTRY_1 , ' (' , C_AREA_1 , ') ' , C_LOCAL_1, C_EXT_1)\n",
    "                           WHEN (C_CTRY_1 IS NULL AND C_AREA_1 IS NOT NULL AND C_LOCAL_1 IS NOT NULL AND C_EXT_1 IS NOT NULL) \n",
    "                           THEN CONCAT(' (' , C_AREA_1 , ') ' , C_LOCAL_1, C_EXT_1)\n",
    "                           WHEN (C_CTRY_1 IS NULL AND C_AREA_1 IS NULL AND C_LOCAL_1 IS NOT NULL AND C_EXT_1 IS NOT NULL) \n",
    "                           THEN CONCAT(C_LOCAL_1, C_EXT_1)\n",
    "                           \n",
    "                           ELSE NULL\n",
    "                           END\n",
    "                       ) as Phone1,\n",
    "                       (\n",
    "                           CASE \n",
    "                           WHEN (C_CTRY_2 IS NOT NULL AND C_AREA_2 IS NOT NULL AND C_LOCAL_2 IS NOT NULL AND C_EXT_2 IS NULL) \n",
    "                           THEN CONCAT('+' , C_CTRY_2 , ' (' , C_AREA_2 , ') ' , C_LOCAL_2)\n",
    "                           WHEN (C_CTRY_2 IS NULL AND C_AREA_2 IS NOT NULL AND C_LOCAL_2 IS NOT NULL AND C_EXT_2 IS NULL) \n",
    "                           THEN CONCAT(' (' , C_AREA_2 , ') ' , C_LOCAL_2)\n",
    "                           WHEN (C_CTRY_2 IS NULL AND C_AREA_2 IS NULL AND C_LOCAL_2 IS NOT NULL AND C_EXT_2 IS NULL) \n",
    "                           THEN C_LOCAL_2\n",
    "                           \n",
    "                           WHEN (C_CTRY_2 IS NOT NULL AND C_AREA_2 IS NOT NULL AND C_LOCAL_2 IS NOT NULL AND C_EXT_2 IS NOT NULL) \n",
    "                           THEN CONCAT('+' , C_CTRY_2 , ' (' , C_AREA_2 , ') ' , C_LOCAL_2, C_EXT_2)\n",
    "                           WHEN (C_CTRY_2 IS NULL AND C_AREA_2 IS NOT NULL AND C_LOCAL_2 IS NOT NULL AND C_EXT_2 IS NOT NULL) \n",
    "                           THEN CONCAT(' (' , C_AREA_2 , ') ' , C_LOCAL_2, C_EXT_2)\n",
    "                           WHEN (C_CTRY_2 IS NULL AND C_AREA_2 IS NULL AND C_LOCAL_2 IS NOT NULL AND C_EXT_2 IS NOT NULL) \n",
    "                           THEN CONCAT(C_LOCAL_2, C_EXT_2)\n",
    "                           \n",
    "                           ELSE NULL\n",
    "                           END\n",
    "                       ) as Phone2,\n",
    "                       (\n",
    "                           CASE \n",
    "                           WHEN (C_CTRY_3 IS NOT NULL AND C_AREA_3 IS NOT NULL AND C_LOCAL_3 IS NOT NULL AND C_EXT_3 IS NULL) \n",
    "                           THEN CONCAT('+' , C_CTRY_3 , ' (' , C_AREA_3 , ') ' , C_LOCAL_3)\n",
    "                           WHEN (C_CTRY_3 IS NULL AND C_AREA_3 IS NOT NULL AND C_LOCAL_3 IS NOT NULL AND C_EXT_3 IS NULL) \n",
    "                           THEN CONCAT(' (' , C_AREA_3 , ') ' , C_LOCAL_3)\n",
    "                           WHEN (C_CTRY_3 IS NULL AND C_AREA_3 IS NULL AND C_LOCAL_3 IS NOT NULL AND C_EXT_3 IS NULL) \n",
    "                           THEN C_LOCAL_3\n",
    "                           \n",
    "                           WHEN (C_CTRY_3 IS NOT NULL AND C_AREA_3 IS NOT NULL AND C_LOCAL_3 IS NOT NULL AND C_EXT_3 IS NOT NULL) \n",
    "                           THEN CONCAT('+' , C_CTRY_3 , ' (' , C_AREA_3 , ') ' , C_LOCAL_3, C_EXT_3)\n",
    "                           WHEN (C_CTRY_3 IS NULL AND C_AREA_3 IS NOT NULL AND C_LOCAL_3 IS NOT NULL AND C_EXT_3 IS NOT NULL) \n",
    "                           THEN CONCAT(' (' , C_AREA_3 , ') ' , C_LOCAL_3, C_EXT_3)\n",
    "                           WHEN (C_CTRY_3 IS NULL AND C_AREA_3 IS NULL AND C_LOCAL_3 IS NOT NULL AND C_EXT_3 IS NOT NULL) \n",
    "                           THEN CONCAT(C_LOCAL_3, C_EXT_3)\n",
    "                           \n",
    "                           ELSE NULL\n",
    "                           END\n",
    "                       ) as Phone3,\n",
    "                       C_EMAIL_1 as Email1,\n",
    "                       C_EMAIL_2 as Email2,\n",
    "                       NAT.TX_NAME as NationalTaxRateDesc,\n",
    "                       NAT.TX_RATE as NationalTaxRate,\n",
    "                       LCL.TX_NAME as LocalTaxRateDesc,\n",
    "                       LCL.TX_RATE as LocalTaxRate,\n",
    "                       AgencyID as AgencyID,\n",
    "                       CreditRating as CreditRating,\n",
    "                       NetWorth as NetWorth,\n",
    "                        COALESCE(CASE \n",
    "                            WHEN NetWorth > 1000000 THEN 'HighValue+' \n",
    "                            ELSE NULL \n",
    "                        END,\n",
    "                       CASE \n",
    "                            WHEN NumberChildren > 3 THEN 'Expenses+' \n",
    "                            WHEN NumberCreditCards > 5 THEN 'Expenses+'\n",
    "                            ELSE NULL \n",
    "                        END,\n",
    "                       CASE \n",
    "                            WHEN Age > 45 THEN 'Boomer+' \n",
    "                            ELSE NULL \n",
    "                        END,\n",
    "                       CASE \n",
    "                            WHEN Income < 50000 THEN 'MoneyAlert+' \n",
    "                            WHEN CreditRating < 600 THEN 'MoneyAlert+' \n",
    "                            WHEN NetWorth < 100000 THEN 'MoneyAlert+' \n",
    "                            ELSE Null \n",
    "                        END,\n",
    "                       CASE \n",
    "                            WHEN NumberCars > 3 THEN 'Spender+' \n",
    "                            WHEN NumberCreditCards > 7 THEN 'Spender+' \n",
    "                            ELSE Null \n",
    "                        END,\n",
    "                       CASE \n",
    "                            WHEN Age < 25 THEN 'Inherited' \n",
    "                            WHEN NetWorth > 100000 THEN 'Inherited'  \n",
    "                            ELSE Null  \n",
    "                        END) as MarketingNameplate, \n",
    "                       CAST('True' as BOOLEAN) as IsCurrent, \n",
    "                       CAST('2' as INT) as BatchID, \n",
    "                       to_date('2015-01-01', 'yyyy-MM-dd') as EffectiveDate, \n",
    "                       to_date('9999-12-31', 'yyyy-MM-dd') as EndDate\n",
    "                       From customers as c \n",
    "                       left join TaxRate as NAT on c.C_NAT_TX_ID = NAT.TX_ID \n",
    "                       left join TaxRate as LCL on c.C_LCL_TX_ID = LCL.TX_ID \n",
    "                       left join Prospect as p on (c.C_L_NAME = p.LastName and c.C_F_NAME = p.FirstName \n",
    "                            and c.C_ADLINE1 = p.AddressLine1 and c.C_ADLINE2 =  p.AddressLine2 and c.C_ZIPCODE = p.PostalCode)\n",
    "                        left join StatusType on StatusType.ST_ID = c.C_ST_ID \"\"\")\n",
    "    \n",
    "    dimCustomer.createOrReplaceTempView(\"dimCustomer_stream\")\n",
    "\n",
    "    dimCustomer = cast_to_target_schema(\"dimCustomer_stream\", \"DimCustomer\")\n",
    "     \n",
    "    dimCustomer.write.mode(\"append\").saveAsTable( \"DimCustomer\", mode=\"append\")\n",
    "    \n",
    "    return dimCustomer\n",
    "    \n",
    "# load_dimen_customer(\"test\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC \n",
    "# MAGIC ### DimenAccount\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def load_dimen_account(dbname, staging_area_folder_upl):\n",
    "    #spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "            `CDC_FLAG` STRING,\n",
    "            `CDC_DSN` INTEGER,\n",
    "            `CA_ID` LONG,\n",
    "            `CA_B_ID` STRING,\n",
    "            `CA_C_ID` STRING,\n",
    "            `CA_NAME` STRING,\n",
    "            `CA_TAX_ST` STRING,\n",
    "            `CA_ST_ID` STRING\n",
    "    \"\"\"\n",
    "    account_base = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"|\")\n",
    "        .schema(schema)\n",
    "        .load(f\"{staging_area_folder_upl}/Account.txt\")\n",
    "    )\n",
    "    account_base.createOrReplaceTempView(\"account_base_batch1\")\n",
    "    \n",
    "    account_base.show(2)\n",
    "    \n",
    "    add_account_df= spark.sql(\"\"\" Select *\n",
    "                           From account_base_batch1 as a where a.CDC_FLAG = 'I'  \"\"\")\n",
    "\n",
    "    updated_account_df=spark.sql(\"\"\" Select *\n",
    "                           From account_base_batch1 as a where a.CDC_FLAG = 'U'  \"\"\")\n",
    "    \n",
    "    #new_account_df = new_customer_records_rdd.toDF(new_customer_schema).select(\"C_ID\", \"CA_ID\", \"CA_TAX_ST\", \"CA_B_ID\", \"CA_NAME\")\n",
    "\n",
    "    Accounts = account_base.union(add_account_df).join(updated_account_df, on=['CA_ID'], how='left_anti').union(updated_account_df)\n",
    "    Accounts.createOrReplaceTempView(\"accounts\")\n",
    "    \n",
    "    dimAccount = spark.sql(\"\"\" Select CDC_DSN AS SK_AccountID,\n",
    "                           CA_ID as AccountID,\n",
    "                           CA_C_ID as SK_CustomerID,\n",
    "                           CA_B_ID as SK_BrokerID,\n",
    "                           ST_NAME as Status,\n",
    "                           CA_NAME as AccountDesc,\n",
    "                           CA_TAX_ST as TaxStatus,\n",
    "                           CAST('True' as BOOLEAN) as IsCurrent,\n",
    "                           CAST('2' as INT) as BatchID,\n",
    "                           to_date('2015-01-01', 'yyyy-MM-dd') as EffectiveDate, \n",
    "                           to_date('9999-12-31', 'yyyy-MM-dd') as EndDate \n",
    "                           From accounts join StatusType on accounts.CA_ST_ID = StatusType.ST_ID \"\"\")\n",
    "\n",
    "    #dimAccount.printSchema()\n",
    "    \n",
    "    dimAccount.createOrReplaceTempView(\"dimAccount_stream\")\n",
    "    dimAccount = cast_to_target_schema(\"dimAccount_stream\", \"DimAccount\")\n",
    "\n",
    "    dimAccount.write.mode(\"append\").saveAsTable( \"DimAccount\", mode=\"append\")\n",
    "    \n",
    "    return dimAccount\n",
    "    \n",
    "# load_dimen_account(\"test\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### DimTrade\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def load_update_dimen_trade(dbname,staging_area_folder_up1):\n",
    "    #spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "            `CDC_FLAG` String,\n",
    "            `CDC_DSN` String,\n",
    "            `T_ID` INTEGER,\n",
    "            `T_DTS` TIMESTAMP,\n",
    "            `T_ST_ID` String,\n",
    "            `T_TT_ID` String,\n",
    "            `T_IS_CASH`  Boolean,\n",
    "            `T_S_SYMB` String,\n",
    "            `T_QTY` Float,\n",
    "            `T_BID_PRICE` Float,\n",
    "            `T_CA_ID` String,\n",
    "            `T_EXEC_NAME` String,\n",
    "            `T_TRADE_PRICE` Float,\n",
    "            `T_CHRG` Float,\n",
    "            `T_COMM` Float,\n",
    "            `T_TAX` Float\n",
    "    \"\"\"\n",
    "    trade_base = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"|\")\n",
    "        .schema(schema)\n",
    "        .load(f\"{staging_area_folder_up1}/Trade.txt\")\n",
    "    )\n",
    "    trade_base.createOrReplaceTempView(\"trade_base_batch1\")\n",
    "        \n",
    "    add_trade_df= spark.sql(\"\"\" Select *\n",
    "                           From trade_base_batch1 as a where a.CDC_FLAG = 'I'  \"\"\")\n",
    "\n",
    "    updated_trade_df=spark.sql(\"\"\" Select *\n",
    "                           From trade_base_batch1 as a where a.CDC_FLAG = 'U'  \"\"\")\n",
    "    \n",
    "\n",
    "    trades = trade_base.union(add_trade_df).join(updated_trade_df, on=['T_ID'], how='left_anti').union(updated_trade_df)\n",
    "    trades.createOrReplaceTempView(\"trade_view\")\n",
    "    \n",
    "    \n",
    "    trade = spark.sql(\"\"\"\n",
    "            SELECT T.T_ID,\n",
    "                CASE WHEN T.CDC_FLAG = 'I' then TH.TH_DTS ELSE NULL END as create_date,\n",
    "                CASE WHEN T.CDC_FLAG = 'I' then TH.TH_DTS ELSE NULL END as create_time,\n",
    "                CASE \n",
    "                WHEN T.CDC_FLAG = 'I' then NULL \n",
    "                WHEN TH.TH_ST_ID in ('CMPT', 'CNCL') THEN TH.TH_DTS ELSE NULL END as close_date,\n",
    "                CASE WHEN T.CDC_FLAG = 'I' then NULL \n",
    "                WHEN TH.TH_ST_ID in ('CMPT', 'CNCL') THEN TH.TH_DTS ELSE NULL END as close_time,\n",
    "             ST.ST_NAME,\n",
    "             TT.TT_NAME,\n",
    "             T.T_IS_CASH,\n",
    "             T.T_QTY,\n",
    "             T.T_BID_PRICE,\n",
    "             T.T_EXEC_NAME,\n",
    "             T.T_TRADE_PRICE,\n",
    "             T.T_CA_ID,\n",
    "             T.T_S_SYMB,\n",
    "             TH.TH_DTS,\n",
    "             T.T_CHRG,\n",
    "             T.T_COMM,\n",
    "             T.T_TAX\n",
    "             FROM trade_view T\n",
    "             INNER JOIN tradeHistory TH ON T.T_ID = TH.TH_T_ID\n",
    "             INNER JOIN StatusType ST ON T.T_ST_ID = ST.ST_ID\n",
    "             INNER JOIN TradeType TT ON T.T_TT_ID = TT.TT_ID\n",
    "    \"\"\")\n",
    "    create_date_dim = spark.sql(\"\"\"\n",
    "        SELECT SK_DateID as SK_CreateDateID, DateValue FROM DimDate\n",
    "    \"\"\")\n",
    "    create_time_dim = spark.sql(\"\"\"\n",
    "        SELECT SK_TimeID as SK_CreateTimeID, TimeValue FROM DimTime\n",
    "    \"\"\")\n",
    "    close_date_dim = spark.sql(\"\"\"\n",
    "        SELECT SK_DateID as SK_CloseDateID, DateValue FROM DimDate\n",
    "    \"\"\")\n",
    "    close_time_dim = spark.sql(\"\"\"\n",
    "        SELECT SK_TimeID as SK_CloseTimeID, TimeValue FROM DimTime\n",
    "    \"\"\")\n",
    "\n",
    "#     trade.show()\n",
    "    \n",
    "    trade = (\n",
    "    trade\n",
    "    .groupBy( 'T_ID', 'ST_NAME', 'TT_NAME', 'T_IS_CASH','T_CA_ID','T_S_SYMB','TH_DTS', 'T_QTY', 'T_BID_PRICE', 'T_EXEC_NAME', 'T_TRADE_PRICE', 'T_CHRG', 'T_COMM', 'T_TAX')\n",
    "    .agg(\n",
    "        collect_set(col('create_date')).alias('create_date'), \n",
    "        collect_set(col('create_time')).alias('create_time'),\n",
    "        collect_set(col('close_date')).alias('close_date'),\n",
    "        collect_set(col('close_time')).alias('close_time')\n",
    "    )\n",
    "    .select(\n",
    "        expr('filter(create_date, element -> element is not null)')[0].alias('create_date'),\n",
    "        expr('filter(create_time, element -> element is not null)')[0].alias('create_time'),\n",
    "        expr('filter(close_date, element -> element is not null)')[0].alias('close_date'),\n",
    "        expr('filter(close_time, element -> element is not null)')[0].alias('close_time'),\n",
    "'T_ID', 'ST_NAME', 'TT_NAME', 'T_IS_CASH','T_CA_ID','T_S_SYMB','TH_DTS', 'T_QTY', 'T_BID_PRICE', 'T_EXEC_NAME', 'T_TRADE_PRICE', 'T_CHRG', 'T_COMM', 'T_TAX'\n",
    "    )\n",
    ")\n",
    "\n",
    "    # Join with date\n",
    "    trade = trade.join(create_date_dim, to_date(create_date_dim.DateValue) == to_date(trade.create_date), \"left\").join(create_time_dim, date_format(create_time_dim.TimeValue, \"HH:mm:ss\") == date_format(trade.create_time, \"HH:mm:ss\"), \"left\").join(close_date_dim, to_date(close_date_dim.DateValue) == to_date(trade.create_date), \"left\").join(close_time_dim, date_format(close_time_dim.TimeValue, \"HH:mm:ss\") == date_format(trade.create_time, \"HH:mm:ss\"), \"left\")\n",
    "\n",
    "    # Create new view\n",
    "    trade.createOrReplaceTempView(\"trade_insert\")\n",
    "    #trade.printSchema()\n",
    "    \n",
    "    dimTrade=spark.sql(\"\"\"\n",
    "        SELECT  INT(T_ID) as TradeID,\n",
    "         da.SK_BrokerID  as SK_BrokerID,\n",
    "         SK_CreateDateID as SK_CreateDateID,\n",
    "         SK_CreateTimeID as SK_CreateTimeID,\n",
    "         SK_CloseDateID as SK_CloseDateID,\n",
    "         SK_CloseTimeID as SK_CloseTimeID,\n",
    "         ST_NAME as Status,\n",
    "         TT_NAME as Type,\n",
    "         T_IS_CASH as CashFlag,\n",
    "         ds.SK_SecurityID as SK_SecurityID,\n",
    "         ds.SK_CompanyID as SK_CompanyID,\n",
    "         T_QTY as Quantity,\n",
    "         T_BID_PRICE as BidPrice,\n",
    "         da.SK_CustomerID as SK_CustomerID,\n",
    "         da.SK_AccountID as SK_AccountID,\n",
    "         T_EXEC_NAME as ExecutedBy,\n",
    "         T_TRADE_PRICE as TradePrice,\n",
    "         T_CHRG as Fee,\n",
    "         T_COMM as Comission,\n",
    "         T_TAX as Tax,\n",
    "         2 as BatchID\n",
    "        FROM trade_insert inner join DimAccount as da on trade_insert.T_CA_ID = da.SK_AccountID \n",
    "        AND trade_insert.TH_DTS BETWEEN da.EffectiveDate AND da.EndDate inner join DimSecurity\n",
    "        as ds on (trade_insert.T_S_SYMB = ds.Symbol AND trade_insert.TH_DTS BETWEEN ds.EffectiveDate AND ds.EndDate)\n",
    "    \"\"\")\n",
    "  \n",
    "    #dimAccount.printSchema()\n",
    "    \n",
    "    dimTrade.write.mode(\"append\").saveAsTable( \"DimTrade\", mode=\"append\")\n",
    "    \n",
    "    return dimTrade\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Facts\n",
    "# MAGIC ### Fact Cash Balances\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def load_update_fact_cash_balances(dbname, staging_area_folder_upl):\n",
    "    #spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "            `CDC_FLAG` STRING,\n",
    "            `CDC_DSN` INTEGER,\n",
    "            `CT_CA_ID` INTEGER,\n",
    "            `CT_DTS` TIMESTAMP,\n",
    "            `CT_AMT` FLOAT,\n",
    "            `CT_NAME` STRING\n",
    "    \"\"\"\n",
    "    cash = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"|\")\n",
    "        .schema(schema)\n",
    "        .load(f\"{staging_area_folder_upl}/CashTransaction.txt\")\n",
    "    )\n",
    "    \n",
    "    cash.createOrReplaceTempView(\"cashTrans\")\n",
    "    factCashBalances = spark.sql(\"\"\" \n",
    "                       Select SK_CustomerID, \n",
    "                           AccountID AS SK_AccountID, \n",
    "                           SK_DateID, \n",
    "                           sum(CT_AMT) as Cash, \n",
    "                           CAST('2' as INT) as BatchID \n",
    "                       From cashTrans join DimAccount as ac on (CT_CA_ID =ac.AccountID) \n",
    "                       join DimDate as dt on dt.DateValue = Date(CT_DTS) \n",
    "                       Group by AccountID, SK_CustomerID, SK_DateID\"\"\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    factCashBalances.write.option(\"append\", \"true\").saveAsTable(\"FactCashBalances\", mode=\"append\")\n",
    "#     factCashBalances.show(3)\n",
    "    return factCashBalances\n",
    "\n",
    "# load_fact_cash_balances(\"FactCashBalances\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Fact Holdings\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def load_update_fact_holdings(dbname, staging_area_folder_upl):\n",
    "    #spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "            `CDC_FLAG` STRING,\n",
    "            `CDC_DSN` INTEGER,\n",
    "            `HH_H_T_ID` INTEGER,\n",
    "            `HH_T_ID` INTEGER,\n",
    "            `HH_BEFORE_QTY` FLOAT,\n",
    "            `HH_AFTER_QTY` FLOAT\n",
    "    \"\"\"\n",
    "    holding = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"|\")\n",
    "        .schema(schema)\n",
    "        .load(f\"{staging_area_folder_upl}/HoldingHistory.txt\")\n",
    "    )\n",
    "    holding.createOrReplaceTempView(\"holdings\")\n",
    "    factHoldings = spark.sql(\"\"\" \n",
    "                       Select \n",
    "                       SK_CustomerID, \n",
    "                       SK_AccountID, \n",
    "                       SK_SecurityID, \n",
    "                       SK_CompanyID,\n",
    "                       TradePrice as CurrentPrice,\n",
    "                       SK_CloseDateID as SK_DateID ,\n",
    "                       SK_CloseTimeID as SK_TimeID,\n",
    "                       HH_H_T_ID as TradeId,\n",
    "                       HH_T_ID as CurrentTradeID,\n",
    "                       HH_AFTER_QTY as CurrentHolding,\n",
    "                       CAST('2' as INT) as BatchID \n",
    "                       From holdings join DimTrade as ac on (HH_T_ID =ac.TradeID)\"\"\")\n",
    "    \n",
    "    \n",
    "    factHoldings.write.option(\"append\", \"true\").saveAsTable(\"FactHoldings\", mode=\"append\")\n",
    "    factHoldings.show(2)\n",
    "    \n",
    "    return factHoldings\n",
    "\n",
    "# load_fact_holdings(\"FactHoldings\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC #### FactWatches\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\"\"\"\n",
    "            SK_CustomerID BIGINT,\n",
    "            SK_SecurityID BIGINT,\n",
    "            SK_DateID_DatePlaced BIGINT,\n",
    "            SK_DateID_DateRemoved BIGINT,\n",
    "            BatchID INTEGER\n",
    "            \n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "def load_update_fact_watches(dbname, staging_area_folder_upl):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    # Customer ID, Ticker symbol, Datetime, activate or cancel watch\n",
    "    schema = \"\"\"\n",
    "            `CDC_FLAG` STRING,\n",
    "            `CDC_DSN` INTEGER,\n",
    "            `W_C_ID` BIGINT, \n",
    "            `W_S_SYMB` STRING,\n",
    "            `W_DTS` DATE,\n",
    "            `W_ACTION` STRING\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.read.format(\"csv\") \\\n",
    "        .option(\"delimiter\", \"|\") \\\n",
    "        .schema(schema) \\\n",
    "        .load(f\"{staging_area_folder_upl}/WatchHistory.txt\") \\\n",
    "    .createOrReplaceTempView(\"watches\")\n",
    "    \n",
    "    actv_watches = spark.sql(\"SELECT * FROM watches\").where(col(\"W_ACTION\") == 'ACTV')\n",
    "    actv_watches.createOrReplaceTempView(\"actv_watches\")\n",
    "    cncl_watches = spark.sql(\"SELECT * FROM watches\").where(col(\"W_ACTION\") == 'CNCL')\n",
    "    cncl_watches.createOrReplaceTempView(\"cncl_watches\")\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT w1.W_C_ID, w1.W_S_SYMB, w1.W_DTS AS DatePlaced, w2.W_DTS AS DateRemoved FROM actv_watches w1 LEFT JOIN cncl_watches w2 ON w1.W_C_ID = w2.W_C_ID AND w1.W_S_SYMB = w2.W_S_SYMB\n",
    "    \"\"\").createOrReplaceTempView(\"watches\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "        INSERT INTO FactWatches(SK_CustomerID, SK_SecurityID, SK_DateID_DatePlaced, \n",
    "                                SK_DateID_DateRemoved, BatchID)\n",
    "            SELECT \n",
    "                c.SK_CustomerID,\n",
    "                s.SK_SecurityID,\n",
    "                d1.SK_DateID AS SK_DateID_DatePlaced, \n",
    "                d2.SK_DateID AS SK_DateID_DateRemoved,\n",
    "                2 AS BatchID\n",
    "            FROM watches w LEFT JOIN \n",
    "                DimDate d1 ON DatePlaced = d1.DateValue LEFT JOIN \n",
    "                DimDate d2 ON DateRemoved = d2.DateValue LEFT JOIN\n",
    "                DimSecurity s ON (\n",
    "                    W_S_SYMB = s.Symbol AND \n",
    "                    s.isCurrent = True\n",
    "                ) LEFT JOIN \n",
    "                DimCustomer c ON (\n",
    "                    W_C_ID = c.CustomerID AND\n",
    "                    c.IsCurrent = True\n",
    "                )\n",
    "    \"\"\")\n",
    "    \n",
    "    return spark.sql(\"SELECT * FROM FactWatches WHERE BatchID=2\")\n",
    "\n",
    "# spark.sql(\"USE test\")\n",
    "# spark.sql(\"DELETE FROM FactWatches WHERE BatchID=2\")\n",
    "# watches= load_fact_watches(\"test\")\n",
    "# watches.where(\"BatchID = 2\").limit(10).toPandas()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## FactMarketistory\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def load_update_staging_FactMarketStory(dbname, staging_area_folder_upl):\n",
    "    \n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    # spark.sql(\"\"\"DROP TABLE FactMarketHistory\"\"\")\n",
    "\n",
    "    # create_fact_market_history(dbname)\n",
    "\n",
    "    schema = \"\"\"\n",
    "        `CDC_FLAG` STRING,\n",
    "        `CDC_DSN` INTEGER,\n",
    "        `DM_DATE` DATE,\n",
    "        `DM_S_SYMB` STRING,\n",
    "        `DM_CLOSE` FLOAT,\n",
    "        `DM_HIGH` FLOAT,\n",
    "        `DM_LOW` FLOAT,\n",
    "        `DM_VOL` INTEGER\n",
    "    \"\"\"\n",
    "\n",
    "    DailyMarket_ = spark.read.format(\"csv\").option(\"delimiter\", \"|\").schema(schema).load(f\"{staging_area_folder_upl}/DailyMarket.txt\")\n",
    "    DailyMarket_.createOrReplaceTempView(\"dailymarket\")\n",
    "    \n",
    "    # TODO: DI Message\n",
    "    DailyMarket_ = spark.sql(\n",
    "        \"\"\"\n",
    "        WITH DailyMarket AS (\n",
    "            SELECT DM.*, MIN(dm2.DM_DATE) as FiftyTwoWeekHighDate, MIN(dm3.DM_DATE) as FiftyTwoWeekLowDate\n",
    "            FROM (\n",
    "             SELECT dm.DM_CLOSE,\n",
    "                dm.DM_S_SYMB,\n",
    "                dm.DM_HIGH,\n",
    "                dm.DM_LOW,\n",
    "                dm.DM_VOL,\n",
    "                dm.DM_DATE,\n",
    "                max(dm.DM_HIGH) OVER (\n",
    "                    PARTITION BY dm.DM_S_SYMB\n",
    "                    ORDER BY CAST(dm.DM_DATE AS timestamp)\n",
    "                    RANGE BETWEEN INTERVAL 364 DAYS PRECEDING AND CURRENT ROW\n",
    "                 ) AS FiftyTwoWeekHigh,\n",
    "                 min(dm.DM_LOW) OVER (\n",
    "                    PARTITION BY dm.DM_S_SYMB\n",
    "                    ORDER BY CAST(dm.DM_DATE AS timestamp)\n",
    "                    RANGE BETWEEN INTERVAL 364 DAYS PRECEDING AND CURRENT ROW\n",
    "                 ) AS FiftyTwoWeekLow\n",
    "                 FROM dailymarket dm\n",
    "            ) DM INNER JOIN dailymarket dm2 ON DM.FiftyTwoWeekHigh = dm2.DM_HIGH AND dm2.DM_DATE BETWEEN date_sub(DM.DM_DATE, 364) AND DM.DM_DATE\n",
    "         INNER JOIN dailymarket dm3 ON DM.FiftyTwoWeekLow = dm3.DM_LOW AND dm3.DM_DATE BETWEEN date_sub(DM.DM_DATE, 364) AND dm.DM_DATE\n",
    "            GROUP BY DM.DM_DATE, DM.DM_CLOSE, DM.DM_HIGH, DM.DM_LOW, DM.DM_VOL, DM.FiftyTwoWeekHigh, DM.FiftyTwoWeekLow, dm.DM_S_SYMB\n",
    "        ), FinData AS (\n",
    "            SELECT\n",
    "            SK_CompanyID,\n",
    "            SUM(FI_BASIC_EPS) OVER (\n",
    "                PARTITION BY FI_QTR\n",
    "                ORDER BY FI_YEAR, FI_QTR\n",
    "                ROWS BETWEEN 4 PRECEDING AND CURRENT ROW\n",
    "            ) as Eps\n",
    "            From Financial\n",
    "        ), CompEarning AS (\n",
    "            SELECT dc.CompanyID, fd.Eps\n",
    "            FROM DimCompany dc\n",
    "            INNER JOIN FinData fd ON dc.CompanyID = fd.SK_CompanyID\n",
    "        )\n",
    "        SELECT  cast(dm.DM_CLOSE as float) as ClosePrice,\n",
    "                cast(dm.DM_HIGH as float) as DayHigh,\n",
    "                cast(dm.DM_LOW as float) as DayLow,\n",
    "                cast(dm.DM_VOL as int) as Volume,\n",
    "                cast(ds.SK_SecurityID as int) as SK_SecurityID,\n",
    "                cast(ds.SK_CompanyID as int) as SK_CompanyID,\n",
    "                cast(dd1.SK_DateID as int) as SK_DateID,\n",
    "                cast(dd2.SK_DateID as int) as SK_FiftyTwoWeekHighDate,\n",
    "                cast(dd3.SK_DateID as int) as SK_FiftyTwoWeekLowDate,\n",
    "                cast(dm.FiftyTwoWeekHigh as float) as FiftyTwoWeekHigh,\n",
    "                cast(dm.FiftyTwoWeekLow as float) as FiftyTwoWeekLow,\n",
    "                cast(((ds.dividend / dm.DM_CLOSE) * 100.0) as float) as Yield,\n",
    "                CASE \n",
    "                    WHEN ISNULL(ce.Eps) or ce.Eps = 0 THEN NULL \n",
    "                    ELSE cast((dm.DM_CLOSE / ce.Eps) as float)\n",
    "                END as PERatio,\n",
    "                cast('2' as int) as BatchID\n",
    "        FROM DailyMarket dm\n",
    "        INNER JOIN DimSecurity ds ON ds.Symbol = dm.DM_S_SYMB AND ds.IsCurrent = 1\n",
    "        INNER JOIN DimDate dd1 ON dd1.DateValue = dm.DM_DATE\n",
    "        INNER JOIN DimDate dd2 ON dd2.DateValue = dm.FiftyTwoWeekHighDate\n",
    "        INNER JOIN DimDate dd3 ON dd3.DateValue = dm.FiftyTwoWeekLowDate\n",
    "        LEFT JOIN CompEarning ce ON ds.SK_CompanyID = ce.CompanyID\n",
    "         \"\"\")\n",
    "    \n",
    "    DailyMarket_.createOrReplaceTempView(\"dailymarket_insert\")\n",
    "    spark.sql(\"\"\"\n",
    "               INSERT INTO FactMarketHistory(ClosePrice, DayHigh, DayLow, Volume, SK_SecurityID, SK_CompanyID, SK_DateID, SK_FiftyTwoWeekHighDate, SK_FiftyTwoWeekLowDate,  FiftyTwoWeekHigh, FiftyTwoWeekLow, Yield, PERatio, BatchID)\n",
    "       SELECT * FROM dailymarket_insert\n",
    "    \"\"\")\n",
    "    \n",
    "    return spark.sql(\"\"\"\n",
    "        SELECT * FROM FactMarketHistory WHERE BatchID = 2\n",
    "    \"\"\")\n",
    "\n",
    "# marketh = load_staging_FactMarketStory(\"test\", \"gs://tpcdi-with-spark-bdma/TPCDI_Data/TPCDI_Data/Scale3/Batch2/\")\n",
    "# marketh.show()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Prospect\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#Create prospect\n",
    "from pyspark.sql.functions import udf, struct\n",
    "from datetime import datetime\n",
    "\n",
    "def get_marketingnameplate(row):\n",
    "    result = []\n",
    "    \n",
    "    if (row.NetWorth and row.NetWorth > 1000000) or (row.Income and row.Income > 200000):\n",
    "        result.append(\"HighValue\")\n",
    "    if (row.NumberChildren and row.NumberChildren > 3) or ( row.NumberCreditCards and row.NumberCreditCards > 5):\n",
    "        result.append(\"Expenses\")\n",
    "    if (row.Age and row.Age > 45):\n",
    "        result.append(\"Boomer\")\n",
    "    if (row.Income and row.Income < 50000) or (row.CreditRating and row.CreditRating < 600) or (row.NetWorth and row.NetWorth < 100000):\n",
    "        result.append(\"MoneyAlert\")\n",
    "    if (row.NumberCars and row.NumberCars > 3) or (row.NumberCreditCards and row.NumberCreditCards > 7):\n",
    "        result.append(\"Spender\")\n",
    "    if (row.Age and row.Age < 25) and (row.NetWorth and row.NetWorth > 1000000):\n",
    "        result.append(\"Inherited\")\n",
    "    \n",
    "    return \"+\".join(result) if result else None\n",
    "\n",
    "\n",
    "def load_update_staging_Prospect(dbname, staging_area_folder_upl):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    # spark.sql(\"\"\"DROP TABLE Prospect \"\"\")\n",
    "\n",
    "    # create_prospect_table(dbname)\n",
    "\n",
    "    schema = \"\"\"\n",
    "        `AgencyID` String,\n",
    "        `LastName` String,\n",
    "        `FirstName` String,\n",
    "        `MiddleInitial` String,\n",
    "        `Gender` String,\n",
    "        `AddressLine1` String,\n",
    "        `AddressLine2` String,\n",
    "        `PostalCode` String,\n",
    "        `City` String,\n",
    "        `State` String,\n",
    "        `Country` String,\n",
    "        `Phone` String,\n",
    "        `Income` Integer,\n",
    "        `NumberCars` Integer,\n",
    "        `NumberChildren` Integer,\n",
    "        `MaritalStatus` String,\n",
    "        `Age` Integer,\n",
    "        `CreditRating` Integer,\n",
    "        `OwnOrRentFlag` String,\n",
    "        `Employer` String,\n",
    "        `NumberCreditCards` Integer,\n",
    "        `NetWorth` Integer\n",
    "    \"\"\"\n",
    "    Prospect_ = spark.read.format(\"csv\").option(\"delimiter\", \",\").schema(schema).load(f\"{staging_area_folder_upl}/Prospect.csv\")\n",
    "    \n",
    "    udf_marketing = udf(lambda row: get_marketingnameplate(row), StringType())\n",
    "    Prospect_ = Prospect_.withColumn('MarketingNameplate', udf_marketing(struct([Prospect_[x] for x in Prospect_.columns])))\n",
    "    \n",
    "    now = datetime.utcnow()\n",
    "    \n",
    "    DimDate = spark.sql(\"\"\"\n",
    "        SELECT SK_DateID FROM DimDate WHERE SK_DateID = 20201231\n",
    "    \"\"\")\n",
    "    Prospect_ = Prospect_.crossJoin(DimDate)\n",
    "    Prospect_.createOrReplaceTempView(\"Prospect_\")\n",
    "    \n",
    "    Prospect_ = spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT p.AgencyID as AgencyID, \n",
    "               2 as BatchID, \n",
    "               CASE\n",
    "                   WHEN dc.Status = 'ACTIVE' THEN True ELSE False\n",
    "               END as IsCustomer,\n",
    "               p.SK_DateID as SK_RecordDateID,\n",
    "               p.SK_DateID as SK_UpdateDateID,\n",
    "               p.LastName as LastName,\n",
    "               p.FirstName as FirstName,\n",
    "               p.MiddleInitial as MiddleInitial,\n",
    "               p.Gender as Gender,\n",
    "               p.AddressLine1 as AddressLine1,\n",
    "               p.AddressLine2 as AddressLine2,\n",
    "               p.PostalCode as PostalCode,\n",
    "               p.City as City,\n",
    "               p.State as State,\n",
    "               p.Country as Country,\n",
    "               p.Phone as Phone,\n",
    "               p.Income as Income,\n",
    "               p.NumberCars as NumberCars,\n",
    "               p.NumberChildren as NumberChildren,\n",
    "               p.MaritalStatus as MaritalStatus,\n",
    "               p.Age as Age,\n",
    "               p.CreditRating as CreditRating,\n",
    "               p.OwnOrRentFlag as OwnOrRentFlag,\n",
    "               p.Employer as Employer,\n",
    "               p.NumberCreditCards as NumberCreditCards,\n",
    "               p.NetWorth as NetWorth,\n",
    "               p.MarketingNameplate as MarketingNameplate\n",
    "        FROM Prospect_ p\n",
    "        LEFT JOIN DimCustomer dc ON \n",
    "        upper(p.FirstName) = upper(dc.FirstName) AND upper(p.LastName) = upper(dc.LastName)\n",
    "        AND upper(p.AddressLine1) = upper(dc.AddressLine1) AND upper(p.AddressLine2) = upper(dc.AddressLine2)\n",
    "        AND upper(p.PostalCode) = upper(dc.PostalCode)\n",
    "    \"\"\")\n",
    "    Prospect_.createOrReplaceTempView(\"Prospect_\")\n",
    "\n",
    "    combined_prospect = spark.sql(\"\"\"\n",
    "                CREATE TABLE CombinedProspect AS\n",
    "                SELECT\n",
    "                    COALESCE(p.AgencyID, np.AgencyID) AS AgencyID,\n",
    "                    CAST(COALESCE(p.SK_RecordDateID, np.SK_RecordDateID) AS INT) AS SK_RecordDateID,\n",
    "                    CAST(COALESCE(p.SK_UpdateDateID, np.SK_UpdateDateID) AS INT) AS SK_UpdateDateID,\n",
    "                    COALESCE(p.BatchID, np.BatchID) AS BatchID,\n",
    "                    COALESCE(p.IsCustomer, np.IsCustomer) AS IsCustomer,\n",
    "                    COALESCE(np.LastName, p.LastName) AS LastName,\n",
    "                    COALESCE(np.FirstName, p.FirstName) AS FirstName,\n",
    "                    COALESCE(np.MiddleInitial, p.MiddleInitial) AS MiddleInitial,\n",
    "                    COALESCE(np.Gender, p.Gender) AS Gender,\n",
    "                    COALESCE(np.AddressLine1, p.AddressLine1) AS AddressLine1,\n",
    "                    COALESCE(np.AddressLine2, p.AddressLine2) AS AddressLine2,\n",
    "                    COALESCE(np.PostalCode, p.PostalCode) AS PostalCode,\n",
    "                    COALESCE(np.City, p.City) AS City,\n",
    "                    COALESCE(np.State, p.State) AS State,\n",
    "                    COALESCE(np.Country, p.Country) AS Country,\n",
    "                    COALESCE(np.Phone, p.Phone) AS Phone,\n",
    "                    COALESCE(np.Income, p.Income) AS Income,\n",
    "                    COALESCE(np.NumberCars, p.NumberCars) AS NumberCars,\n",
    "                    COALESCE(np.NumberChildren, p.NumberChildren) AS NumberChildren,\n",
    "                    COALESCE(np.MaritalStatus, p.MaritalStatus) AS MaritalStatus,\n",
    "                    COALESCE(np.Age, p.Age) AS Age,\n",
    "                    COALESCE(np.CreditRating, p.CreditRating) AS CreditRating,\n",
    "                    COALESCE(np.OwnOrRentFlag, p.OwnOrRentFlag) AS OwnOrRentFlag,\n",
    "                    COALESCE(np.Employer, p.Employer) AS Employer,\n",
    "                    COALESCE(np.NumberCreditCards, p.NumberCreditCards) AS NumberCreditCards,\n",
    "                    COALESCE(np.NetWorth, p.NetWorth) AS NetWorth,\n",
    "                    COALESCE(np.MarketingNameplate, p.MarketingNameplate) AS MarketingNameplate\n",
    "                FROM\n",
    "                    Prospect p\n",
    "                FULL OUTER JOIN \n",
    "                    Prospect_ np\n",
    "                ON \n",
    "                    p.AgencyID = np.AgencyID;\n",
    "    \"\"\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "            INSERT OVERWRITE TABLE Prospect\n",
    "            SELECT * FROM CombinedProspect\n",
    "              \"\"\")\n",
    "\n",
    "    spark.sql(\"\"\"DROP TABLE CombinedProspect\"\"\") \n",
    "\n",
    "    return spark.sql(\"\"\"\n",
    "        SELECT * FROM Prospect WHERE BatchID = 2\n",
    "    \"\"\")\n",
    "# Prospect_ = load_staging_Prospect(\"test\", \"gs://tpcdi-with-spark-bdma/TPCDI_Data/TPCDI_Data/Scale3/Batch2/\")\n",
    "# Prospect_.limit(3).toPandas()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Run Incremental Update\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import string\n",
    "import random\n",
    "\n",
    "from statistics import geometric_mean\n",
    "\n",
    "def get_max(num1, num2):\n",
    "    if num1 > num2:\n",
    "        return num1\n",
    "    return num2\n",
    "\n",
    "def id_generator(size=6, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))\n",
    "\n",
    "\n",
    "def run_historical_load(dbname, scale_factor, file_id):\n",
    "    metrics = {}\n",
    "    # Init DB\n",
    "    start = time.time()\n",
    "    clean_warehouse(dbname)\n",
    "    create_warehouse(dbname)\n",
    "    end = time.time() - start\n",
    "\n",
    "    metrics[\"create_db_time\"] = end\n",
    "\n",
    "    staging_area_folder = f\"{os.getcwd()}/data/{scale_factor}/Batch1/\"\n",
    "\n",
    "    # Run historical load\n",
    "    start = time.time()\n",
    "    dimdate = load_dim_date(dbname, staging_area_folder)\n",
    "    dimtime = load_dim_time(dbname, staging_area_folder)\n",
    "    taxrate = load_tax_rate(dbname, staging_area_folder)\n",
    "    staginghr = load_staging_hr_file(dbname, staging_area_folder)\n",
    "    industry = load_staging_Industry(dbname, staging_area_folder)\n",
    "\n",
    "\n",
    "    load_finwire_files(dbname, scale_factor)\n",
    "    dimcompany = load_finwires_into_dim_company(dbname, scale_factor)\n",
    "    dimsecurity = load_finwires_into_dim_security(dbname)\n",
    "    fintable = load_finwires_into_financial_table(dbname)\n",
    "\n",
    "    statustype = load_status_type(dbname, staging_area_folder)\n",
    "    tradetype = load_trade_type(dbname, staging_area_folder)\n",
    "\n",
    "    factmarkethistory = load_staging_FactMarketStory(dbname, staging_area_folder)\n",
    "    prospect = load_staging_Prospect(dbname, staging_area_folder)\n",
    "\n",
    "    customer = load_customers(dbname, staging_area_folder)\n",
    "    account = load_account(dbname, staging_area_folder)\n",
    "\n",
    "    dimtrade = load_staging_dim_trade(dbname, staging_area_folder)\n",
    "    factcashbalance = load_fact_cash_balances(dbname, staging_area_folder)\n",
    "    holding = load_fact_holdings(dbname, staging_area_folder)\n",
    "    watch = load_fact_watches(dbname, staging_area_folder)\n",
    "    end = time.time() - start\n",
    "\n",
    "    metrics[\"et\"] = end\n",
    "    dimdate_count = dimdate.count()\n",
    "    dimtime_count = dimtime.count()\n",
    "    taxrate_count = taxrate.count()\n",
    "    staginghr_count = staginghr.count()\n",
    "    dimcompany_count = dimcompany.count()\n",
    "    dimsecurity_count = dimsecurity.count()\n",
    "    fintable_count = fintable.count()\n",
    "    statustype_count = statustype.count()\n",
    "    tradetype_count = tradetype.count()\n",
    "    factmarkethistory_count = factmarkethistory.count()\n",
    "    prospect_count = prospect.count()\n",
    "    industry_count = industry.count()\n",
    "    dimtrade_count = dimtrade.count()\n",
    "    factcashbalance_count = factcashbalance.count()\n",
    "    holding_count = holding.count()\n",
    "    watch_count = watch.count()\n",
    "    customer_count = customer.count()\n",
    "    account_count = account.count()\n",
    "\n",
    "    # Sum the individual counts\n",
    "    rows = dimdate_count + dimtime_count + taxrate_count + staginghr_count + dimcompany_count + dimsecurity_count + fintable_count + statustype_count + tradetype_count + factmarkethistory_count + prospect_count + industry_count + dimtrade_count + factcashbalance_count + holding_count + watch_count + customer_count + account_count\n",
    "\n",
    "    metrics[\"rows\"] = rows\n",
    "    metrics[\"throughput\"] = (rows / end)\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics, index=[0])\n",
    "    \n",
    "    metrics_df.to_csv(f\"{os.getcwd()}/results/data/historical_load_{scale_factor}_{file_id}.csv\", index=False)\n",
    "    return metrics_df\n",
    "\n",
    "def run_incremental_load_1(dbname, scale_factor, file_id, result_queue):\n",
    "    metrics = {}\n",
    "    staging_area_folder = f\"{os.getcwd()}/data/{scale_factor}/Batch2\"\n",
    "\n",
    "    # Run incremental update\n",
    "    print(\"Executing incremental load 1\")\n",
    "    start_time = time.time()\n",
    "    customer = load_dimen_customer(dbname, staging_area_folder)\n",
    "    account = load_dimen_account(dbname, staging_area_folder)\n",
    "    dimtrade = load_update_dimen_trade(dbname, staging_area_folder)\n",
    "\n",
    "    factcashbalance = load_update_fact_cash_balances(dbname, staging_area_folder)\n",
    "    holding = load_update_fact_holdings(dbname, staging_area_folder)\n",
    "    watch = load_update_fact_watches(dbname, staging_area_folder)\n",
    "\n",
    "    factmarkethistory =load_update_staging_FactMarketStory(dbname, staging_area_folder)\n",
    "    prospect = load_update_staging_Prospect(dbname, staging_area_folder)\n",
    "    # End the timer\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Incremental load execution time: {execution_time:.6f} seconds\")\n",
    "\n",
    "    metrics[\"et\"] = execution_time\n",
    "\n",
    "\n",
    "    factmarkethistory_count = factmarkethistory.count()\n",
    "    prospect_count = prospect.count()\n",
    "    dimtrade_count = dimtrade.count()\n",
    "    factcashbalance_count = factcashbalance.count()\n",
    "    holding_count = holding.count()\n",
    "    watch_count = watch.count()\n",
    "    customer_count = customer.count()\n",
    "    account_count = account.count()\n",
    "\n",
    "    # Sum the individual counts\n",
    "    rows = factmarkethistory_count + prospect_count + dimtrade_count + factcashbalance_count + holding_count + watch_count + customer_count + account_count\n",
    "\n",
    "\n",
    "    metrics[\"rows\"] = rows\n",
    "    metrics[\"throughput\"] = (rows / get_max(execution_time,1800))\n",
    "\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics, index=[0])\n",
    "    metrics_df.to_csv(f\"{os.getcwd()}/results/data/incremental_load_1_{scale_factor}_{file_id}.csv\", index=False)\n",
    "\n",
    "    result_queue.put(metrics_df)  # Put the result in the queue\n",
    "\n",
    "    #return metrics_df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 4: Incremental Update 2\n",
    "# MAGIC This step:\n",
    "# MAGIC - populates the data warehouse\n",
    "# MAGIC - applies transformations described in clause 4.6 of the tpc-di manual (batch3 folder on the generated data)\n",
    "# MAGIC - performs validations based on clause 7.4\n",
    "# MAGIC - upon completion of the validation stage, a phase completion record is written into DIMessages table\n",
    "# MAGIC - this step must be timed\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#get_max(5,10)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC \n",
    "# MAGIC ### Transformation Details for Incremental Updates\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC \n",
    "# MAGIC ### DimenCustomer\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def load_dimen_customer_2(dbname, staging_area_folder_up2):\n",
    "    #spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "            `CDC_FLAG` STRING,\n",
    "            `CDC_DSN` LONG,\n",
    "            `C_ID` STRING,\n",
    "            `C_TAX_ID` STRING,\n",
    "            `C_ST_ID` STRING,\n",
    "            `C_L_NAME` STRING,\n",
    "            `C_F_NAME` STRING,\n",
    "            `C_M_NAME` STRING,\n",
    "            `C_GNDR` STRING,\n",
    "            `C_TIER` STRING,\n",
    "            `C_DOB` STRING,\n",
    "            `C_ADLINE1` STRING,\n",
    "            `C_ADLINE2` STRING,\n",
    "            `C_ZIPCODE` STRING,\n",
    "            `C_CITY` STRING,\n",
    "            `C_STATE_PROV` STRING,\n",
    "            `C_CTRY` STRING,\n",
    "            `C_CTRY_1` STRING,\n",
    "            `C_AREA_1` STRING,\n",
    "            `C_LOCAL_1` STRING,\n",
    "            `C_EXT_1` STRING,\n",
    "            `C_CTRY_2` STRING,\n",
    "            `C_AREA_2` STRING,\n",
    "            `C_LOCAL_2` STRING,\n",
    "            `C_EXT_2` STRING,\n",
    "            `C_CTRY_3` STRING,\n",
    "            `C_AREA_3` STRING,\n",
    "            `C_LOCAL_3` STRING,\n",
    "            `C_EXT_3` STRING,\n",
    "            `C_EMAIL_1` STRING,\n",
    "            `C_EMAIL_2` STRING,\n",
    "            `C_LCL_TX_ID` STRING,\n",
    "            `C_NAT_TX_ID` STRING\n",
    "    \"\"\"\n",
    "    customer_base = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"|\")\n",
    "        .schema(schema)\n",
    "        .load(f\"{staging_area_folder_up2}/Customer.txt\")\n",
    "    )\n",
    "    customer_base.createOrReplaceTempView(\"customer_base_batch1\")    \n",
    "    new_customer_df= spark.sql(\"\"\" Select *\n",
    "                           From customer_base_batch1 as a where a.CDC_FLAG = 'I'  \"\"\")\n",
    "\n",
    "    update_customer_df=spark.sql(\"\"\" Select *\n",
    "                           From customer_base_batch1 as a where a.CDC_FLAG = 'U'  \"\"\")\n",
    "    \n",
    "    Customers = new_customer_df.join(update_customer_df, on=['C_ID'], how='left_anti')\n",
    "    \n",
    "    Customers.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "    # Customers.show(2)\n",
    "\n",
    "    #### Added on line 2540 \"ST_NAME as Status, \"\n",
    "    dimCustomer = spark.sql(\"\"\"\n",
    "                       Select \n",
    "                       CDC_DSN AS SK_CustomerID,\n",
    "                       c.C_ID as CustomerID,\n",
    "                       C_TAX_ID as TaxID,\n",
    "                       ST_NAME as Status, \n",
    "                       C_L_NAME as LastName,\n",
    "                       C_F_NAME as FirstName,\n",
    "                       C_M_NAME as MiddleInitial,\n",
    "                       (CASE WHEN (C_GNDR = 'F' OR C_GNDR='M') THEN C_GNDR ELSE 'U' END) as Gender,\n",
    "                       C_TIER as Tier,\n",
    "                       C_DOB as DOB,\n",
    "                       C_ADLINE1 as AddressLine1,\n",
    "                       C_ADLINE2 as AddressLine2,\n",
    "                       C_ZIPCODE as PostalCode,\n",
    "                       C_CITY as City,\n",
    "                       C_STATE_PROV as StateProv,\n",
    "                       C_CTRY as Country,\n",
    "                       (\n",
    "                           CASE \n",
    "                           WHEN (C_CTRY_1 IS NOT NULL AND C_AREA_1 IS NOT NULL AND C_LOCAL_1 IS NOT NULL AND C_EXT_1 IS NULL) \n",
    "                           THEN CONCAT('+' , C_CTRY_1 , ' (' , C_AREA_1 , ') ' , C_LOCAL_1)\n",
    "                           WHEN (C_CTRY_1 IS NULL AND C_AREA_1 IS NOT NULL AND C_LOCAL_1 IS NOT NULL AND C_EXT_1 IS NULL) \n",
    "                           THEN CONCAT(' (' , C_AREA_1 , ') ' , C_LOCAL_1)\n",
    "                           WHEN (C_CTRY_1 IS NULL AND C_AREA_1 IS NULL AND C_LOCAL_1 IS NOT NULL AND C_EXT_1 IS NULL) \n",
    "                           THEN C_LOCAL_1\n",
    "                           \n",
    "                           WHEN (C_CTRY_1 IS NOT NULL AND C_AREA_1 IS NOT NULL AND C_LOCAL_1 IS NOT NULL AND C_EXT_1 IS NOT NULL) \n",
    "                           THEN CONCAT('+' , C_CTRY_1 , ' (' , C_AREA_1 , ') ' , C_LOCAL_1, C_EXT_1)\n",
    "                           WHEN (C_CTRY_1 IS NULL AND C_AREA_1 IS NOT NULL AND C_LOCAL_1 IS NOT NULL AND C_EXT_1 IS NOT NULL) \n",
    "                           THEN CONCAT(' (' , C_AREA_1 , ') ' , C_LOCAL_1, C_EXT_1)\n",
    "                           WHEN (C_CTRY_1 IS NULL AND C_AREA_1 IS NULL AND C_LOCAL_1 IS NOT NULL AND C_EXT_1 IS NOT NULL) \n",
    "                           THEN CONCAT(C_LOCAL_1, C_EXT_1)\n",
    "                           \n",
    "                           ELSE NULL\n",
    "                           END\n",
    "                       ) as Phone1,\n",
    "                       (\n",
    "                           CASE \n",
    "                           WHEN (C_CTRY_2 IS NOT NULL AND C_AREA_2 IS NOT NULL AND C_LOCAL_2 IS NOT NULL AND C_EXT_2 IS NULL) \n",
    "                           THEN CONCAT('+' , C_CTRY_2 , ' (' , C_AREA_2 , ') ' , C_LOCAL_2)\n",
    "                           WHEN (C_CTRY_2 IS NULL AND C_AREA_2 IS NOT NULL AND C_LOCAL_2 IS NOT NULL AND C_EXT_2 IS NULL) \n",
    "                           THEN CONCAT(' (' , C_AREA_2 , ') ' , C_LOCAL_2)\n",
    "                           WHEN (C_CTRY_2 IS NULL AND C_AREA_2 IS NULL AND C_LOCAL_2 IS NOT NULL AND C_EXT_2 IS NULL) \n",
    "                           THEN C_LOCAL_2\n",
    "                           \n",
    "                           WHEN (C_CTRY_2 IS NOT NULL AND C_AREA_2 IS NOT NULL AND C_LOCAL_2 IS NOT NULL AND C_EXT_2 IS NOT NULL) \n",
    "                           THEN CONCAT('+' , C_CTRY_2 , ' (' , C_AREA_2 , ') ' , C_LOCAL_2, C_EXT_2)\n",
    "                           WHEN (C_CTRY_2 IS NULL AND C_AREA_2 IS NOT NULL AND C_LOCAL_2 IS NOT NULL AND C_EXT_2 IS NOT NULL) \n",
    "                           THEN CONCAT(' (' , C_AREA_2 , ') ' , C_LOCAL_2, C_EXT_2)\n",
    "                           WHEN (C_CTRY_2 IS NULL AND C_AREA_2 IS NULL AND C_LOCAL_2 IS NOT NULL AND C_EXT_2 IS NOT NULL) \n",
    "                           THEN CONCAT(C_LOCAL_2, C_EXT_2)\n",
    "                           \n",
    "                           ELSE NULL\n",
    "                           END\n",
    "                       ) as Phone2,\n",
    "                       (\n",
    "                           CASE \n",
    "                           WHEN (C_CTRY_3 IS NOT NULL AND C_AREA_3 IS NOT NULL AND C_LOCAL_3 IS NOT NULL AND C_EXT_3 IS NULL) \n",
    "                           THEN CONCAT('+' , C_CTRY_3 , ' (' , C_AREA_3 , ') ' , C_LOCAL_3)\n",
    "                           WHEN (C_CTRY_3 IS NULL AND C_AREA_3 IS NOT NULL AND C_LOCAL_3 IS NOT NULL AND C_EXT_3 IS NULL) \n",
    "                           THEN CONCAT(' (' , C_AREA_3 , ') ' , C_LOCAL_3)\n",
    "                           WHEN (C_CTRY_3 IS NULL AND C_AREA_3 IS NULL AND C_LOCAL_3 IS NOT NULL AND C_EXT_3 IS NULL) \n",
    "                           THEN C_LOCAL_3\n",
    "                           \n",
    "                           WHEN (C_CTRY_3 IS NOT NULL AND C_AREA_3 IS NOT NULL AND C_LOCAL_3 IS NOT NULL AND C_EXT_3 IS NOT NULL) \n",
    "                           THEN CONCAT('+' , C_CTRY_3 , ' (' , C_AREA_3 , ') ' , C_LOCAL_3, C_EXT_3)\n",
    "                           WHEN (C_CTRY_3 IS NULL AND C_AREA_3 IS NOT NULL AND C_LOCAL_3 IS NOT NULL AND C_EXT_3 IS NOT NULL) \n",
    "                           THEN CONCAT(' (' , C_AREA_3 , ') ' , C_LOCAL_3, C_EXT_3)\n",
    "                           WHEN (C_CTRY_3 IS NULL AND C_AREA_3 IS NULL AND C_LOCAL_3 IS NOT NULL AND C_EXT_3 IS NOT NULL) \n",
    "                           THEN CONCAT(C_LOCAL_3, C_EXT_3)\n",
    "                           \n",
    "                           ELSE NULL\n",
    "                           END\n",
    "                       ) as Phone3,\n",
    "                       C_EMAIL_1 as Email1,\n",
    "                       C_EMAIL_2 as Email2,\n",
    "                       NAT.TX_NAME as NationalTaxRateDesc,\n",
    "                       NAT.TX_RATE as NationalTaxRate,\n",
    "                       LCL.TX_NAME as LocalTaxRateDesc,\n",
    "                       LCL.TX_RATE as LocalTaxRate,\n",
    "                       AgencyID as AgencyID,\n",
    "                       CreditRating as CreditRating,\n",
    "                       NetWorth as NetWorth,\n",
    "                        COALESCE(CASE \n",
    "                            WHEN NetWorth > 1000000 THEN 'HighValue+' \n",
    "                            ELSE NULL \n",
    "                        END,\n",
    "                       CASE \n",
    "                            WHEN NumberChildren > 3 THEN 'Expenses+' \n",
    "                            WHEN NumberCreditCards > 5 THEN 'Expenses+'\n",
    "                            ELSE NULL \n",
    "                        END,\n",
    "                       CASE \n",
    "                            WHEN Age > 45 THEN 'Boomer+' \n",
    "                            ELSE NULL \n",
    "                        END,\n",
    "                       CASE \n",
    "                            WHEN Income < 50000 THEN 'MoneyAlert+' \n",
    "                            WHEN CreditRating < 600 THEN 'MoneyAlert+' \n",
    "                            WHEN NetWorth < 100000 THEN 'MoneyAlert+' \n",
    "                            ELSE Null \n",
    "                        END,\n",
    "                       CASE \n",
    "                            WHEN NumberCars > 3 THEN 'Spender+' \n",
    "                            WHEN NumberCreditCards > 7 THEN 'Spender+' \n",
    "                            ELSE Null \n",
    "                        END,\n",
    "                       CASE \n",
    "                            WHEN Age < 25 THEN 'Inherited' \n",
    "                            WHEN NetWorth > 100000 THEN 'Inherited'  \n",
    "                            ELSE Null  \n",
    "                        END) as MarketingNameplate, \n",
    "                       CAST('True' as BOOLEAN) as IsCurrent, \n",
    "                       CAST('3' as INT) as BatchID, \n",
    "                       to_date('2015-01-01', 'yyyy-MM-dd') as EffectiveDate, \n",
    "                       to_date('9999-12-31', 'yyyy-MM-dd') as EndDate\n",
    "                       From customers as c \n",
    "                       left join TaxRate as NAT on c.C_NAT_TX_ID = NAT.TX_ID \n",
    "                       left join TaxRate as LCL on c.C_LCL_TX_ID = LCL.TX_ID \n",
    "                       left join Prospect as p on (c.C_L_NAME = p.LastName and c.C_F_NAME = p.FirstName \n",
    "                            and c.C_ADLINE1 = p.AddressLine1 and c.C_ADLINE2 =  p.AddressLine2 and c.C_ZIPCODE = p.PostalCode)\n",
    "                            left join StatusType on StatusType.ST_ID = c.C_ST_ID \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #dimCustomer.printSchema()\n",
    "    \n",
    "    dimCustomer.write.mode(\"append\").saveAsTable( \"DimCustomer\", mode=\"append\")\n",
    "    \n",
    "    return dimCustomer\n",
    "    \n",
    "#load_dimen_customer(\"test\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC \n",
    "# MAGIC ### DimenAccount\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def load_dimen_account_2(dbname, staging_area_folder_up2):\n",
    "    #spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "            `CDC_FLAG` STRING,\n",
    "            `CDC_DSN` INTEGER,\n",
    "            `CA_ID` LONG,\n",
    "            `CA_B_ID` INTEGER,\n",
    "            `CA_C_ID` STRING,\n",
    "            `CA_NAME` STRING,\n",
    "            `CA_TAX_ST` STRING,\n",
    "            `CA_ST_ID` STRING\n",
    "    \"\"\"\n",
    "    account_base = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"|\")\n",
    "        .schema(schema)\n",
    "        .load(f\"{staging_area_folder_up2}/Account.txt\")\n",
    "    )\n",
    "    account_base.createOrReplaceTempView(\"account_base_batch1\")\n",
    "    \n",
    "    \n",
    "    add_account_df= spark.sql(\"\"\" Select *\n",
    "                           From account_base_batch1 as a where a.CDC_FLAG = 'I'  \"\"\")\n",
    "\n",
    "    updated_account_df=spark.sql(\"\"\" Select *\n",
    "                           From account_base_batch1 as a where a.CDC_FLAG = 'U'  \"\"\")\n",
    "    \n",
    "    #new_account_df = new_customer_records_rdd.toDF(new_customer_schema).select(\"C_ID\", \"CA_ID\", \"CA_TAX_ST\", \"CA_B_ID\", \"CA_NAME\")\n",
    "\n",
    "    Accounts = account_base.union(add_account_df).join(updated_account_df, on=['CA_ID'], how='left_anti').union(updated_account_df)\n",
    "    Accounts.createOrReplaceTempView(\"accounts\")\n",
    "    \n",
    "    # Accounts.show(2)\n",
    "\n",
    "    dimAccount = spark.sql(\"\"\" Select CDC_DSN AS SK_AccountID,\n",
    "                           CA_ID as AccountID,\n",
    "                           CA_C_ID as SK_CustomerID,\n",
    "                           CA_B_ID as SK_BrokerID,\n",
    "                           ST_NAME as Status,\n",
    "                           CA_NAME as AccountDesc,\n",
    "                           CA_TAX_ST as TaxStatus,\n",
    "                           CAST('True' as BOOLEAN) as IsCurrent,\n",
    "                           CAST('3' as INT) as BatchID,\n",
    "                           to_date('2015-01-01', 'yyyy-MM-dd') as EffectiveDate, \n",
    "                           to_date('9999-12-31', 'yyyy-MM-dd') as EndDate \n",
    "                           From accounts join StatusType on accounts.CA_ST_ID = StatusType.ST_ID \"\"\")\n",
    "\n",
    "    #dimAccount.printSchema()\n",
    "\n",
    "    dimAccount.createOrReplaceTempView(\"dimAccount_stream\")\n",
    "    dimAccount = cast_to_target_schema(\"dimAccount_stream\", \"DimAccount\")\n",
    "    \n",
    "    dimAccount.write.mode(\"append\").saveAsTable( \"DimAccount\", mode=\"append\")\n",
    "    \n",
    "    return dimAccount\n",
    "    \n",
    "#load_dimen_account(\"test\")\n",
    "\n",
    "def load_update_dimen_trade_2(dbname,staging_area_folder_up1):\n",
    "    #spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "            `CDC_FLAG` String,\n",
    "            `CDC_DSN` String,\n",
    "            `T_ID` INTEGER,\n",
    "            `T_DTS` TIMESTAMP,\n",
    "            `T_ST_ID` String,\n",
    "            `T_TT_ID` String,\n",
    "            `T_IS_CASH`  Boolean,\n",
    "            `T_S_SYMB` String,\n",
    "            `T_QTY` Float,\n",
    "            `T_BID_PRICE` Float,\n",
    "            `T_CA_ID` String,\n",
    "            `T_EXEC_NAME` String,\n",
    "            `T_TRADE_PRICE` Float,\n",
    "            `T_CHRG` Float,\n",
    "            `T_COMM` Float,\n",
    "            `T_TAX` Float\n",
    "    \"\"\"\n",
    "    trade_base = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"|\")\n",
    "        .schema(schema)\n",
    "        .load(f\"{staging_area_folder_up1}/Trade.txt\")\n",
    "    )\n",
    "    trade_base.createOrReplaceTempView(\"trade_base_batch1\")\n",
    "        \n",
    "    add_trade_df= spark.sql(\"\"\" Select *\n",
    "                           From trade_base_batch1 as a where a.CDC_FLAG = 'I'  \"\"\")\n",
    "\n",
    "    updated_trade_df=spark.sql(\"\"\" Select *\n",
    "                           From trade_base_batch1 as a where a.CDC_FLAG = 'U'  \"\"\")\n",
    "    \n",
    "\n",
    "    trades = trade_base.union(add_trade_df).join(updated_trade_df, on=['T_ID'], how='left_anti').union(updated_trade_df)\n",
    "    trades.createOrReplaceTempView(\"trade_view\")\n",
    "    \n",
    "    \n",
    "    trade = spark.sql(\"\"\"\n",
    "            SELECT T.T_ID,\n",
    "                CASE WHEN T.CDC_FLAG = 'I' then TH.TH_DTS ELSE NULL END as create_date,\n",
    "                CASE WHEN T.CDC_FLAG = 'I' then TH.TH_DTS ELSE NULL END as create_time,\n",
    "                CASE \n",
    "                WHEN T.CDC_FLAG = 'I' then NULL \n",
    "                WHEN TH.TH_ST_ID in ('CMPT', 'CNCL') THEN TH.TH_DTS ELSE NULL END as close_date,\n",
    "                CASE WHEN T.CDC_FLAG = 'I' then NULL \n",
    "                WHEN TH.TH_ST_ID in ('CMPT', 'CNCL') THEN TH.TH_DTS ELSE NULL END as close_time,\n",
    "             ST.ST_NAME,\n",
    "             TT.TT_NAME,\n",
    "             T.T_IS_CASH,\n",
    "             T.T_QTY,\n",
    "             T.T_BID_PRICE,\n",
    "             T.T_EXEC_NAME,\n",
    "             T.T_TRADE_PRICE,\n",
    "             T.T_CA_ID,\n",
    "             T.T_S_SYMB,\n",
    "             TH.TH_DTS,\n",
    "             T.T_CHRG,\n",
    "             T.T_COMM,\n",
    "             T.T_TAX\n",
    "             FROM trade_view T\n",
    "             INNER JOIN tradeHistory TH ON T.T_ID = TH.TH_T_ID\n",
    "             INNER JOIN StatusType ST ON T.T_ST_ID = ST.ST_ID\n",
    "             INNER JOIN TradeType TT ON T.T_TT_ID = TT.TT_ID\n",
    "    \"\"\")\n",
    "    create_date_dim = spark.sql(\"\"\"\n",
    "        SELECT SK_DateID as SK_CreateDateID, DateValue FROM DimDate\n",
    "    \"\"\")\n",
    "    create_time_dim = spark.sql(\"\"\"\n",
    "        SELECT SK_TimeID as SK_CreateTimeID, TimeValue FROM DimTime\n",
    "    \"\"\")\n",
    "    close_date_dim = spark.sql(\"\"\"\n",
    "        SELECT SK_DateID as SK_CloseDateID, DateValue FROM DimDate\n",
    "    \"\"\")\n",
    "    close_time_dim = spark.sql(\"\"\"\n",
    "        SELECT SK_TimeID as SK_CloseTimeID, TimeValue FROM DimTime\n",
    "    \"\"\")\n",
    "\n",
    "#     trade.show()\n",
    "    \n",
    "    trade = (\n",
    "    trade\n",
    "    .groupBy( 'T_ID', 'ST_NAME', 'TT_NAME', 'T_IS_CASH','T_CA_ID','T_S_SYMB','TH_DTS', 'T_QTY', 'T_BID_PRICE', 'T_EXEC_NAME', 'T_TRADE_PRICE', 'T_CHRG', 'T_COMM', 'T_TAX')\n",
    "    .agg(\n",
    "        collect_set(col('create_date')).alias('create_date'), \n",
    "        collect_set(col('create_time')).alias('create_time'),\n",
    "        collect_set(col('close_date')).alias('close_date'),\n",
    "        collect_set(col('close_time')).alias('close_time')\n",
    "    )\n",
    "    .select(\n",
    "        expr('filter(create_date, element -> element is not null)')[0].alias('create_date'),\n",
    "        expr('filter(create_time, element -> element is not null)')[0].alias('create_time'),\n",
    "        expr('filter(close_date, element -> element is not null)')[0].alias('close_date'),\n",
    "        expr('filter(close_time, element -> element is not null)')[0].alias('close_time'),\n",
    "'T_ID', 'ST_NAME', 'TT_NAME', 'T_IS_CASH','T_CA_ID','T_S_SYMB','TH_DTS', 'T_QTY', 'T_BID_PRICE', 'T_EXEC_NAME', 'T_TRADE_PRICE', 'T_CHRG', 'T_COMM', 'T_TAX'\n",
    "    )\n",
    ")\n",
    "\n",
    "    # Join with date\n",
    "    trade = trade.join(create_date_dim, to_date(create_date_dim.DateValue) == to_date(trade.create_date), \"left\").join(create_time_dim, date_format(create_time_dim.TimeValue, \"HH:mm:ss\") == date_format(trade.create_time, \"HH:mm:ss\"), \"left\").join(close_date_dim, to_date(close_date_dim.DateValue) == to_date(trade.create_date), \"left\").join(close_time_dim, date_format(close_time_dim.TimeValue, \"HH:mm:ss\") == date_format(trade.create_time, \"HH:mm:ss\"), \"left\")\n",
    "\n",
    "    # Create new view\n",
    "    trade.createOrReplaceTempView(\"trade_insert\")\n",
    "    #trade.printSchema()\n",
    "    \n",
    "    dimTrade=spark.sql(\"\"\"\n",
    "        SELECT  INT(T_ID) as TradeID,\n",
    "         da.SK_BrokerID  as SK_BrokerID,\n",
    "         SK_CreateDateID as SK_CreateDateID,\n",
    "         SK_CreateTimeID as SK_CreateTimeID,\n",
    "         SK_CloseDateID as SK_CloseDateID,\n",
    "         SK_CloseTimeID as SK_CloseTimeID,\n",
    "         ST_NAME as Status,\n",
    "         TT_NAME as Type,\n",
    "         T_IS_CASH as CashFlag,\n",
    "         ds.SK_SecurityID as SK_SecurityID,\n",
    "         ds.SK_CompanyID as SK_CompanyID,\n",
    "         T_QTY as Quantity,\n",
    "         T_BID_PRICE as BidPrice,\n",
    "         da.SK_CustomerID as SK_CustomerID,\n",
    "         da.SK_AccountID as SK_AccountID,\n",
    "         T_EXEC_NAME as ExecutedBy,\n",
    "         T_TRADE_PRICE as TradePrice,\n",
    "         T_CHRG as Fee,\n",
    "         T_COMM as Comission,\n",
    "         T_TAX as Tax,\n",
    "         3 as BatchID\n",
    "        FROM trade_insert inner join DimAccount as da on trade_insert.T_CA_ID = da.SK_AccountID \n",
    "        AND trade_insert.TH_DTS BETWEEN da.EffectiveDate AND da.EndDate inner join DimSecurity\n",
    "        as ds on (trade_insert.T_S_SYMB = ds.Symbol AND trade_insert.TH_DTS BETWEEN ds.EffectiveDate AND ds.EndDate)\n",
    "    \"\"\")\n",
    "  \n",
    "    #dimAccount.printSchema()\n",
    "    \n",
    "    dimTrade.write.mode(\"append\").saveAsTable( \"DimTrade\", mode=\"append\")\n",
    "    \n",
    "    return dimTrade\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Facts\n",
    "# MAGIC ### Fact Cash Balances\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def load_update_fact_cash_balances_2(dbname, staging_area_folder_upl):\n",
    "    #spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "            `CDC_FLAG` String,\n",
    "            `CDC_DSN` String,\n",
    "            `CT_CA_ID` INTEGER,\n",
    "            `CT_DTS` TIMESTAMP,\n",
    "            `CT_AMT` FLOAT,\n",
    "            `CT_NAME` STRING\n",
    "    \"\"\"\n",
    "    cash = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"|\")\n",
    "        .schema(schema)\n",
    "        .load(f\"{staging_area_folder_upl}/CashTransaction.txt\")\n",
    "    )\n",
    "    \n",
    "    cash.createOrReplaceTempView(\"cashTrans\")\n",
    "    # cash.show(2)\n",
    "    factCashBalances = spark.sql(\"\"\" \n",
    "                       Select SK_CustomerID, \n",
    "                           AccountID AS SK_AccountID, \n",
    "                           SK_DateID, \n",
    "                           sum(CT_AMT) as Cash, \n",
    "                           CAST('3' as INT) as BatchID \n",
    "                       From cashTrans join DimAccount as ac on (CT_CA_ID =ac.AccountID) \n",
    "                       join DimDate as dt on dt.DateValue = Date(CT_DTS) \n",
    "                       Group by AccountID, SK_CustomerID, SK_DateID\"\"\")\n",
    "    \n",
    "    # factCashBalances.show(2)\n",
    "    \n",
    "    factCashBalances.write.option(\"append\", \"true\").saveAsTable(\"FactCashBalances\", mode=\"append\")\n",
    "#     factCashBalances.show(3)\n",
    "    return factCashBalances\n",
    "\n",
    "# load_fact_cash_balances(\"FactCashBalances\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Fact Holdings\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def load_update_fact_holdings_2(dbname, staging_area_folder_upl):\n",
    "    #spark.sql(f\"USE {dbname}\")\n",
    "    schema = \"\"\"\n",
    "            `CDC_FLAG` String,\n",
    "            `CDC_DSN` String,\n",
    "            `HH_H_T_ID` INTEGER,\n",
    "            `HH_T_ID` INTEGER,\n",
    "            `HH_BEFORE_QTY` FLOAT,\n",
    "            `HH_AFTER_QTY` FLOAT\n",
    "    \"\"\"\n",
    "    holding = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"delimiter\", \"|\")\n",
    "        .schema(schema)\n",
    "        .load(f\"{staging_area_folder_upl}/HoldingHistory.txt\")\n",
    "    )\n",
    "    holding.createOrReplaceTempView(\"holdings\")\n",
    "    factHoldings = spark.sql(\"\"\" \n",
    "                       Select \n",
    "                       SK_CustomerID, \n",
    "                       SK_AccountID, \n",
    "                       SK_SecurityID, \n",
    "                       SK_CompanyID,\n",
    "                       TradePrice as CurrentPrice,\n",
    "                       SK_CloseDateID as SK_DateID ,\n",
    "                       SK_CloseTimeID as SK_TimeID,\n",
    "                       HH_H_T_ID as TradeId,\n",
    "                       HH_T_ID as CurrentTradeID,\n",
    "                       HH_AFTER_QTY as CurrentHolding,\n",
    "                       CAST('3' as INT) as BatchID \n",
    "                       From holdings join DimTrade as ac on (HH_T_ID =ac.TradeID)\"\"\")\n",
    "    \n",
    "    \n",
    "    factHoldings.write.option(\"append\", \"true\").saveAsTable(\"FactHoldings\", mode=\"append\")\n",
    "    factHoldings.show(2)\n",
    "    \n",
    "    return factHoldings\n",
    "\n",
    "# load_fact_holdings(\"FactHoldings\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC #### FactWatches\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\"\"\"\n",
    "            SK_CustomerID BIGINT,\n",
    "            SK_SecurityID BIGINT,\n",
    "            SK_DateID_DatePlaced BIGINT,\n",
    "            SK_DateID_DateRemoved BIGINT,\n",
    "            BatchID INTEGER\n",
    "            \n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "def load_update_fact_watches_2(dbname, staging_area_folder_upl):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    # Customer ID, Ticker symbol, Datetime, activate or cancel watch\n",
    "    schema = \"\"\"\n",
    "            `CDC_FLAG` String,\n",
    "            `CDC_DSN` String,\n",
    "            `W_C_ID` BIGINT, \n",
    "            `W_S_SYMB` STRING,\n",
    "            `W_DTS` DATE,\n",
    "            `W_ACTION` STRING\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.read.format(\"csv\") \\\n",
    "        .option(\"delimiter\", \"|\") \\\n",
    "        .schema(schema) \\\n",
    "        .load(f\"{staging_area_folder_upl}/WatchHistory.txt\") \\\n",
    "    .createOrReplaceTempView(\"watches\")\n",
    "    \n",
    "    actv_watches = spark.sql(\"SELECT * FROM watches\").where(col(\"W_ACTION\") == 'ACTV')\n",
    "    actv_watches.createOrReplaceTempView(\"actv_watches\")\n",
    "    cncl_watches = spark.sql(\"SELECT * FROM watches\").where(col(\"W_ACTION\") == 'CNCL')\n",
    "    cncl_watches.createOrReplaceTempView(\"cncl_watches\")\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT w1.W_C_ID, w1.W_S_SYMB, w1.W_DTS AS DatePlaced, w2.W_DTS AS DateRemoved FROM actv_watches w1 LEFT JOIN cncl_watches w2 ON w1.W_C_ID = w2.W_C_ID AND w1.W_S_SYMB = w2.W_S_SYMB\n",
    "    \"\"\").createOrReplaceTempView(\"watches\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "        INSERT INTO FactWatches(SK_CustomerID, SK_SecurityID, SK_DateID_DatePlaced, \n",
    "                                SK_DateID_DateRemoved, BatchID)\n",
    "            SELECT \n",
    "                c.SK_CustomerID,\n",
    "                s.SK_SecurityID,\n",
    "                d1.SK_DateID AS SK_DateID_DatePlaced, \n",
    "                d2.SK_DateID AS SK_DateID_DateRemoved,\n",
    "                3 AS BatchID\n",
    "            FROM watches w LEFT JOIN \n",
    "                DimDate d1 ON DatePlaced = d1.DateValue LEFT JOIN \n",
    "                DimDate d2 ON DateRemoved = d2.DateValue LEFT JOIN\n",
    "                DimSecurity s ON (\n",
    "                    W_S_SYMB = s.Symbol AND \n",
    "                    s.isCurrent = True\n",
    "                ) LEFT JOIN \n",
    "                DimCustomer c ON (\n",
    "                    W_C_ID = c.CustomerID AND\n",
    "                    c.IsCurrent = True\n",
    "                )\n",
    "    \"\"\")\n",
    "    \n",
    "    return spark.sql(\"SELECT * FROM FactWatches WHERE BatchID=3\")\n",
    "\n",
    "# spark.sql(\"USE test\")\n",
    "# spark.sql(\"DELETE FROM FactWatches WHERE BatchID=2\")\n",
    "# watches= load_fact_watches(\"test\")\n",
    "# watches.where(\"BatchID = 2\").limit(10).toPandas()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## FactMarketistory\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def load_update_staging_FactMarketStory_2(dbname, staging_area_folder_upl):\n",
    "    \n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    # spark.sql(\"\"\"DROP TABLE FactMarketHistory\"\"\")\n",
    "\n",
    "    # create_fact_market_history(dbname)\n",
    "\n",
    "    schema = \"\"\"\n",
    "        `CDC_FLAG` String,\n",
    "        `CDC_DSN` String,\n",
    "        `DM_DATE` DATE,\n",
    "        `DM_S_SYMB` STRING,\n",
    "        `DM_CLOSE` FLOAT,\n",
    "        `DM_HIGH` FLOAT,\n",
    "        `DM_LOW` FLOAT,\n",
    "        `DM_VOL` INTEGER\n",
    "    \"\"\"\n",
    "\n",
    "    DailyMarket_ = spark.read.format(\"csv\").option(\"delimiter\", \"|\").schema(schema).load(f\"{staging_area_folder_upl}/DailyMarket.txt\")\n",
    "    DailyMarket_.createOrReplaceTempView(\"dailymarket\")\n",
    "    \n",
    "    # TODO: DI Message\n",
    "    DailyMarket_ = spark.sql(\n",
    "        \"\"\"\n",
    "        WITH DailyMarket AS (\n",
    "            SELECT DM.*, MIN(dm2.DM_DATE) as FiftyTwoWeekHighDate, MIN(dm3.DM_DATE) as FiftyTwoWeekLowDate\n",
    "            FROM (\n",
    "             SELECT dm.DM_CLOSE,\n",
    "                dm.DM_S_SYMB,\n",
    "                dm.DM_HIGH,\n",
    "                dm.DM_LOW,\n",
    "                dm.DM_VOL,\n",
    "                dm.DM_DATE,\n",
    "                max(dm.DM_HIGH) OVER (\n",
    "                    PARTITION BY dm.DM_S_SYMB\n",
    "                    ORDER BY CAST(dm.DM_DATE AS timestamp)\n",
    "                    RANGE BETWEEN INTERVAL 364 DAYS PRECEDING AND CURRENT ROW\n",
    "                 ) AS FiftyTwoWeekHigh,\n",
    "                 min(dm.DM_LOW) OVER (\n",
    "                    PARTITION BY dm.DM_S_SYMB\n",
    "                    ORDER BY CAST(dm.DM_DATE AS timestamp)\n",
    "                    RANGE BETWEEN INTERVAL 364 DAYS PRECEDING AND CURRENT ROW\n",
    "                 ) AS FiftyTwoWeekLow\n",
    "                 FROM dailymarket dm\n",
    "            ) DM INNER JOIN dailymarket dm2 ON DM.FiftyTwoWeekHigh = dm2.DM_HIGH AND dm2.DM_DATE BETWEEN date_sub(DM.DM_DATE, 364) AND DM.DM_DATE\n",
    "         INNER JOIN dailymarket dm3 ON DM.FiftyTwoWeekLow = dm3.DM_LOW AND dm3.DM_DATE BETWEEN date_sub(DM.DM_DATE, 364) AND dm.DM_DATE\n",
    "            GROUP BY DM.DM_DATE, DM.DM_CLOSE, DM.DM_HIGH, DM.DM_LOW, DM.DM_VOL, DM.FiftyTwoWeekHigh, DM.FiftyTwoWeekLow, dm.DM_S_SYMB\n",
    "        ), FinData AS (\n",
    "            SELECT\n",
    "            SK_CompanyID,\n",
    "            SUM(FI_BASIC_EPS) OVER (\n",
    "                PARTITION BY FI_QTR\n",
    "                ORDER BY FI_YEAR, FI_QTR\n",
    "                ROWS BETWEEN 4 PRECEDING AND CURRENT ROW\n",
    "            ) as Eps\n",
    "            From Financial\n",
    "        ), CompEarning AS (\n",
    "            SELECT dc.CompanyID, fd.Eps\n",
    "            FROM DimCompany dc\n",
    "            INNER JOIN FinData fd ON dc.CompanyID = fd.SK_CompanyID\n",
    "        )\n",
    "        SELECT  cast(dm.DM_CLOSE as float) as ClosePrice,\n",
    "                cast(dm.DM_HIGH as float) as DayHigh,\n",
    "                cast(dm.DM_LOW as float) as DayLow,\n",
    "                cast(dm.DM_VOL as int) as Volume,\n",
    "                cast(ds.SK_SecurityID as int) as SK_SecurityID,\n",
    "                cast(ds.SK_CompanyID as int) as SK_CompanyID,\n",
    "                cast(dd1.SK_DateID as int) as SK_DateID,\n",
    "                cast(dd2.SK_DateID as int) as SK_FiftyTwoWeekHighDate,\n",
    "                cast(dd3.SK_DateID as int) as SK_FiftyTwoWeekLowDate,\n",
    "                cast(dm.FiftyTwoWeekHigh as float) as FiftyTwoWeekHigh,\n",
    "                cast(dm.FiftyTwoWeekLow as float) as FiftyTwoWeekLow,\n",
    "                cast(((ds.dividend / dm.DM_CLOSE) * 100.0) as float) as Yield,\n",
    "                CASE \n",
    "                    WHEN ISNULL(ce.Eps) or ce.Eps = 0 THEN NULL \n",
    "                    ELSE cast((dm.DM_CLOSE / ce.Eps) as float)\n",
    "                END as PERatio,\n",
    "                cast('3' as int) as BatchID\n",
    "        FROM DailyMarket dm\n",
    "        INNER JOIN DimSecurity ds ON ds.Symbol = dm.DM_S_SYMB AND ds.IsCurrent = 1\n",
    "        INNER JOIN DimDate dd1 ON dd1.DateValue = dm.DM_DATE\n",
    "        INNER JOIN DimDate dd2 ON dd2.DateValue = dm.FiftyTwoWeekHighDate\n",
    "        INNER JOIN DimDate dd3 ON dd3.DateValue = dm.FiftyTwoWeekLowDate\n",
    "        LEFT JOIN CompEarning ce ON ds.SK_CompanyID = ce.CompanyID\n",
    "         \"\"\")\n",
    "    \n",
    "    DailyMarket_.createOrReplaceTempView(\"dailymarket_insert\")\n",
    "    spark.sql(\"\"\"\n",
    "               INSERT INTO FactMarketHistory(ClosePrice, DayHigh, DayLow, Volume, SK_SecurityID, SK_CompanyID, SK_DateID, SK_FiftyTwoWeekHighDate, SK_FiftyTwoWeekLowDate,  FiftyTwoWeekHigh, FiftyTwoWeekLow, Yield, PERatio, BatchID)\n",
    "       SELECT * FROM dailymarket_insert\n",
    "    \"\"\")\n",
    "    \n",
    "    return spark.sql(\"\"\"\n",
    "        SELECT * FROM FactMarketHistory WHERE BatchID = 3\n",
    "    \"\"\")\n",
    "\n",
    "def load_update_staging_Prospect_2(dbname, staging_area_folder_upl):\n",
    "    spark.sql(f\"USE {dbname}\")\n",
    "    # spark.sql(\"\"\"DROP TABLE Prospect \"\"\")\n",
    "\n",
    "    # create_prospect_table(dbname)\n",
    "\n",
    "    schema = \"\"\"\n",
    "        `AgencyID` String,\n",
    "        `LastName` String,\n",
    "        `FirstName` String,\n",
    "        `MiddleInitial` String,\n",
    "        `Gender` String,\n",
    "        `AddressLine1` String,\n",
    "        `AddressLine2` String,\n",
    "        `PostalCode` String,\n",
    "        `City` String,\n",
    "        `State` String,\n",
    "        `Country` String,\n",
    "        `Phone` String,\n",
    "        `Income` Integer,\n",
    "        `NumberCars` Integer,\n",
    "        `NumberChildren` Integer,\n",
    "        `MaritalStatus` String,\n",
    "        `Age` Integer,\n",
    "        `CreditRating` Integer,\n",
    "        `OwnOrRentFlag` String,\n",
    "        `Employer` String,\n",
    "        `NumberCreditCards` Integer,\n",
    "        `NetWorth` Integer\n",
    "    \"\"\"\n",
    "    Prospect_ = spark.read.format(\"csv\").option(\"delimiter\", \",\").schema(schema).load(f\"{staging_area_folder_upl}/Prospect.csv\")\n",
    "    \n",
    "    udf_marketing = udf(lambda row: get_marketingnameplate(row), StringType())\n",
    "    Prospect_ = Prospect_.withColumn('MarketingNameplate', udf_marketing(struct([Prospect_[x] for x in Prospect_.columns])))\n",
    "    \n",
    "    now = datetime.utcnow()\n",
    "    \n",
    "    DimDate = spark.sql(\"\"\"\n",
    "        SELECT SK_DateID FROM DimDate WHERE SK_DateID = 20201231\n",
    "    \"\"\")\n",
    "    Prospect_ = Prospect_.crossJoin(DimDate)\n",
    "    Prospect_.createOrReplaceTempView(\"Prospect_\")\n",
    "    \n",
    "    Prospect_ = spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT p.AgencyID as AgencyID, \n",
    "               3 as BatchID, \n",
    "               CASE\n",
    "                   WHEN dc.Status = 'ACTIVE' THEN True ELSE False\n",
    "               END as IsCustomer,\n",
    "               p.SK_DateID as SK_RecordDateID,\n",
    "               p.SK_DateID as SK_UpdateDateID,\n",
    "               p.LastName as LastName,\n",
    "               p.FirstName as FirstName,\n",
    "               p.MiddleInitial as MiddleInitial,\n",
    "               p.Gender as Gender,\n",
    "               p.AddressLine1 as AddressLine1,\n",
    "               p.AddressLine2 as AddressLine2,\n",
    "               p.PostalCode as PostalCode,\n",
    "               p.City as City,\n",
    "               p.State as State,\n",
    "               p.Country as Country,\n",
    "               p.Phone as Phone,\n",
    "               p.Income as Income,\n",
    "               p.NumberCars as NumberCars,\n",
    "               p.NumberChildren as NumberChildren,\n",
    "               p.MaritalStatus as MaritalStatus,\n",
    "               p.Age as Age,\n",
    "               p.CreditRating as CreditRating,\n",
    "               p.OwnOrRentFlag as OwnOrRentFlag,\n",
    "               p.Employer as Employer,\n",
    "               p.NumberCreditCards as NumberCreditCards,\n",
    "               p.NetWorth as NetWorth,\n",
    "               p.MarketingNameplate as MarketingNameplate\n",
    "        FROM Prospect_ p\n",
    "        LEFT JOIN DimCustomer dc ON \n",
    "        upper(p.FirstName) = upper(dc.FirstName) AND upper(p.LastName) = upper(dc.LastName)\n",
    "        AND upper(p.AddressLine1) = upper(dc.AddressLine1) AND upper(p.AddressLine2) = upper(dc.AddressLine2)\n",
    "        AND upper(p.PostalCode) = upper(dc.PostalCode)\n",
    "    \"\"\")\n",
    "    Prospect_.createOrReplaceTempView(\"Prospect_\")\n",
    "\n",
    "    combined_prospect = spark.sql(\"\"\"\n",
    "                CREATE TABLE CombinedProspect AS\n",
    "                SELECT\n",
    "                    COALESCE(p.AgencyID, np.AgencyID) AS AgencyID,\n",
    "                    CAST(COALESCE(p.SK_RecordDateID, np.SK_RecordDateID) AS INT) AS SK_RecordDateID,\n",
    "                    CAST(COALESCE(p.SK_UpdateDateID, np.SK_UpdateDateID) AS INT) AS SK_UpdateDateID,\n",
    "                    COALESCE(p.BatchID, np.BatchID) AS BatchID,\n",
    "                    COALESCE(p.IsCustomer, np.IsCustomer) AS IsCustomer,\n",
    "                    COALESCE(np.LastName, p.LastName) AS LastName,\n",
    "                    COALESCE(np.FirstName, p.FirstName) AS FirstName,\n",
    "                    COALESCE(np.MiddleInitial, p.MiddleInitial) AS MiddleInitial,\n",
    "                    COALESCE(np.Gender, p.Gender) AS Gender,\n",
    "                    COALESCE(np.AddressLine1, p.AddressLine1) AS AddressLine1,\n",
    "                    COALESCE(np.AddressLine2, p.AddressLine2) AS AddressLine2,\n",
    "                    COALESCE(np.PostalCode, p.PostalCode) AS PostalCode,\n",
    "                    COALESCE(np.City, p.City) AS City,\n",
    "                    COALESCE(np.State, p.State) AS State,\n",
    "                    COALESCE(np.Country, p.Country) AS Country,\n",
    "                    COALESCE(np.Phone, p.Phone) AS Phone,\n",
    "                    COALESCE(np.Income, p.Income) AS Income,\n",
    "                    COALESCE(np.NumberCars, p.NumberCars) AS NumberCars,\n",
    "                    COALESCE(np.NumberChildren, p.NumberChildren) AS NumberChildren,\n",
    "                    COALESCE(np.MaritalStatus, p.MaritalStatus) AS MaritalStatus,\n",
    "                    COALESCE(np.Age, p.Age) AS Age,\n",
    "                    COALESCE(np.CreditRating, p.CreditRating) AS CreditRating,\n",
    "                    COALESCE(np.OwnOrRentFlag, p.OwnOrRentFlag) AS OwnOrRentFlag,\n",
    "                    COALESCE(np.Employer, p.Employer) AS Employer,\n",
    "                    COALESCE(np.NumberCreditCards, p.NumberCreditCards) AS NumberCreditCards,\n",
    "                    COALESCE(np.NetWorth, p.NetWorth) AS NetWorth,\n",
    "                    COALESCE(np.MarketingNameplate, p.MarketingNameplate) AS MarketingNameplate\n",
    "                FROM\n",
    "                    Prospect p\n",
    "                FULL OUTER JOIN \n",
    "                    Prospect_ np\n",
    "                ON \n",
    "                    p.AgencyID = np.AgencyID;\n",
    "    \"\"\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "            INSERT OVERWRITE TABLE Prospect\n",
    "            SELECT * FROM CombinedProspect\n",
    "              \"\"\")\n",
    "\n",
    "    spark.sql(\"\"\"DROP TABLE CombinedProspect\"\"\") \n",
    "\n",
    "    return spark.sql(\"\"\"\n",
    "        SELECT * FROM Prospect WHERE BatchID = 3\n",
    "    \"\"\")\n",
    "\n",
    "def run_incremental_load_2(dbname, scale_factor, file_id, result_queue):\n",
    "    metrics = {}\n",
    "\n",
    "\n",
    "    staging_area_folder = f\"{os.getcwd()}/data/{scale_factor}/Batch3\"\n",
    "\n",
    "    # Run incremental update\n",
    "    print(\"Executing incremental load 2\")\n",
    "    start_time = time.time()\n",
    "    customer = load_dimen_customer_2(dbname, staging_area_folder)\n",
    "    account = load_dimen_account_2(dbname, staging_area_folder)\n",
    "    dimtrade = load_update_dimen_trade_2(dbname, staging_area_folder)\n",
    "\n",
    "    factcashbalance = load_update_fact_cash_balances_2(dbname, staging_area_folder)\n",
    "    holding = load_update_fact_holdings_2(dbname, staging_area_folder)\n",
    "    watch = load_update_fact_watches_2(dbname, staging_area_folder)\n",
    "\n",
    "    factmarkethistory =load_update_staging_FactMarketStory_2(dbname, staging_area_folder)\n",
    "    prospect = load_update_staging_Prospect_2(dbname, staging_area_folder)\n",
    "\n",
    "    # End the timer\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Incremental load execution time: {execution_time:.6f} seconds\")\n",
    "\n",
    "\n",
    "    factmarkethistory_count = factmarkethistory.count()\n",
    "    prospect_count = prospect.count()\n",
    "    dimtrade_count = dimtrade.count()\n",
    "    factcashbalance_count = factcashbalance.count()\n",
    "    holding_count = holding.count()\n",
    "    watch_count = watch.count()\n",
    "    customer_count = customer.count()\n",
    "    account_count = account.count()\n",
    "\n",
    "    # Sum the individual counts\n",
    "    rows = factmarkethistory_count + prospect_count + dimtrade_count + factcashbalance_count + holding_count + watch_count + customer_count + account_count\n",
    "\n",
    "    metrics[\"rows\"] = rows\n",
    "    metrics[\"throughput\"] = (rows / get_max(execution_time,1800))\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics, index=[0])\n",
    "    metrics_df.to_csv(f\"{os.getcwd()}/results/data/incremental_load_2_{scale_factor}_{file_id}.csv\", index=False)\n",
    "\n",
    "    result_queue.put(metrics_df)  # Put the result in the queue\n",
    "\n",
    "    #return metrics_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# spark.sql(f\"USE {\"test\"}\")\n",
    "# spark.sql(\"SELECT * FROM dimessages\")\n",
    "\n",
    "\n",
    "#try:\n",
    "\n",
    "# run()\n",
    "#except Exception as e:\n",
    "#    print(e)\n",
    "#    input()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 5: Automated Audit Phase\n",
    "# MAGIC This step:\n",
    "# MAGIC - audit data is loaded into the audit table following these rules:\n",
    "# MAGIC   - The first row in every audit data file contains only the field names, not audit data. This record may be used to aid in the load process, but must not be loaded into the Audit table.\n",
    "# MAGIC   - Each field in the audit data must be loaded into the cooresponding column (the column of the same name) of the Audit table.\n",
    "# MAGIC - it's valid to create helper functions to aid performance of automated audit\n",
    "# MAGIC - contents of data warehouse must not be modified on this stage. \n",
    "# MAGIC - at the beginning of this step, data visibility query 1 (appendix C from the manual) must be executed.\n",
    "# MAGIC - audit query must be executed (appendix A).\n",
    "# MAGIC - a valid benchmark run must report \"OK\" for every test.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution function visibility query 1\n",
    "def execute_visibility_query_1(spark,visibility_query1=tpcdi_visibility_q1):\n",
    "    print(\"Starting visibility queries 1...\")\n",
    "    # Start measure the time execution\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < 20:  # Run queries for the duration of the load function\n",
    "        # Simulate executing the query\n",
    "        print(f\"Executing visibility 1 query at {time.strftime('%Y-%m-%d %H:%M:%S')}...\")\n",
    "        start_time_query = time.time()\n",
    "        # execute the query\n",
    "        result=spark.sql(visibility_query1)\n",
    "        # calculate the time execution\n",
    "        elapsed = time.time() - start_time_query\n",
    "        print(f\"Execution time query:{elapsed}\")\n",
    "        time.sleep(5)# Wait 5 seconds before running the next query\n",
    "\n",
    "# Execution function visibility query 2\n",
    "def execute_visibility_query_2(spark,visibility_query2=tpcdi_visibility_q2):\n",
    "    print(\"Starting visibility queries 2...\")\n",
    "    # Start measure the time execution\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < 20:  # Run queries for the duration of the load function\n",
    "        # Simulate executing the query\n",
    "        print(f\"Executing visibility 2 query at {time.strftime('%Y-%m-%d %H:%M:%S')}...\")\n",
    "        start_time_query = time.time()\n",
    "        # execute the query\n",
    "        result=spark.sql(visibility_query2)\n",
    "        # calculate the time execution\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Execution time query:{elapsed}\")\n",
    "        time.sleep(5)# Wait 5 seconds before running the next query\n",
    "\n",
    "#########################################################################\n",
    "#                                                                       #\n",
    "#                         VALIDITY QUERIES                              #        \n",
    "#                                                                       #    \n",
    "#########################################################################\n",
    "\n",
    "\n",
    " # The batch validation query writes results into the DImessages table defined in Clause 3.2.8,\n",
    " # which are used in the automated audit phase to validate the transformed data in the Data\n",
    " # Warehouse.\n",
    "\n",
    "# Execution function validity query\n",
    "def execute_validity_query_1(spark,validity_query=tpcdi_validation_query_1):\n",
    "    # Start measure the time execution\n",
    "    start_time = time.time()\n",
    "    result=spark.sql(validity_query)\n",
    "    # calculate the time execution\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Execution time validation query 1:{elapsed}\")\n",
    "\n",
    "\n",
    "# Execution function validity query\n",
    "def execute_validity_query_2(spark,validity_query=tpcdi_validation_query_2):\n",
    "    # Start measure the time execution\n",
    "    start_time = time.time()\n",
    "    result=spark.sql(validity_query)\n",
    "    # calculate the time execution\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Execution time query validation query 2:{elapsed}\")\n",
    "\n",
    "\n",
    "# Run the data load and visibility queries concurrently\n",
    "def run_data_load_with_visibility_queries(dbname, scale_factor, file_id,incremental_load_func, visibility_query_func):\n",
    "    # Create a queue to store the load_data result\n",
    "    result_queue = queue.Queue()\n",
    "    \n",
    "    # Create threads for data load and visibility queries\n",
    "    data_incremental_load_thread = threading.Thread(target=incremental_load_func, args=(dbname, scale_factor, file_id, result_queue,))\n",
    "    visibility_thread = threading.Thread(target=visibility_query_func)\n",
    "\n",
    "    # Start both threads\n",
    "    data_incremental_load_thread.start()\n",
    "    visibility_thread.start()\n",
    "\n",
    "    # Wait for both threads to finish\n",
    "    data_incremental_load_thread.join()\n",
    "    df_metrics = result_queue.get()  # Retrieve the DataFrame from the queue\n",
    "    visibility_thread.join()\n",
    "    # Return the loaded data as the final output\n",
    "    return df_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audit_data(staging_area_folder):\n",
    "    files = os.listdir(staging_area_folder)\n",
    "    columns = [\"DataSet\", \"BatchID\", \"Date\", \"Attribute\", \"Value\", \"DValue\"]\n",
    "    for file in files:\n",
    "        if \"audit\" in file:\n",
    "            audit_data = spark.read.csv(f\"{staging_area_folder}/{file}\", header=True, sep=\",\")\n",
    "            audit_data = audit_data.toDF(*columns)\n",
    "            audit_data.createOrReplaceTempView(\"audit_temp\")\n",
    "            audit_data = cast_to_target_schema(\"audit_temp\", \"Audit\")\n",
    "            audit_data.write.mode(\"append\").saveAsTable(\"Audit\", mode=\"append\")\n",
    "\n",
    "def load_queries(file_path):\n",
    "    with open(file_path) as file:\n",
    "        query = \" \".join(file.readlines())\n",
    "        return query\n",
    "\n",
    "def run_audit(dbname, scale_factor):\n",
    "    # This was already created at the start of the process\n",
    "    # create_audit_table(dbname)\n",
    "    batches = [\"Batch1\", \"Batch2\", \"Batch3\"]\n",
    "    file_path = './Audit Queries/tpcdi_audit.sql'\n",
    "    # for batch in batches:\n",
    "    #     staging_area_folder = f\"{os.getcwd()}/data/{scale_factor}/{batch}\"\n",
    "    #     load_audit_data(staging_area_folder)\n",
    "    spark.sql(load_queries(file_path)).show(200)\n",
    "    print(\"Audit Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Starting historical load\")\n",
    "clean_warehouse(\"test\")\n",
    "\n",
    "file_id = id_generator()\n",
    "# run_historical_load(dbname='test', scale_factor=\"Scale3\", file_id=file_id)\n",
    "# print(\"Historical load finished\")\n",
    "# dimessages=spark.sql(\"SELECT * FROM dimessages\")\n",
    "# dimessages.show()\n",
    "# Event to signal when the data loading starts\n",
    "data_load_start_event = threading.Event()\n",
    "\n",
    "def run(file_id=file_id,scale_factors=[\"Scale3\"], dbname = \"test\"):\n",
    "\n",
    "    for scale_factor in scale_factors:\n",
    "        metrics = {}\n",
    "        print(\"Starting historical load\")\n",
    "        hist_res = run_historical_load(dbname, scale_factor, file_id)\n",
    "        print(\"Historical load st:\")\n",
    "        print(hist_res)\n",
    "        print(\"Executing validation query historical load\")\n",
    "        execute_validity_query_1(spark=visibility_spark)\n",
    "        execute_validity_query_2(spark=visibility_spark)\n",
    "        # Starting data load 1\n",
    "        print(f\"Starting data load 1 at {time.strftime('%Y-%m-%d %H:%M:%S')}...\")\n",
    "        hist_incr_1=run_data_load_with_visibility_queries(\n",
    "        dbname, scale_factor, file_id,\n",
    "        incremental_load_func=run_incremental_load_1, \n",
    "        visibility_query_func=execute_visibility_query_1(spark=visibility_spark)\n",
    "        )\n",
    "        print(\"Historical incremental 1 st:\")\n",
    "        print(hist_incr_1)\n",
    "        print(\"Executing validation query incremental load 1\")\n",
    "        execute_validity_query_1(spark=visibility_spark)\n",
    "        execute_validity_query_2(spark=visibility_spark)\n",
    "        # starting data load 2\n",
    "        print(f\"Starting data load 2 at {time.strftime('%Y-%m-%d %H:%M:%S')}...\")\n",
    "        hist_incr_2=run_data_load_with_visibility_queries(\n",
    "        dbname, scale_factor, file_id,\n",
    "        incremental_load_func=run_incremental_load_2, \n",
    "        visibility_query_func=execute_visibility_query_2(spark=visibility_spark)\n",
    "        )\n",
    "        print(\"Historical incremental 2 st:\")\n",
    "        print(hist_incr_2)\n",
    "        print(\"Executing validation query incremental load 2\")\n",
    "        execute_validity_query_1(spark=visibility_spark)\n",
    "        execute_validity_query_2(spark=visibility_spark)\n",
    "\n",
    "        batches = [\"Batch1\", \"Batch2\", \"Batch3\"]\n",
    "        for batch in batches:\n",
    "            staging_area_folder = f\"{os.getcwd()}/data/{scale_factor}/{batch}\"\n",
    "            load_audit_data(staging_area_folder)\n",
    "\n",
    "        run_audit(dbname, scale_factor)\n",
    "        \n",
    "        metrics[\"TPC_DI_RPS\"] = int(geometric_mean([hist_res[\"throughput\"], hist_incr_1[\"throughput\"], hist_incr_2[\"throughput\"]]))\n",
    "        metrics_df = pd.DataFrame(metrics, index=[0])\n",
    "        metrics_df.to_csv(f\"{os.getcwd()}/results/data/overall_stats_{scale_factor}_{file_id}.csv\", index=False)\n",
    "\n",
    "\n",
    "run(file_id=file_id,scale_factors=[\"Scale3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batches = [\"Batch1\", \"Batch2\", \"Batch3\"]\n",
    "# for batch in batches:\n",
    "#     staging_area_folder = f\"{os.getcwd()}/data/{scale_factor}/{batch}\"\n",
    "#     load_audit_data(staging_area_folder)\n",
    "\n",
    "# query = input()\n",
    "# while query != \"quit\":\n",
    "#     try:\n",
    "#         query = input()\n",
    "#         if \"select\" in query.lower():\n",
    "#             spark.sql(query).show()\n",
    "#             run_audit(dbname, scale_factor)\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "\n",
    "# run_audit(\"test\", scale_factor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing dimessages table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Spark DataFrame to a Pandas DataFrame\n",
    "dimessages=spark.sql(\"SELECT * FROM dimessages\")\n",
    "dimessages_pandas = dimessages.toPandas()\n",
    "# Remove leading and trailing spaces from the column\n",
    "dimessages_pandas['MessageSource'] = dimessages_pandas['MessageSource'].str.strip()\n",
    "dimessages_pandas['MessageText'] = dimessages_pandas['MessageText'].str.strip()\n",
    "dimessages_pandas['MessageType'] = dimessages_pandas['MessageType'].str.strip()\n",
    "dimessages_pandas['MessageData'] = dimessages_pandas['MessageData'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimessages_pandas.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimessages_pandas[dimessages_pandas['MessageType']=='PCR']#[dimessages_pandas['MessageSource']=='DimAccount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
