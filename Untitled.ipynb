{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8b35156-6ee1-4606-8e22-199122b64257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/spark3/spark-3.5.3-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CELL 1\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74c1bdac-40fa-4bf8-8935-ba4504a553bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/13 23:21:42 WARN Utils: Your hostname, Medio-Metro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.54 instead (on interface en0)\n",
      "24/11/13 23:21:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/13 23:21:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/13 23:21:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "## CELL 2\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54d13025-7f18-4111-98ad-ba3661c0e536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/15 00:26:43 WARN Utils: Your hostname, Medio-Metro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.54 instead (on interface en0)\n",
      "24/11/15 00:26:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/15 00:26:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/15 00:26:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "Warehouse test deleted.\n",
      "Created dim account.\n",
      "Created dim broker.\n",
      "Created dim company.\n",
      "Created dim customer.\n",
      "Created dim date.\n",
      "Created dim security.\n",
      "Created dim time.\n",
      "Created dim trades.\n",
      "Created dim messages.\n",
      "Created fact cash balances\n",
      "Created fact holdings\n",
      "Created fact market history\n",
      "Created fact watches\n",
      "Created table industry.\n",
      "Created table Finacial.\n",
      "Created table Prospect.\n",
      "Created table StatusType.\n",
      "Created table TaxRate.\n",
      "Created table TradeType.\n",
      "Created table Audit.\n",
      "Created dim trades.\n",
      "Warehouse test deleted.\n",
      "Created dim account.\n",
      "Created dim broker.\n",
      "Created dim company.\n",
      "Created dim customer.\n",
      "Created dim date.\n",
      "Created dim security.\n",
      "Created dim time.\n",
      "Created dim trades.\n",
      "Created dim messages.\n",
      "Created fact cash balances\n",
      "Created fact holdings\n",
      "Created fact market history\n",
      "Created fact watches\n",
      "Created table industry.\n",
      "Created table Finacial.\n",
      "Created table Prospect.\n",
      "Created table StatusType.\n",
      "Created table TaxRate.\n",
      "Created table TradeType.\n",
      "Created table Audit.\n",
      "Created dim company.                                                            \n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1967Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1967Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1967Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1967Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1968Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1968Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1968Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1968Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1969Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1969Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1969Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1969Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1970Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1970Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1970Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1970Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1971Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1971Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1971Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1971Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1972Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1972Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1972Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1972Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1973Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1973Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1973Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1973Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1974Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1974Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1974Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1974Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1975Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1975Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1975Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1975Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1976Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1976Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1976Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1976Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1977Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1977Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1977Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1977Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1978Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1978Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1978Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1978Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1979Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1979Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1979Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1979Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1980Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1980Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1980Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1980Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1981Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1981Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1981Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1981Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1982Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1982Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1982Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1982Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1983Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1983Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1983Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1983Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1984Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1984Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1984Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1984Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1985Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1985Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1985Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1985Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1986Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1986Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1986Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1986Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1987Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1987Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1987Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1987Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1988Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1988Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1988Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1988Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1989Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1989Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1989Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1989Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1990Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1990Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1990Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1990Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1991Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1991Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1991Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1991Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1992Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1992Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1992Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1992Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1993Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1993Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1993Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1993Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1994Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1994Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1994Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1994Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1995Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1995Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1995Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1995Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1996Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1996Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1996Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1996Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1997Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1997Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1997Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1997Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1998Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1998Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1998Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1998Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1999Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1999Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1999Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE1999Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2000Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2000Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2000Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2000Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2001Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2001Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2001Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2001Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2002Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2002Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2002Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2002Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2003Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2003Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2003Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2003Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2004Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2004Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2004Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2004Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2005Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2005Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2005Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2005Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2006Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2006Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2006Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2006Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2007Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2007Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2007Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2007Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2008Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2008Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2008Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2008Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2009Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2009Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2009Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2009Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2010Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2010Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2010Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2010Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2011Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2011Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2011Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2011Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2012Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2012Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2012Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2012Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2013Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2013Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2013Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2013Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2014Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2014Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2014Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2014Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2015Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2015Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2015Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2015Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2016Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2016Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2016Q3\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2016Q4\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2017Q1\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2017Q2\n",
      "Processing /Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/FINWIRE2017Q3\n",
      "24/11/15 00:28:12 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/11/15 00:28:54 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "Created dim security.                                                           \n",
      "24/11/15 00:29:08 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "24/11/15 00:29:41 WARN DAGScheduler: Broadcasting large task binary with size 1932.1 KiB\n",
      "24/11/15 00:29:53 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "24/11/15 00:30:59 WARN DAGScheduler: Broadcasting large task binary with size 1994.7 KiB\n",
      "24/11/15 00:41:18 ERROR Executor: Exception in task 0.0 in stage 26.0 (TID 627) \n",
      "org.apache.spark.SparkException: Encountered error while reading file file:///Users/sebastianneri/Documents/BDMA/ULB/INFO-H419%20-%20Data%20warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/DailyMarket.txt. Details:\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.hadoop.fs.FSError: java.io.IOException: Operation canceled\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:284)\n",
      "\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 20 more\n",
      "Caused by: java.io.IOException: Operation canceled\n",
      "\tat java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.io.FileInputStream.read(FileInputStream.java:255)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 43 more\n",
      "24/11/15 00:41:18 WARN TaskSetManager: Lost task 0.0 in stage 26.0 (TID 627) (192.168.1.54 executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///Users/sebastianneri/Documents/BDMA/ULB/INFO-H419%20-%20Data%20warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/DailyMarket.txt. Details:\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.hadoop.fs.FSError: java.io.IOException: Operation canceled\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:284)\n",
      "\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 20 more\n",
      "Caused by: java.io.IOException: Operation canceled\n",
      "\tat java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.io.FileInputStream.read(FileInputStream.java:255)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 43 more\n",
      "\n",
      "24/11/15 00:41:18 ERROR TaskSetManager: Task 0 in stage 26.0 failed 1 times; aborting job\n",
      "24/11/15 00:41:18 WARN TaskSetManager: Lost task 0.0 in stage 31.0 (TID 629) (192.168.1.54 executor driver): TaskKilled (Stage cancelled: Job 26 cancelled )\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/TPC-DI.py\", line 2420, in <module>\n",
      "    \n",
      "    ^\n",
      "  File \"/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/TPC-DI.py\", line 2378, in run_historical_load\n",
      "    prospect = load_staging_Prospect(dbname, staging_area_folder)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/TPC-DI.py\", line 1454, in load_staging_FactMarketStory\n",
      "    INSERT INTO FactMarketHistory(ClosePrice, DayHigh, DayLow, Volume, SK_SecurityID, SK_CompanyID, SK_DateID, SK_FiftyTwoWeekHighDate, SK_FiftyTwoWeekLowDate,  FiftyTwoWeekHigh, FiftyTwoWeekLow, Yield, PERatio, BatchID)\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/spark3/spark-3.5.3-bin-hadoop3/python/pyspark/sql/session.py\", line 1631, in sql\n",
      "    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/spark3/spark-3.5.3-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "  File \"/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/spark3/spark-3.5.3-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/spark3/spark-3.5.3-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o29.sql.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 627) (192.168.1.54 executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///Users/sebastianneri/Documents/BDMA/ULB/INFO-H419%20-%20Data%20warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/DailyMarket.txt. Details:\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.hadoop.fs.FSError: java.io.IOException: Operation canceled\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:284)\n",
      "\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 20 more\n",
      "Caused by: java.io.IOException: Operation canceled\n",
      "\tat java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.io.FileInputStream.read(FileInputStream.java:255)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 43 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: org.apache.spark.SparkException: Encountered error while reading file file:///Users/sebastianneri/Documents/BDMA/ULB/INFO-H419%20-%20Data%20warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/DailyMarket.txt. Details:\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.hadoop.fs.FSError: java.io.IOException: Operation canceled\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:284)\n",
      "\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 20 more\n",
      "Caused by: java.io.IOException: Operation canceled\n",
      "\tat java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.io.FileInputStream.read(FileInputStream.java:255)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 43 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 TPC-DI.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a7577d-5ef0-48b8-aadd-acc91c42450a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 2.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall numpy\n",
    "!pip install numpy==1.26.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "495fa7f3-f48b-409d-b203-22bbed7ea1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65ae46b4-7ce8-4ac0-bd44-b788d9be8bd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dbutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdbutils\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dbutils'"
     ]
    }
   ],
   "source": [
    "import dbutils"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
