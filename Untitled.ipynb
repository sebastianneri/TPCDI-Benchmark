{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8b35156-6ee1-4606-8e22-199122b64257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/spark3/spark-3.5.3-bin-hadoop3'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CELL 1\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74c1bdac-40fa-4bf8-8935-ba4504a553bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/15 14:28:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/15 14:28:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "## CELL 2\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f9939c55-87d9-4951-8c5a-ae323b8aa469",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"\n",
    "        `DM_DATE` DATE,\n",
    "        `DM_S_SYMB` STRING,\n",
    "        `DM_CLOSE` FLOAT,\n",
    "        `DM_HIGH` FLOAT,\n",
    "        `DM_LOW` FLOAT,\n",
    "        `DM_VOL` INTEGER\n",
    "    \"\"\"\n",
    "scale_factor = 'Scale3'\n",
    "staging_area_folder = f\"{os.getcwd()}/data/{scale_factor}/Batch1\"\n",
    "DailyMarket_ = spark.read.format(\"csv\").option(\"delimiter\", \"|\").schema(schema).load(\"/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/DailyMarket.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e5274f5a-9d28-497b-b90c-765d531ff5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "54d13025-7f18-4111-98ad-ba3661c0e536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/15 16:34:17 WARN Utils: Your hostname, Medio-Metro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.54 instead (on interface en0)\n",
      "24/11/15 16:34:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/15 16:34:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Spark UI is available at http://192.168.1.54:4040\n",
      "Warehouse test deleted.\n",
      "Created dim account.\n",
      "Created dim broker.\n",
      "Created dim company.\n",
      "Created dim customer.\n",
      "Created dim date.\n",
      "Created dim security.\n",
      "Created dim time.\n",
      "Created dim trades.\n",
      "Created dim messages.\n",
      "Created fact cash balances\n",
      "Created fact holdings\n",
      "Created fact market history\n",
      "Created fact watches\n",
      "Created table industry.\n",
      "Created table Finacial.\n",
      "Created table Prospect.\n",
      "Created table StatusType.\n",
      "Created table TaxRate.\n",
      "Created table TradeType.\n",
      "Created table Audit.\n",
      "Created dim trades.\n",
      "Warehouse test deleted.\n",
      "Created dim account.\n",
      "Created dim broker.\n",
      "Created dim company.\n",
      "Created dim customer.\n",
      "Created dim date.\n",
      "Created dim security.\n",
      "Created dim time.\n",
      "Created dim trades.\n",
      "Created dim messages.\n",
      "Created fact cash balances\n",
      "Created fact holdings\n",
      "Created fact market history\n",
      "Created fact watches\n",
      "Created table industry.\n",
      "Created table Finacial.\n",
      "Created table Prospect.\n",
      "Created table StatusType.\n",
      "Created table TaxRate.\n",
      "Created table TradeType.\n",
      "Created table Audit.\n",
      "Created dim company.                                                            \n",
      "24/11/15 16:35:58 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/11/15 16:37:02 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "Created dim security.                                                           \n",
      "24/11/15 16:37:13 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "24/11/15 16:37:36 WARN DAGScheduler: Broadcasting large task binary with size 1932.1 KiB\n",
      "24/11/15 16:37:45 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "24/11/15 16:38:35 WARN DAGScheduler: Broadcasting large task binary with size 1994.7 KiB\n",
      "1                                                                               \n",
      "/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/DailyMarket.txt\n",
      "24/11/15 16:48:40 ERROR Executor: Exception in task 0.0 in stage 27.0 (TID 628)\n",
      "org.apache.spark.SparkException: Encountered error while reading file file:///Users/sebastianneri/Documents/BDMA/ULB/INFO-H419%20-%20Data%20warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/DailyMarket.txt. Details:\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.hadoop.fs.FSError: java.io.IOException: Operation canceled\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:284)\n",
      "\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 20 more\n",
      "Caused by: java.io.IOException: Operation canceled\n",
      "\tat java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.io.FileInputStream.read(FileInputStream.java:255)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 43 more\n",
      "24/11/15 16:48:40 WARN TaskSetManager: Lost task 0.0 in stage 27.0 (TID 628) (192.168.1.54 executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///Users/sebastianneri/Documents/BDMA/ULB/INFO-H419%20-%20Data%20warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/DailyMarket.txt. Details:\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.hadoop.fs.FSError: java.io.IOException: Operation canceled\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:284)\n",
      "\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 20 more\n",
      "Caused by: java.io.IOException: Operation canceled\n",
      "\tat java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.io.FileInputStream.read(FileInputStream.java:255)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 43 more\n",
      "\n",
      "24/11/15 16:48:40 ERROR TaskSetManager: Task 0 in stage 27.0 failed 1 times; aborting job\n",
      "24/11/15 16:48:40 WARN ColumnIndexStoreImpl: Unable to read offset index for column [SK_SecurityID]\n",
      "java.nio.channels.ClosedByInterruptException\n",
      "\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n",
      "\tat sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:293)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.seek(RawLocalFileSystem.java:157)\n",
      "\tat org.apache.hadoop.fs.BufferedFSInputStream.seek(BufferedFSInputStream.java:102)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:71)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:270)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.fill(FSInputChecker.java:215)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:256)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.seek(FSInputChecker.java:441)\n",
      "\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:71)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:356)\n",
      "\tat org.apache.parquet.hadoop.util.H1SeekableInputStream.seek(H1SeekableInputStream.java:46)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readOffsetIndex(ParquetFileReader.java:1428)\n",
      "\tat org.apache.parquet.hadoop.ColumnIndexStoreImpl$IndexStoreImpl.<init>(ColumnIndexStoreImpl.java:58)\n",
      "\tat org.apache.parquet.hadoop.ColumnIndexStoreImpl.<init>(ColumnIndexStoreImpl.java:140)\n",
      "\tat org.apache.parquet.hadoop.ColumnIndexStoreImpl.create(ColumnIndexStoreImpl.java:126)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.getColumnIndexStore(ParquetFileReader.java:1175)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.getRowRanges(ParquetFileReader.java:1186)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.getFilteredRecordCount(ParquetFileReader.java:865)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.initialize(SpecificParquetRecordReaderBase.java:135)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initialize(VectorizedParquetRecordReader.java:194)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:291)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:217)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/11/15 16:48:40 WARN TaskSetManager: Lost task 0.0 in stage 28.0 (TID 629) (192.168.1.54 executor driver): TaskKilled (Stage cancelled: Job 23 cancelled part of cancelled job tag broadcast exchange (runId 87fd72c5-7f52-40d0-abfd-3f656617387e))\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/TPC-DI.py\", line 2428, in <module>\n",
      "    run_historical_load()\n",
      "  File \"/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/TPC-DI.py\", line 2385, in run_historical_load\n",
      "    factmarkethistory =load_staging_FactMarketStory(dbname, staging_area_folder)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark/TPC-DI.py\", line 1459, in load_staging_FactMarketStory\n",
      "    spark.sql(\"\"\"\n",
      "  File \"/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/spark3/spark-3.5.3-bin-hadoop3/python/pyspark/sql/session.py\", line 1631, in sql\n",
      "    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/spark3/spark-3.5.3-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "  File \"/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/spark3/spark-3.5.3-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/spark3/spark-3.5.3-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o29.sql.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 1 times, most recent failure: Lost task 0.0 in stage 27.0 (TID 628) (192.168.1.54 executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///Users/sebastianneri/Documents/BDMA/ULB/INFO-H419%20-%20Data%20warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/DailyMarket.txt. Details:\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.hadoop.fs.FSError: java.io.IOException: Operation canceled\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:284)\n",
      "\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 20 more\n",
      "Caused by: java.io.IOException: Operation canceled\n",
      "\tat java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.io.FileInputStream.read(FileInputStream.java:255)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 43 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: org.apache.spark.SparkException: Encountered error while reading file file:///Users/sebastianneri/Documents/BDMA/ULB/INFO-H419%20-%20Data%20warehouses/Projects/TPC-DI/TPCDI-Benchmark/data/Scale3/Batch1/DailyMarket.txt. Details:\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.hadoop.fs.FSError: java.io.IOException: Operation canceled\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:211)\n",
      "\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:284)\n",
      "\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:460)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:272)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:300)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n",
      "\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 20 more\n",
      "Caused by: java.io.IOException: Operation canceled\n",
      "\tat java.io.FileInputStream.readBytes(Native Method)\n",
      "\tat java.io.FileInputStream.read(FileInputStream.java:255)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202)\n",
      "\t... 43 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 TPC-DI.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a7577d-5ef0-48b8-aadd-acc91c42450a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 2.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall numpy\n",
    "!pip install numpy==1.26.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "495fa7f3-f48b-409d-b203-22bbed7ea1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sebastianneri/Documents/BDMA/ULB/INFO-H419 - Data warehouses/Projects/TPC-DI/TPCDI-Benchmark'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65ae46b4-7ce8-4ac0-bd44-b788d9be8bd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dbutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdbutils\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dbutils'"
     ]
    }
   ],
   "source": [
    "import dbutils"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
